{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43eda2f0",
      "metadata": {
        "id": "43eda2f0"
      },
      "source": [
        "# Focus Area 3 — Temperature Quality &amp; Microclimates\n",
        "**Core Objective**: To demonstrate the advantages of high-resolution temperature data in\n",
        "capturing microclimates and computing derived metrics like PET, for better assessment of heat-\n",
        "related risks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa97e7ce",
      "metadata": {
        "id": "fa97e7ce"
      },
      "source": [
        "## Temperature data\n",
        "- CBAM data\n",
        "- ERA5 data\n",
        "- TAHMO data\n",
        "<br>\n",
        "\n",
        "EA: March 2024 heatwave\n",
        "\n",
        "\n",
        "Choose the current location and get the nearest GHCNd weather station and visualise the temperature over the last half a century\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6df409a",
      "metadata": {
        "id": "f6df409a"
      },
      "source": [
        "Require 2 files\n",
        "- The Metadata file: Ground_Metadata.csv\n",
        "- The Ground_station data file: Ground_data.csv\n",
        "\n",
        "For TAHMO data we shall extract the data during this workshop period.\n",
        "\n",
        "Metadata file format (Columns):\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>TAHMO Metadata</title>\n",
        "</head>\n",
        "<body>\n",
        "    <table border=\"1\">\n",
        "        <tr>\n",
        "            <th>Code</th>\n",
        "            <th>lat</th>\n",
        "            <th>lon</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>TA00283</td>\n",
        "            <td>1.2345</td>\n",
        "            <td>36.7890</td>\n",
        "        </tr>\n",
        "        <!-- More rows as needed -->\n",
        "    </table>\n",
        "</html>\n",
        "\n",
        "Data file format (Columns): Temperature / Precipitation data for multiple stations\n",
        "<html>\n",
        "<head>\n",
        "    <title>TAHMO Data</title>\n",
        "</head>\n",
        "<body>\n",
        "    <table border=\"1\">\n",
        "        <tr>\n",
        "            <th>Date</th>\n",
        "            <th>TA00283</th>\n",
        "            <th>TA00284</th>\n",
        "            <th>TA00285</th>\n",
        "            <!-- More station codes as needed -->\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>2023-01-01</td>\n",
        "            <td>25.3</td>\n",
        "            <td>26.1</td>\n",
        "            <td>24.8</td>\n",
        "        </tr>\n",
        "        <!-- More rows as needed -->\n",
        "    </table>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps Breakdown\n",
        "- Step 1: Setting up environment and Authentication\n",
        "- Step 2: Extract and visualise TAHMO temperature data\n",
        "- Step 3: Extract ERA5 data and compare with ground data\n",
        "- Step 4: Extract CBAM data and compare with ground data\n",
        "- Step 5: Compare CBAM and ERA5\n",
        "- Step 6: Compute PET and stress days with CBAM and ERA5\n",
        "- Step 7: Visualise the heat change over the last half a century\n"
      ],
      "metadata": {
        "id": "NpbU68UkNNRY"
      },
      "id": "NpbU68UkNNRY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "\n",
        "\n",
        "Country = \"Kenya\"  # Use Kenya, Uganda, or Rwanda\n",
        "\n",
        "Country_region = { # Defines the bounding boxes for the selected country or region\n",
        "    'Kenya': [(36.13, -0.3), (36.13, -2.0), (38, -2.0), (38, -0.3)],\n",
        "    'Uganda': [(36.13, -0.3), (36.13, -2.0), (38, -2.0), (38, -0.3)],\n",
        "    'Rwanda': [(36.13, -0.3), (36.13, -2.0), (38, -2.0), (38, -0.3)],\n",
        "}"
      ],
      "metadata": {
        "id": "sJOQ0Mye_xH8"
      },
      "id": "sJOQ0Mye_xH8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 1: Setting up environment and Authentication**"
      ],
      "metadata": {
        "id": "APyuTWVCNHYt"
      },
      "id": "APyuTWVCNHYt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XWw3IATm-_J9",
      "metadata": {
        "id": "XWw3IATm-_J9"
      },
      "outputs": [],
      "source": [
        "# @title 1a) Setting up environment installing required Dependencies {\"display-mode\":\"form\"}\n",
        "# @markdown This cell installs the required python dependencies and functions for the notebook.<br>\n",
        "# @markdown If you encounter any errors, please restart the runtime and try again. <br>\n",
        "\n",
        "print(\"Installing required dependencies...\")\n",
        "!pip install git+https://github.com/TAHMO/NOAA.git > /dev/null 2>&1\n",
        "\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "\n",
        "# check there was no error\n",
        "import sys\n",
        "if not sys.argv[0].endswith(\"kernel_launcher.py\"):\n",
        "    print(\"❌ Errors occurred during installation. Please restart the runtime and try again.\")\n",
        "else:\n",
        "    print(\"✅ Dependencies installed successfully.\")\n",
        "\n",
        "print(\"Importing required libraries...\")\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import ee\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr, ttest_rel\n",
        "import random\n",
        "\n",
        "# import os\n",
        "# os.chdir('NOAA-workshop')\n",
        "\n",
        "from utils.ground_stations import plot_stations_folium\n",
        "from utils.helpers import get_region_geojson\n",
        "from utils.CHIRPS_helpers import get_chirps_pentad_gee\n",
        "from utils.CBAM_helpers import CBAMClient, extract_cbam_data # CBAM helper functions\n",
        "from utils.plotting import select, scale, plot_xarray_data, plot_xarray_data2, compare_xarray_datasets, compare_xarray_datasets2 # Plotting helper functionsfrom utils.IMERG_helpers import get_imerg_raw\n",
        "from utils.ERA5_helpers import era5_data_extracts, era5_var_handling\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import pandas as pd\n",
        "import json\n",
        "import ee\n",
        "from scipy.stats import pearsonr\n",
        "import seaborn as sns\n",
        "from utils.filter_stations import RetrieveData\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"✅ Libraries imported successfully.\")\n",
        "\n",
        "def build_xr_from_stations(ds, stations_metadata, var_name=None):\n",
        "    # Auto-detect variable if not provided\n",
        "    if var_name is None:\n",
        "        candidate_vars = ['total_precipitation', 'total_rainfall', 'precipitation']\n",
        "        found = [v for v in candidate_vars if v in ds.data_vars]\n",
        "        if not found:\n",
        "            raise ValueError(f\"None of expected precipitation variable names {candidate_vars} found in dataset vars: {list(ds.data_vars)}\")\n",
        "        var_name = found[0]\n",
        "\n",
        "    # Determine dimension names\n",
        "    if {'x', 'y'}.issubset(ds.dims):\n",
        "        lon_dim, lat_dim = 'x', 'y'\n",
        "    elif {'lon', 'lat'}.issubset(ds.dims):\n",
        "        lon_dim, lat_dim = 'lon', 'lat'\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset dims {list(ds.dims)} do not contain expected (x,y) or (lon,lat).\")\n",
        "\n",
        "    all_stations_data = {}\n",
        "    for _, row in stations_metadata.iterrows():\n",
        "        station_code = row['code']\n",
        "        lat = float(row['lat'])\n",
        "        lon = float(row['lon'])\n",
        "        # Skip stations outside domain (quick bounds check)\n",
        "        if not (ds[lon_dim].min() <= lon <= ds[lon_dim].max() and ds[lat_dim].min() <= lat <= ds[lat_dim].max()):\n",
        "            continue\n",
        "        station_da = ds[var_name].sel({lon_dim: lon, lat_dim: lat}, method=\"nearest\")\n",
        "        station_df = station_da.to_dataframe(name=station_code)\n",
        "        all_stations_data[station_code] = station_df[station_code]\n",
        "\n",
        "    combined_df = pd.DataFrame(all_stations_data)\n",
        "    return combined_df\n",
        "\n",
        "\n",
        "\n",
        "def plot_temperatures(tmin_df, tavg_df, tmax_df, station_code=None):\n",
        "    \"\"\"\n",
        "    Plots the daily minimum, average, and maximum temperatures for a specified TAHMO station.\n",
        "\n",
        "    Args:\n",
        "        tmin_df (pd.DataFrame): DataFrame containing daily minimum temperatures.\n",
        "        tavg_df (pd.DataFrame): DataFrame containing daily average temperatures.\n",
        "        tmax_df (pd.DataFrame): DataFrame containing daily maximum temperatures.\n",
        "        station_code (str, optional): The code of the station to plot. If None, a random station from the DataFrame is plotted.\n",
        "    \"\"\"\n",
        "    if station_code is None:\n",
        "        station_code = random.choice(tmin_df.columns.tolist())\n",
        "        print(f\"Randomly selected station: {station_code}\")\n",
        "    elif station_code not in tmin_df.columns:\n",
        "        print(f\"Station code {station_code} not found in the data.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(tmin_df.index, tmin_df[station_code], label='Min Temp', linestyle='-')\n",
        "    plt.plot(tavg_df.index, tavg_df[station_code], label='Avg Temp', linestyle='-')\n",
        "    plt.plot(tmax_df.index, tmax_df[station_code], label='Max Temp', linestyle='-')\n",
        "\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Temperature (°C)')\n",
        "    plt.title(f'Daily Temperatures for Station {station_code}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### 1b) Google drive authentication Step {\"display-mode\":\"form\"}\n",
        "# @markdown This step is allows the notebook to access google drive to retrieve data files\n",
        "# his workshop, we have created the ```noaa-tahmo``` project that you can input as your project id<br><br><br>\n",
        "\n",
        "print(\"Authenticating to Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"✅ Google Drive authenticated successfully.\")\n",
        "\n",
        "# @title Step 1c: Loading the config files\n",
        "config_file_path = '/content/drive/Shareddrives/NOAA-workshop2/config.json'\n",
        "\n",
        "# check if path exists\n",
        "if not os.path.exists(config_file_path):\n",
        "    print(\"❌ Config file not found. Please upload it first.\")\n",
        "else:\n",
        "    print(\"✅ Config file loaded successfully.\")\n",
        "\n",
        "# Loading the config file and parsing from uploaded incase it comes with a different name\n",
        "import json\n",
        "with open(config_file_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "\n",
        "# import ee\n",
        "\n",
        "# # Authenticate and initialise Google Earth Engine\n",
        "# # This will open a link in your browser to grant permissions if necessary.\n",
        "# try:\n",
        "#     print(\"Authenticating Google Earth Engine. Please follow the instructions in your browser.\")\n",
        "#     ee.Authenticate()\n",
        "#     print(\"✅ Authentication successful.\")\n",
        "# except ee.auth.scopes.MissingScopeError:\n",
        "#     print(\"Authentication scopes are missing. Please re-run the cell and grant the necessary permissions.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Authentication failed: {e}\")\n",
        "\n",
        "# # Initialize Earth Engine with your project ID\n",
        "# # Replace 'your-project-id' with your actual Google Cloud Project ID\n",
        "# # You need to create an unpaid project manually through the Google Cloud Console\n",
        "# print(\"\\nIf you already have a project id paste it below. If you do not have a project You need to create an unpaid project manually through the Google Cloud Console\")\n",
        "# print(\"💡 You can create a new project here: https://console.cloud.google.com/projectcreate and copy the project id\")\n",
        "# try:\n",
        "#     # It's recommended to use a project ID associated with your Earth Engine account.\n",
        "#     print(\"\\nEnter your Google Cloud Project ID: \")\n",
        "#     project_id = input(\"\")\n",
        "#     ee.Initialize(project=project_id)\n",
        "#     print(\"✅ Google Earth Engine initialized successfully.\")\n",
        "# except ee.EEException as e:\n",
        "#     if \"PERMISSION_DENIED\" in str(e):\n",
        "#         print(f\"Earth Engine initialization failed due to PERMISSION_DENIED.\")\n",
        "#         print(\"Please ensure the Earth Engine API is enabled for your project:\")\n",
        "#         print(\"Enable the Earth Engine API here: https://console.developers.google.com/apis/api/earthengine.googleapis.com/overview?project=elated-capsule-471808-k1\")\n",
        "#     else:\n",
        "#         print(f\"Earth Engine initialization failed: {e}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An unexpected error occurred during initialization: {e}\")"
      ],
      "metadata": {
        "id": "lY_TL02ishPn"
      },
      "id": "lY_TL02ishPn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Extract and visualise TAHMO temperature data**"
      ],
      "metadata": {
        "id": "UDrNKlQjOOMB"
      },
      "id": "UDrNKlQjOOMB"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2a) Visualise your selected region {\"display-mode\":\"form\"}\n",
        "# @markdown This cell previews the bounding box set at the first configuration section\n",
        "import time\n",
        "import json\n",
        "import plotly.graph_objects as go\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Polygon\n",
        "import sys\n",
        "import importlib\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "def xmin_ymin_xmax_ymax(polygon):\n",
        "    lons = [pt[0] for pt in polygon]\n",
        "    lats = [pt[1] for pt in polygon]\n",
        "    return min(lons), min(lats), max(lons), max(lats)\n",
        "\n",
        "\n",
        "def show_region_plotly(polygon, region_name=\"Region\", margin=0.05):\n",
        "    \"\"\"Plot polygon with Plotly Mapbox\"\"\"\n",
        "    lons = [pt[0] for pt in polygon]\n",
        "    lats = [pt[1] for pt in polygon]\n",
        "    fig = go.Figure(go.Scattermapbox(\n",
        "        lon=lons + [lons[0]],\n",
        "        lat=lats + [lats[0]],\n",
        "        mode=\"lines\",\n",
        "        fill=\"toself\",\n",
        "        fillcolor=\"rgba(0,0,255,0.3)\",\n",
        "        name=region_name\n",
        "    ))\n",
        "    fig.update_layout(\n",
        "        mapbox_style=\"open-street-map\",\n",
        "        mapbox=dict(center={\"lat\": sum(lats)/len(lats), \"lon\": sum(lons)/len(lons)}, zoom=5),\n",
        "        margin=dict(r=0, t=30, l=0, b=0),\n",
        "        title=f\"Region of Interest: {region_name}\",\n",
        "        height=500,\n",
        "        width=900\n",
        "    )\n",
        "    fig.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "region_geom = Country_region[Country]\n",
        "\n",
        "if region_geom:\n",
        "    xmin, ymin, xmax, ymax = xmin_ymin_xmax_ymax(region_geom)\n",
        "    show_region_plotly(region_geom, region_name=Country)\n",
        "    print(f\"📦 Bounding box -> xmin: {xmin}, ymin: {ymin}, xmax: {xmax}, ymax: {ymax}\")\n",
        "else:\n",
        "    print(\"🛑 No geometry available.\")\n"
      ],
      "metadata": {
        "id": "vBpSJl9Kf_XS"
      },
      "id": "vBpSJl9Kf_XS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7PxFVeh5O-jC"
      },
      "id": "7PxFVeh5O-jC"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2b) Extract TAHMO Metadata {\"display-mode\":\"form\"}\n",
        "# @markdown This cell loads the TAHMO data from google drive\n",
        "\n",
        "from utils.filter_stations import RetrieveData\n",
        "import os\n",
        "import time\n",
        "region_query=Country\n",
        "dir_path = '/content/drive/MyDrive/NOAA-workshop-data'\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "# check if the path was created successfully\n",
        "if not os.path.exists(dir_path):\n",
        "    print(\"❌ Path not created successfully.\")\n",
        "else:\n",
        "    print(\"✅ Path created successfully.\")\n",
        "\n",
        "# check if the config exists\n",
        "# if not os.path.exists('/content/config.json'):\n",
        "#     print(\"❌ Config file not found. Please upload it first.\")\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "def plot_stations_plotly(dataframes, colors=None, zoom=5, height=500,\n",
        "                         width=900, legend_title='Station Locations', ghcnd_coords=False):\n",
        "    \"\"\"\n",
        "    Plot stations from one or more dataframes on a Plotly mapbox.\n",
        "\n",
        "    Each dataframe must have 'location.latitude' and 'location.longitude' columns.\n",
        "    'colors' is a list specifying marker colors for each dataframe respectively.\n",
        "    \"\"\"\n",
        "    if colors is None:\n",
        "        colors = [\"blue\", \"red\", \"green\", \"purple\", \"orange\"]\n",
        "\n",
        "    frames = []\n",
        "    for i, df in enumerate(dataframes):\n",
        "        temp = df.copy()\n",
        "        temp[\"color\"] = colors[i % len(colors)]  # cycle colors if more dfs than colors\n",
        "        frames.append(temp)\n",
        "\n",
        "    combined = pd.concat(frames, ignore_index=True)\n",
        "    if ghcnd_coords:\n",
        "      lat, lon, station_id = 'lat', 'lon', 'station'\n",
        "    else:\n",
        "      lat, lon, station_id = 'location.latitude', 'location.longitude', 'code'\n",
        "\n",
        "    fig = px.scatter_mapbox(\n",
        "        combined,\n",
        "        lat=lat,\n",
        "        lon=lon,\n",
        "        color=\"color\",\n",
        "        hover_name=station_id,\n",
        "        zoom=zoom,\n",
        "        height=height,\n",
        "        width=width\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        mapbox_style=\"open-street-map\",\n",
        "        legend_title=legend_title,\n",
        "        margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 0}\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n",
        "api_key = config['apiKey']\n",
        "api_secret = config['apiSecret']\n",
        "\n",
        "# Initialize the class\n",
        "rd = RetrieveData(apiKey=api_key,\n",
        "                  apiSecret=api_secret)\n",
        "\n",
        "# Extracting TAHMO data\n",
        "print(\"Extracting TAHMO data...\")\n",
        "info = rd.get_stations_info()\n",
        "info = info[(info['location.longitude'] >= xmin) &\n",
        "                        (info['location.longitude'] <= xmax) &\n",
        "                        (info['location.latitude'] >= ymin) &\n",
        "                        (info['location.latitude'] <= ymax)]\n",
        "print(\"✅ TAHMO data extracted successfully.\")\n",
        "# Print the total number of stations\n",
        "print(f\"Total number of stations: {len(info)}\")\n",
        "\n",
        "\n",
        "# save the data as csv to the created directory\n",
        "info.to_csv(f'{dir_path}/tahmo_metadata_{region_query}.csv')\n",
        "\n",
        "# wait for 5 seconds before visual\n",
        "time.sleep(5)\n",
        "\n",
        "# Visualise the data\n",
        "plot_stations_plotly([info])"
      ],
      "metadata": {
        "id": "-fpHD0dR6vDm"
      },
      "id": "-fpHD0dR6vDm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7213219a",
      "metadata": {
        "id": "7213219a"
      },
      "outputs": [],
      "source": [
        "# @title 2c) Extract the TAHMO temperature 5 minute data for 2024 and get the tmin, tavg and tmax {\"display-mode\":\"form\"}\n",
        "# Load TAHMO EAC stations previously extracted\n",
        "eac_metadata = pd.read_csv(f'{dir_path}/tahmo_metadata_{region_query}.csv')\n",
        "eac_metadata = eac_metadata[['code',\n",
        "                             'location.latitude',\n",
        "                             'location.longitude']].rename(columns={'location.latitude': 'lat',\n",
        "                                                                    'location.longitude': 'lon'})\n",
        "\n",
        "# Initialize the class\n",
        "rd = RetrieveData(apiKey=api_key,\n",
        "                  apiSecret=api_secret)\n",
        "\n",
        "print('Extracting Temperature data ...')\n",
        "# # Get the temperature data for the EAC stations in 5min intervals\n",
        "# eac_temp = rd.multiple_measurements(stations_list=eac_metadata['code'].tolist(),\n",
        "#                                      startDate=start_date,\n",
        "#                                      endDate=end_date,\n",
        "#                                      variables=['te'],\n",
        "#                                      csv_file = f'{dir_path}/tahmo_temp_{region_query}.csv',\n",
        "#                                      aggregate='5min'\n",
        "#                                      )\n",
        "\n",
        "\n",
        "# # Aggregate the values to get the min, mean and max for the day\n",
        "# tahmo_eac_tmin = rd.aggregate_variables(\n",
        "#     eac_temp,\n",
        "#     freq='1D',\n",
        "#     method='min'\n",
        "# )\n",
        "# tahmo_eac_tavg = rd.aggregate_variables(\n",
        "#     eac_temp,\n",
        "#     freq='1D',\n",
        "#     method='mean'\n",
        "# )\n",
        "# tahmo_eac_tmax = rd.aggregate_variables(\n",
        "#     eac_temp,\n",
        "#     freq='1D',\n",
        "#     method='max'\n",
        "# )\n",
        "\n",
        "\n",
        "# # plot_temperatures(tahmo_eac_tmin, tahmo_eac_tavg, tahmo_eac_tmax)\n",
        "\n",
        "\n",
        "# # Save the variables\n",
        "# tahmo_eac_tmin.to_csv(f'{dir_path}/tahmo_tmin_{region_query}.csv', index=True)\n",
        "# tahmo_eac_tavg.to_csv(f'{dir_path}/tahmo_tmin_{region_query}.csv', index=True)\n",
        "# tahmo_eac_tmax.to_csv(f'{dir_path}/tahmo_tmin_{region_query}.csv', index=True)\n",
        "\n",
        "# Method to cleanup the data\n",
        "def format_cleanup(df, localize_none=True):\n",
        "  # Rename Unnamed: 0 to Date\n",
        "  df.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
        "  # Set Date as index\n",
        "  df.set_index('Date', inplace=True)\n",
        "\n",
        "  # convert Date to datetime\n",
        "  df.index = pd.to_datetime(df.index)\n",
        "  if localize_none:\n",
        "    # set tz_localize to None\n",
        "    df.index = df.index.tz_localize(None)\n",
        "  return df\n",
        "\n",
        "# get only stations that have the metadata\n",
        "def match_with_metadata(df, metadata, column='code', localize_none=True):\n",
        "  # Format the data\n",
        "  df = format_cleanup(df, localize_none=localize_none)\n",
        "\n",
        "  # get the stations list\n",
        "  stations_list = metadata[column].to_list()\n",
        "\n",
        "  # Subset the columns from the dataframe with the data\n",
        "  df = df[df.columns.intersection(stations_list)]\n",
        "\n",
        "  return df\n",
        "\n",
        "# Load the tahmo data\n",
        "base_data_path = '/content/drive/Shareddrives/NOAA-workshop/Datasets/ground'\n",
        "tahmo_tmin = pd.read_csv(os.path.join(base_data_path,'eac_tmin_march_2024.csv' ))\n",
        "tahmo_tmin = match_with_metadata(tahmo_tmin, info)\n",
        "tahmo_tmax = pd.read_csv(os.path.join(base_data_path,'eac_tmax_march_2024.csv' ))\n",
        "tahmo_tmax = match_with_metadata(tahmo_tmax, info)\n",
        "tahmo_tavg = pd.read_csv(os.path.join(base_data_path,'eac_tavg_march_2024.csv' ))\n",
        "tahmo_tavg = match_with_metadata(tahmo_tavg, info)\n",
        "\n",
        "# check if the data is well loaded\n",
        "start_date = tahmo_tavg.index.min().strftime('%Y-%m-%d')\n",
        "end_date = tahmo_tavg.index.max().strftime('%Y-%m-%d')\n",
        "\n",
        "print('✅ Tahmo data loaded')\n",
        "# visualise the tavg data\n",
        "print('Printing the first 5 rows of the tavg data')\n",
        "tahmo_tavg.head().dropna(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2d) Randomly visualise TAHMO Tmin, Tavg, and Tmax data on a single chart {\"display-mode\":\"form\"}\n",
        "\n",
        "# Call the existing plot_temperatures function\n",
        "plot_temperatures(tahmo_tmin, tahmo_tavg, tahmo_tmax)"
      ],
      "metadata": {
        "id": "wmDO3tJlC_fz"
      },
      "id": "wmDO3tJlC_fz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 3: Extract ERA5 data and compare with ground data**"
      ],
      "metadata": {
        "id": "KCmo-VsdP4og"
      },
      "id": "KCmo-VsdP4og"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load ERA5 daily data for the month of March {\"display-mode\":\"form\"}\n",
        "# @markdown The ERA5 equivalent variable is temperature_2m <br>\n",
        "# @markdown We will visualise the station against the ground data\n",
        "\n",
        "# # @title ERA5 builder\n",
        "# import ee\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import xarray as xr\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.animation as animation\n",
        "# import matplotlib.colors\n",
        "# import math\n",
        "# import datetime\n",
        "# import io\n",
        "# from tqdm import tqdm\n",
        "# from datetime import datetime, timedelta\n",
        "# from IPython.display import HTML, display\n",
        "# import cartopy.crs as ccrs\n",
        "# import cartopy.feature as cfeature\n",
        "# from filter_stations import retreive_data, Filter\n",
        "# import base64\n",
        "# import json\n",
        "# import requests\n",
        "# import datetime\n",
        "# from utils.helpers import get_region_geojson, df_to_xarray\n",
        "\n",
        "\n",
        "\n",
        "# def extract_era5_daily(start_date_str, end_date_str, bbox=None, polygon=None, era5_l=False, aggregate='mean'):\n",
        "#     \"\"\"\n",
        "#     Extract ERA5 reanalysis data (daily aggregated) from Google Earth Engine for a given bounding box or polygon and time range.\n",
        "#     The extraction is performed on a daily basis by aggregating hourly images (using the mean) for each day.\n",
        "#     For each day, the function retrieves the ERA5 HOURLY images, aggregates them, adds pixel coordinate bands (longitude\n",
        "#     and latitude), and uses sampleRectangle to extract a grid of pixel values. The results for each variable (band) are then\n",
        "#     organized into pandas DataFrames with the following columns:\n",
        "#       - date: The daily timestamp (ISO formatted)\n",
        "#       - latitude: The latitude coordinate of the pixel center\n",
        "#       - longitude: The longitude coordinate of the pixel center\n",
        "#       - value: The aggregated pixel value for that variable\n",
        "\n",
        "#     Args:\n",
        "#         start_date_str (str): Start datetime in ISO format, e.g., '2020-01-01T00:00:00'.\n",
        "#         end_date_str (str): End datetime in ISO format, e.g., '2020-01-02T00:00:00'.\n",
        "#         bbox (list or tuple, optional): Bounding box specified as [minLon, minLat, maxLon, maxLat]. Default is None.\n",
        "#         polygon (list, optional): Polygon specified as a list of coordinate pairs (e.g., [[lon, lat], ...]).\n",
        "#                                   If provided, the polygon geometry will be used instead of the bounding box.\n",
        "#                                   Default is None.\n",
        "#         era5_l (bool, optional): If True, use ERA5_LAND instead of ERA5. Default is False.\n",
        "#         aggregate (str, optional): Aggregation method ('mean' or 'sum' or 'min', or 'max'). Default is 'mean'.\n",
        "\n",
        "#     Returns:\n",
        "#         dict: A dictionary where keys are variable (band) names and values are pandas DataFrames containing\n",
        "#               the daily aggregated data.\n",
        "#     \"\"\"\n",
        "#     # Convert input datetime strings to Python datetime objects.\n",
        "#     start_date = datetime.datetime.strptime(start_date_str, '%Y-%m-%dT%H:%M:%S')\n",
        "#     end_date   = datetime.datetime.strptime(end_date_str, '%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "#     # Define the geometry: Use polygon if provided, otherwise use bbox.\n",
        "#     if polygon is not None:\n",
        "#         region = ee.Geometry.Polygon(polygon)\n",
        "#     elif bbox is not None:\n",
        "#         region = ee.Geometry.Rectangle(bbox)\n",
        "#     else:\n",
        "#         raise ValueError(\"Either bbox or polygon must be provided.\")\n",
        "\n",
        "#     # Define a scale in meters corresponding approximately to 0.25° (at the equator, 1° ≈ 111320 m).\n",
        "#     scale_m = 27830\n",
        "\n",
        "#     # This dictionary will accumulate extracted records for each variable (band).\n",
        "#     results = {}\n",
        "\n",
        "#     # Loop over each day in the specified time range.\n",
        "#     current = start_date\n",
        "#     while current < end_date:\n",
        "#         next_day = current + datetime.timedelta(days=1)\n",
        "\n",
        "#         # Format the current time window in ISO format.\n",
        "#         t0_str = current.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "#         t1_str = next_day.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "#         print(f\"Processing {t0_str} to {t1_str}\")\n",
        "\n",
        "#         # If ER5 Land (0.1) or ERA5 (0.25)\n",
        "#         if era5_l:\n",
        "#             # Get the ERA5 Land hourly image collection for the current day.\n",
        "#             collection = ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY') \\\n",
        "#                             .filterDate(ee.Date(t0_str), ee.Date(t1_str))\n",
        "#         else:\n",
        "#             # Get the ERA5 hourly image collection for the current day.\n",
        "#             collection = ee.ImageCollection('ECMWF/ERA5/HOURLY') \\\n",
        "#                             .filterDate(ee.Date(t0_str), ee.Date(t1_str))\n",
        "\n",
        "#         # Aggregate the hourly images into a single daily image using the mean.\n",
        "#         if aggregate == 'mean':\n",
        "#             image = collection.mean()\n",
        "#         elif aggregate == 'sum':\n",
        "#             image = collection.sum()\n",
        "#         elif aggregate == 'min':\n",
        "#             image = collection.min()\n",
        "#         elif aggregate == 'max':\n",
        "#             image = collection.max()\n",
        "#         else:\n",
        "#             raise ValueError(f\"Invalid aggregation method: {aggregate} can either be sum, min, max or mean\")\n",
        "\n",
        "#         # Add bands containing the pixel longitude and latitude.\n",
        "#         image = image.addBands(ee.Image.pixelLonLat())\n",
        "\n",
        "#         # Use sampleRectangle to extract a grid of pixel values over the region.\n",
        "#         region_data = image.sampleRectangle(region=region, defaultValue=0).getInfo()\n",
        "\n",
        "#         # The pixel values for each band are in the \"properties\" dictionary.\n",
        "#         props = region_data['properties']\n",
        "\n",
        "#         # Extract the coordinate arrays from the added pixelLonLat bands.\n",
        "#         lon_array = props['longitude']  # 2D array of longitudes\n",
        "#         lat_array = props['latitude']   # 2D array of latitudes\n",
        "\n",
        "#         # Determine the dimensions of the extracted grid.\n",
        "#         nrows = len(lon_array)\n",
        "#         ncols = len(lon_array[0]) if nrows > 0 else 0\n",
        "\n",
        "#         # Identify the names of the bands that hold ERA5 variables, excluding the coordinate bands.\n",
        "#         band_names = [key for key in props.keys() if key not in ['longitude', 'latitude']]\n",
        "\n",
        "#         # Initialize results lists for each band if not already present.\n",
        "#         for band in band_names:\n",
        "#             if band not in results:\n",
        "#                 results[band] = []\n",
        "\n",
        "#         # Loop over each pixel in the grid.\n",
        "#         for i in range(nrows):\n",
        "#             for j in range(ncols):\n",
        "#                 pixel_lon = lon_array[i][j]\n",
        "#                 pixel_lat = lat_array[i][j]\n",
        "#                 # For each ERA5 variable band, extract the pixel value and create a record.\n",
        "#                 for band in band_names:\n",
        "#                     pixel_value = props[band][i][j]\n",
        "#                     record = {\n",
        "#                         'date': t0_str,  # daily timestamp as a string\n",
        "#                         'latitude': pixel_lat,\n",
        "#                         'longitude': pixel_lon,\n",
        "#                         'value': pixel_value\n",
        "#                     }\n",
        "#                     results[band].append(record)\n",
        "\n",
        "#         # Advance to the next day.\n",
        "#         current = next_day\n",
        "\n",
        "#     # Convert the accumulated results for each band into pandas DataFrames.\n",
        "#     dataframes = {band: pd.DataFrame(records) for band, records in results.items()}\n",
        "#     return dataframes\n",
        "\n",
        "\n",
        "\n",
        "# # ERA5 helper expects ISO-like datetime strings with time component (%Y-%m-%dT%H:%M:%S)\n",
        "# iso_start_date = f\"{start_date}T00:00:00\"\n",
        "# iso_end_date = f\"{end_date}T23:59:59\"\n",
        "\n",
        "# era5_region_tmin = extract_era5_daily(iso_start_date, iso_end_date, era5_l=False,\n",
        "#                                    polygon=region_geom, aggregate='min')\n",
        "# era5_region_tavg = extract_era5_daily(iso_start_date, iso_end_date, era5_l=False,\n",
        "#                                    polygon=region_geom, aggregate='mean')\n",
        "# era5_region_tmax = extract_era5_daily(iso_start_date, iso_end_date, era5_l=False,\n",
        "#                                    polygon=region_geom, aggregate='max')\n",
        "\n",
        "# # xarray for tempersture 2m\n",
        "# era5_tmin = era5_var_handling(era5_region_tmin, 'temperature_2m', xarray_ds=True)\n",
        "# era5_tavg = era5_var_handling(era5_region_tavg, 'temperature_2m', xarray_ds=True)\n",
        "# era5_tmax = era5_var_handling(era5_region_tmax, 'temperature_2m', xarray_ds=True)\n",
        "\n",
        "# # save to xarray\n",
        "# era5_tmin.to_netcdf(f'{dir_path}/ERA5_tmin_{region_query}.nc')\n",
        "# era5_tavg.to_netcdf(f'{dir_path}/ERA5_tavg_{region_query}.nc')\n",
        "# era5_tmax.to_netcdf(f'{dir_path}/ERA5_tmax_{region_query}.nc')\n",
        "\n",
        "\n",
        "# Load the data\n",
        "era5_base_path = '/content/drive/Shareddrives/NOAA-workshop/Datasets/reanalysis/era5'\n",
        "\n",
        "# tavg\n",
        "era5_tavg = xr.open_dataset(os.path.join(era5_base_path,'era5_tavg_march_2024.nc'))\n",
        "era5_tavg\n",
        "era5_tmin = xr.open_dataset(os.path.join(era5_base_path,'era5_tmin_march_2024.nc'))\n",
        "era5_tmin\n",
        "era5_tmax = xr.open_dataset(os.path.join(era5_base_path,'era5_tmax_march_2024.nc'))\n",
        "era5_tmax\n",
        "\n",
        "# select the region within xmin, xmax ymin and ymax\n",
        "def subset_within_x_ymin_max(ds, xmin, xmax, ymin, ymax):\n",
        "  return ds.sel(lat=slice(ymin, ymax), lon=slice(xmin, xmax))\n",
        "\n",
        "era5_tavg = subset_within_x_ymin_max(era5_tavg, xmin, xmax, ymin, ymax)\n",
        "era5_tmin = subset_within_x_ymin_max(era5_tmin, xmin, xmax, ymin, ymax)\n",
        "era5_tmax = subset_within_x_ymin_max(era5_tmax, xmin, xmax, ymin, ymax)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import matplotlib.colors as mcolors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "from IPython.display import HTML\n",
        "\n",
        "def point_plot(\n",
        "    weather_df,\n",
        "    metadata_df,\n",
        "    variable_name=\"Observation\",\n",
        "    cmap=\"viridis\",\n",
        "    robust=True,\n",
        "    fig_title=None,\n",
        "    interval=300,\n",
        "    bbox=None,\n",
        "    save=False,\n",
        "    metadata_columns=None,\n",
        "    grid_da=None,\n",
        "    grid_cmap=\"coolwarm\",\n",
        "    grid_alpha=0.6,\n",
        "    grid_da_var=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize point-based weather station data and optionally overlay on a gridded Xarray dataset.\n",
        "\n",
        "    Args:\n",
        "        weather_df (pd.DataFrame): Time-indexed DataFrame with stations as columns.\n",
        "        metadata_df (pd.DataFrame): Station metadata with IDs and coordinates.\n",
        "        variable_name (str): Name of variable being visualized (for point data).\n",
        "        cmap (str): Colormap for point data.\n",
        "        robust (bool): Use 2nd–98th percentile limits for normalization.\n",
        "        fig_title (str): Figure title.\n",
        "        interval (int): Animation interval in milliseconds.\n",
        "        bbox (list): [lon_min, lon_max, lat_min, lat_max]. Inferred if None.\n",
        "        save (bool): Save animation as GIF if True.\n",
        "        metadata_columns (list): [station_id, lat, lon] column names.\n",
        "        grid_da (xr.DataArray or xr.Dataset): Optional Xarray grid to overlay. If Dataset, grid_da_var must be specified.\n",
        "        grid_cmap (str): Colormap for the gridded field.\n",
        "        grid_alpha (float): Transparency for the gridded field.\n",
        "        grid_da_var (str): Name of the variable in grid_da if grid_da is a Dataset.\n",
        "\n",
        "    Returns:\n",
        "        HTML: Inline animation for Jupyter display.\n",
        "    \"\"\"\n",
        "    # --- Validation and setup ---\n",
        "    if metadata_columns is None:\n",
        "        metadata_columns = [\"station_id\", \"lat\", \"lon\"]\n",
        "    station_col, lat_col, lon_col = metadata_columns\n",
        "\n",
        "    for col in [station_col, lat_col, lon_col]:\n",
        "        if col not in metadata_df.columns:\n",
        "            raise ValueError(f\"Missing required metadata column: '{col}' in metadata_df\")\n",
        "\n",
        "    # --- Prepare spatial data ---\n",
        "    # Ensure weather_df columns match metadata_df station IDs for merging\n",
        "    # This assumes weather_df columns are the station IDs\n",
        "    if weather_df.columns.name != station_col:\n",
        "         weather_df.columns.name = station_col\n",
        "\n",
        "\n",
        "    lons = metadata_df.set_index(station_col).loc[weather_df.columns][lon_col].values\n",
        "    lats = metadata_df.set_index(station_col).loc[weather_df.columns][lat_col].values\n",
        "\n",
        "    # --- Color normalization ---\n",
        "    data_values = weather_df.values.flatten()\n",
        "    vmin = np.nanpercentile(data_values, 2 if robust else 0)\n",
        "    vmax = np.nanpercentile(data_values, 98 if robust else 100)\n",
        "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
        "\n",
        "    # --- Create figure ---\n",
        "    fig = plt.figure(figsize=(6, 6))\n",
        "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
        "    ax.add_feature(cfeature.COASTLINE, linewidth=1)\n",
        "    ax.add_feature(cfeature.BORDERS, linestyle=\":\", linewidth=0.5)\n",
        "    ax.add_feature(cfeature.LAND, facecolor=\"lightgray\")\n",
        "    ax.add_feature(cfeature.OCEAN, facecolor=\"aliceblue\")\n",
        "    if bbox is not None:\n",
        "        ax.set_extent(bbox)\n",
        "    else:\n",
        "         # Set extent based on station coordinates with padding if no bbox is provided\n",
        "        pad = 0.5  # degrees of padding\n",
        "        lon_min, lon_max = lons.min() - pad, lons.max() + pad\n",
        "        lat_min, lat_max = lats.min() - pad, lats.max() + pad\n",
        "        ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
        "\n",
        "\n",
        "    # --- Optional: plot gridded Xarray background ---\n",
        "    grid_plot = None\n",
        "    grid_cbar_label = \"Grid\"\n",
        "    if grid_da is not None:\n",
        "        if isinstance(grid_da, xr.Dataset):\n",
        "            if grid_da_var is None or grid_da_var not in grid_da.data_vars:\n",
        "                raise ValueError(\"grid_da_var must be specified and exist in grid_da Dataset\")\n",
        "            grid_data_to_plot = grid_da[grid_da_var]\n",
        "            grid_cbar_label = grid_da_var if grid_da_var else \"Grid\"\n",
        "        elif isinstance(grid_da, xr.DataArray):\n",
        "            grid_data_to_plot = grid_da\n",
        "            grid_cbar_label = grid_da.name if grid_da.name else \"Grid\"\n",
        "        else:\n",
        "            raise TypeError(\"grid_da must be an xarray Dataset or DataArray\")\n",
        "\n",
        "\n",
        "        # Select nearest time step for animation frame 0 if time dimension exists\n",
        "        initial_grid_frame = grid_data_to_plot.isel(time=0) if \"time\" in grid_data_to_plot.dims else grid_data_to_plot\n",
        "\n",
        "        grid_plot = initial_grid_frame.plot(\n",
        "            ax=ax,\n",
        "            transform=ccrs.PlateCarree(),\n",
        "            cmap=grid_cmap,\n",
        "            alpha=grid_alpha,\n",
        "            add_colorbar=True,\n",
        "            cbar_kwargs={\"shrink\": 0.7, \"pad\": 0.02, \"label\": grid_cbar_label},\n",
        "        )\n",
        "\n",
        "\n",
        "    # --- Plot station points ---\n",
        "    scatter = ax.scatter(\n",
        "        lons,\n",
        "        lats,\n",
        "        c=weather_df.iloc[0].values, # Use .values to get numpy array\n",
        "        cmap=cmap,\n",
        "        norm=norm,\n",
        "        s=50,\n",
        "        transform=ccrs.PlateCarree(),\n",
        "        edgecolor=\"black\",\n",
        "        linewidth=0.3,\n",
        "        zorder=3\n",
        "    )\n",
        "\n",
        "    # --- Colorbar for point data---\n",
        "    cbar = plt.colorbar(scatter, ax=ax, orientation=\"vertical\", shrink=0.7, pad=0.02)\n",
        "    cbar.set_label(variable_name, fontsize=10)\n",
        "\n",
        "    # --- Title ---\n",
        "    if fig_title is None:\n",
        "        fig_title = f\"{variable_name} Over Time\"\n",
        "    time_index = weather_df.index\n",
        "    initial_time_label = time_index[0].strftime(\"%Y-%m-%d %H:%M\") if isinstance(time_index[0], pd.Timestamp) else str(time_index[0])\n",
        "    title = ax.set_title(f\"{fig_title}\\n{initial_time_label}\", fontsize=14)\n",
        "\n",
        "    # --- Animation update function ---\n",
        "    def update(frame):\n",
        "        values = weather_df.iloc[frame].values\n",
        "        scatter.set_array(values)\n",
        "\n",
        "        # Update gridded background if it exists and has a time dimension\n",
        "        if grid_plot is not None and \"time\" in grid_data_to_plot.dims:\n",
        "            # Remove previous grid image\n",
        "            if len(ax.images) > 0:\n",
        "                 ax.images[-1].remove()\n",
        "\n",
        "            current_grid_frame = grid_data_to_plot.isel(time=frame)\n",
        "            current_grid_frame.plot(\n",
        "                ax=ax,\n",
        "                transform=ccrs.PlateCarree(),\n",
        "                cmap=grid_cmap,\n",
        "                alpha=grid_alpha,\n",
        "                add_colorbar=False,\n",
        "                zorder=1 # Plot grid below points\n",
        "            )\n",
        "\n",
        "\n",
        "        current_time_label = time_index[frame].strftime(\"%Y-%m-%d %H:%M\") if isinstance(time_index[frame], pd.Timestamp) else str(time_index[frame])\n",
        "        title.set_text(f\"{fig_title}\\n{current_time_label}\")\n",
        "        return [scatter, title] + ax.images # Return all artists that were modified\n",
        "\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=len(weather_df), interval=interval, blit=False)\n",
        "    plt.close(fig)\n",
        "\n",
        "    if save:\n",
        "        ani.save(f\"{fig_title}.gif\", writer=\"pillow\", fps=3, dpi=150)\n",
        "\n",
        "    return HTML(ani.to_jshtml())\n",
        "\n",
        "import xarray as xr\n",
        "\n",
        "# Weather points\n",
        "# Slice the resampled ground station data to match the number of time steps in chirps_ds\n",
        "# num_chirps_timesteps = len(chirps_ds.time)\n",
        "# ground_data_for_plot = region_precip_data.resample('5D').sum().iloc[:num_chirps_timesteps]\n",
        "\n",
        "\n",
        "html_anim = point_plot(\n",
        "    tahmo_tmax,\n",
        "    info,\n",
        "    variable_name=\"Ground Temperature\", # This is the point data variable name\n",
        "    metadata_columns=['code', 'location.latitude', 'location.longitude'],\n",
        "    cmap=\"plasma\",\n",
        "    grid_da=era5_tmax,\n",
        "    grid_cmap=\"coolwarm\",\n",
        "    grid_alpha=0.5,\n",
        "    fig_title=\"Station Temperature vs ERA5 Background\",\n",
        "    grid_da_var='max_temperature',\n",
        ")\n",
        "\n",
        "html_anim"
      ],
      "metadata": {
        "id": "MFMIcmxE8Kgk"
      },
      "id": "MFMIcmxE8Kgk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Extract CBAM data and compare with ground data**"
      ],
      "metadata": {
        "id": "pTWIfawXQUE6"
      },
      "id": "pTWIfawXQUE6"
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations_with_replacement\n",
        "# @title Load CBAM data for the month of March\n",
        "# @markdown Load and compare with the TAHMO data\n",
        "# Data from 2018-2024\n",
        "cbam_eac = xr.open_dataset('/content/drive/Shareddrives/NOAA-workshop/Datasets/reanalysis/CBAM_temp2018_2024.nc')\n",
        "\n",
        "# Subset for march 2024\n",
        "# cbam_eac = cbam_eac.sel(date=slice('2024-03-01', '2024-03-31'))\n",
        "\n",
        "# # Agreegate the data from daiy to monthly\n",
        "# cbam_eac_monthly = cbam_eac.resample(time='M').mean()\n",
        "\n",
        "# cbam_eac_monthly\n",
        "\n",
        "\n",
        "# select the month of march 2024\n",
        "cbam_data = cbam_eac.sel(date=slice('2024-03-01', '2024-03-31')).sel(lat=slice(ymin, ymax), lon=slice(xmin, xmax))\n",
        "\n",
        "del cbam_eac\n",
        "\n",
        "# compute the avg_temperature from the min and max temperature by computing the sum and dividing by 2\n",
        "cbam_data['avg_temperature'] = (cbam_data['max_temperature'] + cbam_data['min_temperature']) / 2\n",
        "\n",
        "# subset to the region xmin, ymin, xmax, ymax\n",
        "\n",
        "# rename date to time\n",
        "cbam_data = cbam_data.rename({'date': 'time'})\n",
        "\n",
        "html_anim = point_plot(\n",
        "    tahmo_tmax,\n",
        "    info,\n",
        "    variable_name=\"Ground Temperature\", # This is the point data variable name\n",
        "    metadata_columns=['code', 'location.latitude', 'location.longitude'],\n",
        "    cmap=\"plasma\",\n",
        "    grid_da=cbam_data,\n",
        "    grid_cmap=\"coolwarm\",\n",
        "    grid_alpha=0.5,\n",
        "    fig_title=\"Station Temperature vs CBAM Background\",\n",
        "    grid_da_var='max_temperature'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "html_anim"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xMXf4WnSm2UW"
      },
      "id": "xMXf4WnSm2UW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Compare CBAM and ERA5**"
      ],
      "metadata": {
        "id": "7ba5Ys3wRRod"
      },
      "id": "7ba5Ys3wRRod"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CBAM vs ERA5 Comparison\n",
        "# @markdown This gives a visual comparison on the two datasets. <br>\n",
        "# @markdown ERA5 has a pixel grid of ~28km x 28km while CBAM ~4km x 4km <br>\n",
        "# @markdown We shall compare the tmin and tmax for ERA5 and CBAM\n",
        "\n",
        "# # subset to cbam min and max\n",
        "cbam_tmin = cbam_data['min_temperature'].to_dataset()\n",
        "cbam_tmax = cbam_data['max_temperature'].to_dataset()\n",
        "\n",
        "# convert lat lon to y, x\n",
        "def convert_lat_lon_to_xy(cbam_tmin, cbam_tmax, eera5_tmin, era5_tmax,\n",
        "                          lat_name='lat', lon_name='lon'):\n",
        "  # rename the datasets\n",
        "  cbam_tmin = cbam_tmin.rename({lat_name: 'y', lon_name: 'x'})\n",
        "  cbam_tmax = cbam_tmax.rename({lat_name: 'y', lon_name: 'x'})\n",
        "  eera5_tmin = eera5_tmin.rename({lat_name: 'y', lon_name: 'x'})\n",
        "  era5_tmax = era5_tmax.rename({lat_name: 'y', lon_name: 'x'})\n",
        "  return cbam_tmin, cbam_tmax, eera5_tmin, era5_tmax\n",
        "\n",
        "cbam_tmin, cbam_tmax, era5_tmin, era5_tmax = convert_lat_lon_to_xy(cbam_tmin, cbam_tmax, era5_tmin, era5_tmax)\n",
        "\n",
        "\n",
        "from utils.plotting import get_extent_from_xarray\n",
        "\n",
        "def plot_multiple_data(data_dict: dict, fig_title: str, plot_size: float = 5, robust: bool = False,\n",
        "                       cols: int = 2, bbox: list = None, polygon: list = None,\n",
        "                       fig_title_fontsize=12):\n",
        "    \"\"\"\n",
        "    Plot multiple xarray datasets in a grid layout with shared animation controls.\n",
        "\n",
        "    Args:\n",
        "        data_dict (dict): Dictionary where keys are titles and values are tuples.\n",
        "                          The tuple should be (data, norm, cmap) or (data, norm, cmap, custom_bbox).\n",
        "        fig_title (str): Main figure title.\n",
        "        plot_size (float): Base size for plot elements.\n",
        "        robust (bool): Whether to use robust scaling.\n",
        "        cols (int): Maximum number of columns in the grid.\n",
        "        bbox (list): Global bounding box [lon_min, lon_max, lat_min, lat_max] for subplots.\n",
        "        polygon (list): Global polygon coordinates for overlay (if bbox is not provided).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (ani, HTML) where ani is the FuncAnimation object and HTML is its HTML representation.\n",
        "    \"\"\"\n",
        "    import math\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.patches as mpatches\n",
        "    import cartopy.crs as ccrs\n",
        "    import cartopy.feature as cfeature\n",
        "    from matplotlib import animation\n",
        "    from IPython.display import HTML\n",
        "    import pandas as pd\n",
        "\n",
        "    # Calculate grid layout\n",
        "    num_plots = len(data_dict)\n",
        "    cols = min(cols, num_plots)\n",
        "    rows = math.ceil(num_plots / cols)\n",
        "\n",
        "    # Optional adjustment for specific numbers of plots\n",
        "    if num_plots in [5, 7]:\n",
        "        rows = math.ceil(num_plots / (cols - 1))\n",
        "        cols -= 1\n",
        "\n",
        "    # Create figure with custom spacing\n",
        "    fig = plt.figure(figsize=(plot_size * 2 * cols, plot_size * rows), constrained_layout=False)\n",
        "    # fig.suptitle(fig_title, fontsize=fig_title_fontsize, y=0.98)\n",
        "    fig.suptitle(fig_title, fontsize=fig_title_fontsize, y=1.02)\n",
        "\n",
        "    fig.subplots_adjust(left=0, right=0.7, top=0.80, bottom=0, wspace=0.01, hspace=0.1)\n",
        "\n",
        "    images = []\n",
        "    axes = []\n",
        "    precomputed = {}\n",
        "    max_steps = 1\n",
        "\n",
        "    # Loop through each subplot\n",
        "    for idx, (title, data_tuple) in enumerate(data_dict.items()):\n",
        "        # Support both 3-item and 4-item tuples.\n",
        "        if len(data_tuple) == 3:\n",
        "            data, norm, cmap = data_tuple\n",
        "            custom_bbox = None\n",
        "        elif len(data_tuple) == 4:\n",
        "            data, norm, cmap, custom_bbox = data_tuple\n",
        "        else:\n",
        "            raise ValueError(\"Data tuple must be (data, norm, cmap) or (data, norm, cmap, custom_bbox).\")\n",
        "\n",
        "        # Determine extent: per subplot if custom_bbox provided; otherwise use global bbox or polygon.\n",
        "        if custom_bbox is not None:\n",
        "            extent = custom_bbox  # Expected format: [lon_min, lon_max, lat_min, lat_max]\n",
        "        elif bbox is not None:\n",
        "            extent = bbox\n",
        "        elif polygon is not None:\n",
        "            lons = [coord[0] for coord in polygon]\n",
        "            lats = [coord[1] for coord in polygon]\n",
        "            extent = [min(lons), max(lons), min(lats), max(lats)]\n",
        "        else:\n",
        "            raise ValueError(\"Either a global bbox, polygon, or per-subplot custom bbox must be provided\")\n",
        "\n",
        "        # Create the subplot\n",
        "        ax = fig.add_subplot(rows, cols, idx + 1, projection=ccrs.PlateCarree())\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_title(title, fontsize=10)\n",
        "        ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
        "        ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)\n",
        "        ax.add_feature(cfeature.LAND, edgecolor='white')\n",
        "        ax.add_feature(cfeature.OCEAN)\n",
        "\n",
        "        # Optionally add a global polygon overlay if provided.\n",
        "        if polygon is not None:\n",
        "            poly_patch = mpatches.Polygon(polygon, closed=True, facecolor='none',\n",
        "                                          edgecolor='red', linewidth=2, transform=ccrs.PlateCarree())\n",
        "            ax.add_patch(poly_patch)\n",
        "\n",
        "        # Precompute time steps for the animation\n",
        "        time_steps = data.sizes.get(\"time\", 1)\n",
        "        max_steps = max(max_steps, time_steps)\n",
        "        precomputed[title] = [data.isel(time=t, missing_dims=\"ignore\") if time_steps > 1 else data\n",
        "                              for t in range(time_steps)]\n",
        "\n",
        "        extent = get_extent_from_xarray(precomputed[title][0])\n",
        "\n",
        "        im = ax.imshow(\n",
        "            precomputed[title][0],\n",
        "            norm=norm,\n",
        "            origin=\"lower\",\n",
        "            cmap=cmap,\n",
        "            transform=ccrs.PlateCarree(),\n",
        "            extent=extent\n",
        "        )\n",
        "\n",
        "        # Add colorbar with reduced padding\n",
        "        cbar = plt.colorbar(\n",
        "            im, ax=ax, orientation=\"vertical\", pad=0.03,\n",
        "            aspect=16, shrink=0.7, extend=(\"both\" if robust else \"neither\")\n",
        "        )\n",
        "        cbar.ax.tick_params(labelsize=8)\n",
        "\n",
        "        images.append(im)\n",
        "        axes.append(ax)\n",
        "\n",
        "    # Define the animation update function\n",
        "    def update(frame):\n",
        "        time_str = \"\"\n",
        "        for idx, (title, data_tuple) in enumerate(data_dict.items()):\n",
        "            if len(data_tuple) == 3:\n",
        "                data, _, _ = data_tuple\n",
        "            elif len(data_tuple) == 4:\n",
        "                data, _, _, _ = data_tuple\n",
        "            time_steps = data.sizes.get(\"time\", 1)\n",
        "            if time_steps > 1:\n",
        "                current_frame = min(frame, time_steps - 1)\n",
        "                images[idx].set_data(precomputed[title][current_frame])\n",
        "                time_str = pd.to_datetime(data[\"time\"][current_frame].item()).strftime('%Y-%m-%d')\n",
        "        if time_str:\n",
        "            fig.suptitle(f\"{fig_title}\\n{time_str}\", fontsize=16, y=0.98)\n",
        "        return images\n",
        "\n",
        "    # Create the animation object\n",
        "    ani = animation.FuncAnimation(\n",
        "        fig=fig,\n",
        "        func=update,\n",
        "        frames=max_steps,\n",
        "        interval=250,\n",
        "        blit=True\n",
        "    )\n",
        "\n",
        "    plt.close(fig)\n",
        "    return ani, HTML(ani.to_jshtml())\n",
        "\n",
        "def compare_xarray_datasets2(\n",
        "    datasets: list, labels: list, fig_title: str,\n",
        "    plot_size: float = 5, robust: bool = False,\n",
        "    cols: int = 2,\n",
        "    bboxes: list = None,\n",
        "    polygon: list = None,\n",
        "    save: bool = False,\n",
        "    cmap = \"coolwarm\"\n",
        ") -> HTML:\n",
        "\n",
        "    import numpy as np\n",
        "    from matplotlib import colors\n",
        "\n",
        "    data_for_plot = {}\n",
        "\n",
        "    # Validate bounding boxes\n",
        "    if bboxes is not None:\n",
        "        if len(bboxes) != len(datasets):\n",
        "            raise ValueError(\"Length of bboxes must match the number of datasets.\")\n",
        "    else:\n",
        "        bboxes = [None] * len(datasets)\n",
        "\n",
        "    # --- Step 1: Compute global min/max across all datasets ---\n",
        "    all_values = []\n",
        "    for ds in datasets:\n",
        "        var_names = list(ds.data_vars)\n",
        "        if len(var_names) != 1:\n",
        "            raise ValueError(f\"Dataset has {len(var_names)} variables; expected exactly one.\")\n",
        "        da = ds[var_names[0]]\n",
        "        valid_values = da.where(np.isfinite(da)).values.flatten()\n",
        "        all_values.extend(valid_values[~np.isnan(valid_values)])\n",
        "\n",
        "    global_min, global_max = np.nanmin(all_values), np.nanmax(all_values)\n",
        "    # print(f\"🌍 Global scale range applied to all plots: [{global_min:.2f}, {global_max:.2f}]\")\n",
        "\n",
        "    global_norm = colors.Normalize(vmin=global_min, vmax=global_max)\n",
        "    # cmap = \"coolwarm\"  # You can change this if you prefer another palette\n",
        "\n",
        "    # --- Step 2: Prepare each dataset for plotting with shared normalization ---\n",
        "    for (ds, label), bbox_for_ds in zip(zip(datasets, labels), bboxes):\n",
        "        var_name = list(ds.data_vars)[0]\n",
        "        data_array = ds[var_name]\n",
        "        data_for_plot[label] = (data_array, global_norm, cmap, bbox_for_ds)\n",
        "\n",
        "    # --- Step 3: Call the plot function with global color scale ---\n",
        "    ani, html_anim = plot_multiple_data(\n",
        "        data_for_plot,\n",
        "        fig_title,\n",
        "        plot_size=plot_size,\n",
        "        robust=robust,\n",
        "        cols=cols,\n",
        "        bbox=None,\n",
        "        polygon=polygon\n",
        "    )\n",
        "\n",
        "    # Optional: Save animation as GIF\n",
        "    if save:\n",
        "        ani.save(\n",
        "            f\"{fig_title.replace(' ', '_')}.gif\",\n",
        "            writer=\"pillow\", fps=4, dpi=300,\n",
        "            savefig_kwargs={\"facecolor\": \"white\"}\n",
        "        )\n",
        "\n",
        "    return html_anim\n",
        "\n",
        "\n",
        "compare_xarray_datasets2(\n",
        "    [era5_tmin, era5_tmax,\n",
        "     cbam_tmin, cbam_tmax],\n",
        "    labels=['ERA5 Minimum Temperature',\n",
        "            'ERA5 Maximum Temperature',\n",
        "            'CBAM Minimum Temperature',\n",
        "            'CBAM Maximum Temperature'],\n",
        "    fig_title='Comparison (ERA5 vs CBAM) - March 2024',\n",
        "    bboxes=[[xmin, ymin, xmax, ymax],\n",
        "            [xmin, ymin, xmax, ymax],\n",
        "            [xmin, ymin, xmax, ymax],\n",
        "            [xmin, ymin, xmax, ymax]],\n",
        "    save=False,\n",
        "    plot_size=3,\n",
        "    robust=True,\n",
        "    cols=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "iucp4rCz2Wyz",
        "cellView": "form"
      },
      "id": "iucp4rCz2Wyz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Compute PET and stress days with CBAM and ERA5**"
      ],
      "metadata": {
        "id": "6zcLtn2pRicl"
      },
      "id": "6zcLtn2pRicl"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ###  6a) Hargreaves Equation for Potential Evapotranspiration (PET)\n",
        "# @markdown The Hargreaves method estimates daily potential evapotranspiration (PET)\n",
        "# @markdown based on temperature range and incoming solar radiation:\n",
        "# @markdown\n",
        "# @markdown $$\n",
        "# @markdown \\text{PET} = 0.0023 \\times R_a \\times (T_{mean} + 17.8) \\times \\sqrt{T_{max} - T_{min}}\n",
        "# @markdown $$\n",
        "# @markdown\n",
        "# @markdown **Where:**\n",
        "# @markdown - $PET$: Potential Evapotranspiration (mm day⁻¹)\n",
        "# @markdown - $R_a$: Extraterrestrial radiation (MJ m⁻² day⁻¹)\n",
        "# @markdown - $T_{mean}$: Mean daily air temperature (°C)\n",
        "# @markdown - $T_{max}$: Maximum daily air temperature (°C)\n",
        "# @markdown - $T_{min}$: Minimum daily air temperature (°C)\n",
        "# @markdown\n",
        "# @markdown 💡 *In this function, a default value of $R_a = 15.0$ MJ m⁻² day⁻¹ is used* <br>\n",
        "\n",
        "# @markdown ### **Stress Condition Criteria**\n",
        "# @markdown A grid cell or station is considered under **heat stress** when:\n",
        "# @markdown\n",
        "# @markdown - $PET > 5$ mm day⁻¹  <br>\n",
        "# @markdown **and**\n",
        "# @markdown - Maximum temperature ($T_{max} > 32\\,°C$)\n",
        "\n",
        "def pet_hargreaves(tmin, tmax, tmean, Ra=15.0):\n",
        "    dtr = np.maximum(tmax - tmin, 0)\n",
        "    return 0.0023 * Ra * (tmean + 17.8) * np.sqrt(dtr)\n",
        "\n",
        "def rmse(a,b): return float(np.sqrt(np.nanmean((np.asarray(a)-np.asarray(b))**2)))\n",
        "\n",
        "pet_era5 = pet_hargreaves(era5_tmin.min_temperature, era5_tmax.max_temperature, era5_tavg.avg_temperature).to_dataset(name='pet')\n",
        "pet_cbam = pet_hargreaves(cbam_data['min_temperature'], cbam_data['max_temperature'], cbam_data['avg_temperature']).to_dataset(name='pet')\n",
        "\n",
        "# rename lat lon to x y for CBAM for consistency with plotting function\n",
        "pet_cbam = pet_cbam.rename({'lat': 'y', 'lon': 'x'})\n",
        "\n",
        "stress_cbam = (pet_cbam['pet'] > 5) & (cbam_data['max_temperature'] > 32)\n",
        "stress_era5 = (pet_era5['pet'] > 5) & (era5_tmax['max_temperature'] > 32)\n",
        "\n",
        "# convert the boolean results to integers (1 for True, 0 for False)\n",
        "stress_cbam = stress_cbam.astype(int).to_dataset(name='stress')\n",
        "stress_era5 = stress_era5.astype(int).to_dataset(name='stress')\n",
        "\n",
        "# @title ### Step 9b: Calculate and Visualize Stress Days\n",
        "# Calculate the total number of stress days for CBAM and ERA5\n",
        "total_stress_cbam = stress_cbam['stress'].sum().item()\n",
        "total_stress_era5 = stress_era5['stress'].sum().item()\n",
        "\n",
        "# Calculate the total number of possible stress points\n",
        "# This is the number of days multiplied by the number of spatial points\n",
        "total_possible_points_cbam = stress_cbam['stress'].size\n",
        "total_possible_points_era5 = stress_era5['stress'].size\n",
        "\n",
        "# Calculate percentages\n",
        "percentage_stress_cbam = (total_stress_cbam / total_possible_points_cbam) * 100\n",
        "percentage_stress_era5 = (total_stress_era5 / total_possible_points_era5) * 100\n",
        "\n",
        "print(f\"Total stress points for CBAM: {total_stress_cbam}/{total_possible_points_cbam} ({percentage_stress_cbam:.2f}%)\")\n",
        "print(f\"Total stress points for ERA5: {total_stress_era5}/{total_possible_points_era5} ({percentage_stress_era5:.2f}%)\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "veZZQab97em0"
      },
      "id": "veZZQab97em0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6b) Visualise Stress Days\n",
        "# @markdown Rerun this cell to skip to the next random date\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import matplotlib.colors as colors\n",
        "\n",
        "def plot_multiple_xarray_data(datasets, labels, fig_title='Xarray Data', cmap='viridis', save=False, bboxes=None):\n",
        "    \"\"\"\n",
        "    Plots multiple xarray DataArrays on separate subplots.\n",
        "\n",
        "    Args:\n",
        "        datasets (list): A list of xarray DataArrays to plot.\n",
        "        labels (list): A list of labels for each dataset (must match the order of datasets).\n",
        "        fig_title (str): Title for the overall figure.\n",
        "        cmap (str): Colormap to use for plotting.\n",
        "        save (bool): Whether to save the figure.\n",
        "        bboxes (list): A list of bounding boxes (xmin, ymin, xmax, ymax) for each subplot.\n",
        "                       If None, the extent is determined by the data.\n",
        "    \"\"\"\n",
        "    if len(datasets) != len(labels):\n",
        "        raise ValueError(\"The number of datasets and labels must be the same.\")\n",
        "\n",
        "    n_plots = len(datasets)\n",
        "    fig, axes = plt.subplots(1, n_plots, figsize=(6 * n_plots, 6),\n",
        "                             subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "\n",
        "    # Ensure axes is an array even for a single plot\n",
        "    if n_plots == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    # Define discrete colormap and normalization for stress data (0 or 1)\n",
        "    cmap_stress = colors.ListedColormap(['lightblue', 'red']) # Assuming 0 is no stress (lightblue) and 1 is stress (red)\n",
        "    bounds = [-0.5, 0.5, 1.5]\n",
        "    norm_stress = colors.BoundaryNorm(bounds, cmap_stress.N)\n",
        "\n",
        "\n",
        "    for i, ds in enumerate(datasets):\n",
        "        ax = axes[i]\n",
        "        label = labels[i]\n",
        "\n",
        "        # Handle potential time dimension and select the first time slice if present\n",
        "        if 'time' in ds.dims:\n",
        "            ds_to_plot = ds.isel(time=0)\n",
        "        else:\n",
        "            ds_to_plot = ds\n",
        "\n",
        "        # Add geographic features\n",
        "        ax.add_feature(cfeature.COASTLINE, linewidth=0.8)\n",
        "        ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.6)\n",
        "        ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
        "        ax.add_feature(cfeature.OCEAN, facecolor='aliceblue') # Add ocean feature\n",
        "\n",
        "        # Plot the data\n",
        "        if 'y' in ds_to_plot.coords and 'x' in ds_to_plot.coords:\n",
        "             # Assuming y and x correspond to lat and lon\n",
        "             # Use discrete colormap and normalization for stress data\n",
        "             im = ds_to_plot.plot(ax=ax, transform=ccrs.PlateCarree(), cmap=cmap_stress, norm=norm_stress, add_colorbar=False)\n",
        "        elif 'lat' in ds_to_plot.coords and 'lon' in ds_to_plot.coords:\n",
        "             # Use discrete colormap and normalization for stress data\n",
        "             im = ds_to_plot.plot(ax=ax, transform=ccrs.PlateCarree(), cmap=cmap_stress, norm=norm_stress, add_colorbar=False)\n",
        "        else:\n",
        "            raise ValueError(\"Dataset must have 'y' and 'x' or 'lat' and 'lon' coordinates.\")\n",
        "\n",
        "\n",
        "        # Set extent if bbox is provided\n",
        "        if bboxes and len(bboxes) > i:\n",
        "            bbox = bboxes[i]\n",
        "            ax.set_extent([bbox[0], bbox[2], bbox[1], bbox[3]], crs=ccrs.PlateCarree())\n",
        "        else:\n",
        "             # Set extent based on data coordinates with increased padding\n",
        "            pad = 2.0 # Increased padding to show a larger area\n",
        "            if 'y' in ds_to_plot.coords and 'x' in ds_to_plot.coords:\n",
        "                lon_min, lon_max = ds_to_plot.x.min().item() - pad, ds_to_plot.x.max().item() + pad\n",
        "                lat_min, lat_max = ds_to_plot.y.min().item() - pad, ds_to_plot.y.max().item() + pad\n",
        "                ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
        "            elif 'lat' in ds_to_plot.coords and 'lon' in ds_to_plot.coords:\n",
        "                lon_min, lon_max = ds_to_plot.lon.min().item() - pad, ds_to_plot.lon.max().item() + pad\n",
        "                lat_min, lat_max = ds_to_plot.lat.min().item() - pad, ds_to_plot.lat.max().item() + pad\n",
        "                ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
        "\n",
        "\n",
        "        ax.set_title(label)\n",
        "        ax.gridlines(draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
        "\n",
        "        # Add colorbar with discrete ticks\n",
        "        cbar = fig.colorbar(im, ax=ax, orientation='vertical', pad=0.05, shrink=0.7, ticks=[0, 1])\n",
        "        cbar.set_label('Stress (0: No, 1: Yes)', fontsize=10)\n",
        "\n",
        "\n",
        "    plt.suptitle(fig_title, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make space for suptitle\n",
        "    plt.show()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(f'{fig_title}.png')\n",
        "\n",
        "\n",
        "# Check if both datasets have stress points\n",
        "if total_stress_cbam > 0 and total_stress_era5 > 0:\n",
        "    print(\"Both CBAM and ERA5 have stress points greater than 0.\")\n",
        "\n",
        "    # Find days where stress points exist for both datasets\n",
        "    stress_days_cbam = stress_cbam['stress'].sum(dim=['y', 'x', 'lat', 'lon']) > 0\n",
        "    stress_days_era5 = stress_era5['stress'].sum(dim=['y', 'x']) > 0 # ERA5 stress is already aggregated spatially\n",
        "\n",
        "    # Find common stress days\n",
        "    common_stress_days_index = stress_days_cbam[stress_days_cbam & stress_days_era5].time.values\n",
        "\n",
        "    if len(common_stress_days_index) > 0:\n",
        "        # Randomly select one common stress day\n",
        "        random_stress_day = np.random.choice(common_stress_days_index)\n",
        "        print(f\"Randomly selected stress day: {pd.to_datetime(random_stress_day).strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # Select the data for the random stress day\n",
        "        stress_cbam_day = stress_cbam.sel(time=random_stress_day)\n",
        "        stress_era5_day = stress_era5.sel(time=random_stress_day)\n",
        "\n",
        "        # Plot the stress maps for the selected day using the new function\n",
        "        plot_multiple_xarray_data(\n",
        "            [stress_era5_day['stress'], stress_cbam_day['stress'].isel(lat=0, lon=0)], # Select a slice for plotting CBAM\n",
        "            labels=['ERA5 Stress', 'CBAM Stress'],\n",
        "            fig_title=f'Heat Stress (PET > 5 and Tmax > 32°C) - {pd.to_datetime(random_stress_day).strftime(\"%Y-%m-%d\")}',\n",
        "            cmap='Reds',\n",
        "            # bboxes=[[xmin, ymin, xmax, ymax], [xmin, ymin, xmax, ymax]], # Add bounding boxes if needed\n",
        "            save=False\n",
        "        )\n",
        "    else:\n",
        "        print(\"No common stress days found between CBAM and ERA5 with stress points.\")\n",
        "\n",
        "elif total_stress_cbam > 0:\n",
        "    print(\"CBAM has stress points greater than 0, but ERA5 does not.\")\n",
        "    # Find days where stress points exist for CBAM\n",
        "    stress_days_cbam = stress_cbam['stress'].sum(dim=['y', 'x', 'lat', 'lon']) > 0\n",
        "\n",
        "    if stress_days_cbam.any():\n",
        "        # Randomly select one stress day from CBAM\n",
        "        random_stress_day = np.random.choice(stress_days_cbam[stress_days_cbam].time.values)\n",
        "        print(f\"Randomly selected stress day for CBAM: {pd.to_datetime(random_stress_day).strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # Select the data for the random stress day\n",
        "        stress_cbam_day = stress_cbam.sel(time=random_stress_day)\n",
        "\n",
        "        # Plot the stress map for CBAM using the new function\n",
        "        plot_multiple_xarray_data(\n",
        "            [stress_cbam_day['stress'].isel(lat=0, lon=0)], # Select a slice for plotting CBAM\n",
        "            labels=['CBAM Stress'],\n",
        "            fig_title=f'Heat Stress (PET > 5 and Tmax > 32°C) - {pd.to_datetime(random_stress_day).strftime(\"%Y-%m-%d\")} (CBAM)',\n",
        "            cmap='Reds',\n",
        "            # bboxes=[[xmin, ymin, xmax, ymax]], # Add bounding boxes if needed\n",
        "            save=False\n",
        "        )\n",
        "\n",
        "elif total_stress_era5 > 0:\n",
        "     print(\"ERA5 has stress points greater than 0, but CBAM does not.\")\n",
        "     # Find days where stress points exist for ERA5\n",
        "     stress_days_era5 = stress_era5['stress'].sum(dim=['y', 'x']) > 0\n",
        "\n",
        "     if stress_days_era5.any():\n",
        "        # Randomly select one stress day from ERA5\n",
        "        random_stress_day = np.random.choice(stress_days_era5[stress_days_era5].time.values)\n",
        "        print(f\"Randomly selected stress day for ERA5: {pd.to_datetime(random_stress_day).strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # Select the data for the random stress day\n",
        "        stress_era5_day = stress_era5.sel(time=random_stress_day)\n",
        "\n",
        "        # Plot the stress map for ERA5 using the new function\n",
        "        plot_multiple_xarray_data(\n",
        "            [stress_era5_day['stress']],\n",
        "            labels=['ERA5 Stress'],\n",
        "            fig_title=f'Heat Stress (PET > 5 and Tmax > 32°C) - {pd.to_datetime(random_stress_day).strftime(\"%Y-%m-%d\")} (ERA5)',\n",
        "            cmap='Reds',\n",
        "            # bboxes=[[xmin, ymin, xmax, ymax]], # Add bounding boxes if needed\n",
        "            save=False\n",
        "        )\n",
        "else:\n",
        "    print(\"Neither CBAM nor ERA5 have stress points greater than 0.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nZMX_PupL9tm"
      },
      "id": "nZMX_PupL9tm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7: Visualise the heat change over the last half a century**"
      ],
      "metadata": {
        "id": "osunMuXJSBiF"
      },
      "id": "osunMuXJSBiF"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title This step loads ERA5 data from 1982 to 2024 and visualise the heat change over the years\n",
        "# # @title ERA5 extract data from 1982 -2024\n",
        "\n",
        "\n",
        "\n",
        "# import ee\n",
        "# import io\n",
        "# import os\n",
        "# import tempfile\n",
        "# import requests\n",
        "# import datetime\n",
        "# import numpy as np\n",
        "# import xarray as xr\n",
        "# import rasterio\n",
        "# from rasterio.io import MemoryFile\n",
        "# from rasterio.transform import xy as rio_xy\n",
        "\n",
        "# # Authenticate / initialize once (uncomment in interactive runtime)\n",
        "# # ee.Authenticate()\n",
        "# # ee.Initialize()\n",
        "\n",
        "# def era5_yearly_to_inmemory_netcdf(\n",
        "#     variable,\n",
        "#     start_year=1982,\n",
        "#     end_year=None,\n",
        "#     region_ee_geometry=None,\n",
        "#     dataset='ERA5_LAND',   # 'ERA5' or 'ERA5_LAND'\n",
        "#     cadence='monthly',       # 'hourly' or 'daily' or 'monthly'\n",
        "#     scale=None,            # meters (defaults used below)\n",
        "#     crs='EPSG:4326',\n",
        "#     save_local_copy=False, # also save .nc to local disk (path returned)\n",
        "#     local_folder='./',\n",
        "#     max_images_per_year=4000  # safety cutoff\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     For each year in [start_year, end_year], download the ERA5 images in that year,\n",
        "#     aggregate them according to the specified cadence, build a time-x-y-xarray dataset\n",
        "#     and write a NetCDF file for that year, then return the NetCDF as an in-memory\n",
        "#     BytesIO object.\n",
        "\n",
        "#     Returns:\n",
        "#         dict: { year (int) : { 'nc_bytes': io.BytesIO, 'local_path': str or None } }\n",
        "#     \"\"\"\n",
        "\n",
        "#     if end_year is None:\n",
        "#         end_year = datetime.datetime.utcnow().year\n",
        "\n",
        "#     # Dataset selection and default scale (meters)\n",
        "#     ds_upper = dataset.upper()\n",
        "#     if ds_upper == 'ERA5_LAND' or ds_upper == 'ERA5-LAND' or ds_upper == 'ERA5LAND':\n",
        "#         coll_hourly = 'ECMWF/ERA5_LAND/HOURLY'\n",
        "#         coll_daily = 'ECMWF/ERA5_LAND/DAILY_AGGR'\n",
        "#         default_scale = 11132\n",
        "#     elif ds_upper == 'ERA5':\n",
        "#         coll_hourly = 'ECMWF/ERA5/HOURLY'\n",
        "#         coll_daily = 'ECMWF/ERA5/DAILY'\n",
        "#         default_scale = 27830\n",
        "#     else:\n",
        "#         raise ValueError(\"dataset must be 'ERA5' or 'ERA5_LAND'\")\n",
        "\n",
        "#     if scale is None:\n",
        "#         scale = default_scale\n",
        "\n",
        "#     if region_ee_geometry is None:\n",
        "#         raise ValueError(\"region_ee_geometry (an ee.Geometry) is required (keep it small!)\")\n",
        "\n",
        "#     # turn region into a geojson / coordinates object for getDownloadURL\n",
        "#     # getInfo() here calls the server once\n",
        "#     region_geojson = region_ee_geometry.getInfo()\n",
        "\n",
        "#     results = {}\n",
        "\n",
        "#     for year in range(start_year, end_year + 1):\n",
        "#         print(f\"\\n--- Processing year {year} ---\")\n",
        "#         start_date_year = f'{year}-01-01'\n",
        "#         end_date_year = f'{year+1}-01-01'\n",
        "\n",
        "#         if cadence == 'hourly':\n",
        "#             coll = ee.ImageCollection(coll_hourly).filterDate(start_date_year, end_date_year).select(variable)\n",
        "#         elif cadence == 'daily':\n",
        "#             coll = ee.ImageCollection(coll_daily).filterDate(start_date_year, end_date_year).select(variable)\n",
        "#         elif cadence == 'monthly':\n",
        "#              # Process month by month for monthly aggregation\n",
        "#             monthly_images = []\n",
        "#             current_month_start = datetime.datetime.strptime(start_date_year, '%Y-%m-%d')\n",
        "#             while current_month_start.year == year:\n",
        "#                 next_month_start = (current_month_start.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)\n",
        "#                 coll_hourly_month = ee.ImageCollection(coll_hourly).filterDate(current_month_start, next_month_start).select(variable)\n",
        "#                 monthly_image = coll_hourly_month.mean() # Aggregate hourly to monthly mean\n",
        "#                 monthly_images.append(monthly_image.set('system:time_start', ee.Date(current_month_start)))\n",
        "#                 current_month_start = next_month_start\n",
        "#             coll = ee.ImageCollection(monthly_images)\n",
        "#         else:\n",
        "#             raise ValueError(\"cadence must be 'hourly', 'daily', or 'monthly'\")\n",
        "\n",
        "\n",
        "#         try:\n",
        "#             n_images = int(coll.size().getInfo())\n",
        "#         except Exception as e:\n",
        "#             raise RuntimeError(f\"Could not fetch collection size for {year}: {e}\")\n",
        "\n",
        "#         if n_images == 0:\n",
        "#             print(f\"No images found for {year} (variable '{variable}', cadence '{cadence}'). Skipping.\")\n",
        "#             continue\n",
        "\n",
        "#         if n_images > max_images_per_year and cadence != 'monthly': # Allow more images for monthly aggregation\n",
        "#              raise RuntimeError(f\"Year {year} has {n_images} images > max_images_per_year ({max_images_per_year}). Aborting for safety.\")\n",
        "\n",
        "#         print(f\"Found {n_images} images for {year}. Downloading each to memory (this may be slow).\")\n",
        "\n",
        "#         # Build lists to stack\n",
        "#         img_arrays = []\n",
        "#         times = []\n",
        "#         ref_shape = None\n",
        "#         ref_transform = None\n",
        "#         ref_crs = None\n",
        "\n",
        "#         # Convert collection to server list and iterate\n",
        "#         coll_list = coll.toList(n_images)\n",
        "\n",
        "#         for i in range(n_images):\n",
        "#             ee_img = ee.Image(coll_list.get(i))\n",
        "#             # time string\n",
        "#             try:\n",
        "#                 time_start_ms = ee.Date(ee_img.get('system:time_start')).getInfo()['value']\n",
        "#                 time_str = datetime.datetime.fromtimestamp(time_start_ms / 1000.0).strftime('%Y-%m-%d')\n",
        "#             except Exception:\n",
        "#                 # fallback: use index-based date\n",
        "#                 time_str = f'{year}-unknown-{i}'\n",
        "#             print(f\"  - image {i+1}/{n_images} date {time_str} ...\", end=' ', flush=True)\n",
        "\n",
        "#             # Request a GeoTIFF download URL (format GEO_TIFF to get raw .tif bytes)\n",
        "#             params = {\n",
        "#                 'bands': [variable],\n",
        "#                 'region': region_geojson,   # geojson-like mapping or coordinates (small)\n",
        "#                 'scale': int(scale),\n",
        "#                 'format': 'GEO_TIFF',\n",
        "#                 'filePerBand': False\n",
        "#             }\n",
        "\n",
        "#             try:\n",
        "#                 url = ee_img.getDownloadURL(params)\n",
        "#             except Exception as e:\n",
        "#                 raise RuntimeError(f\"getDownloadURL failed for {year} image idx {i}: {e}\")\n",
        "\n",
        "#             # Download bytes (may be zipped or raw GeoTIFF depending on params; we asked GEO_TIFF)\n",
        "#             r = requests.get(url, timeout=600)\n",
        "#             if r.status_code != 200:\n",
        "#                 raise RuntimeError(f\"HTTP error {r.status_code} when downloading image: {r.text[:200]}\")\n",
        "\n",
        "#             # Load into rasterio MemoryFile\n",
        "#             with MemoryFile(r.content) as mem:\n",
        "#                 with mem.open() as src:\n",
        "#                     arr = src.read(1)           # single-band image\n",
        "#                     transform = src.transform\n",
        "#                     crs_src = src.crs\n",
        "#                     h, w = src.height, src.width\n",
        "\n",
        "#             # check shape consistency\n",
        "#             if ref_shape is None:\n",
        "#                 ref_shape = (h, w)\n",
        "#                 ref_transform = transform\n",
        "#                 ref_crs = crs_src\n",
        "#             else:\n",
        "#                 if (h, w) != ref_shape:\n",
        "#                     raise RuntimeError(f\"Image {i} shape {h,w} differs from first image shape {ref_shape}. Reprojection/resampling not implemented - aborting.\")\n",
        "\n",
        "#             img_arrays.append(arr)\n",
        "#             times.append(np.datetime64(time_str))\n",
        "#             print(\"OK\")\n",
        "\n",
        "#         # Stack into ndarray (time, y, x)\n",
        "#         data_stack = np.stack(img_arrays, axis=0)  # shape (time, H, W)\n",
        "#         print(f\"Stacked data: {data_stack.shape}\")\n",
        "\n",
        "#         # Build coordinate vectors from transform\n",
        "#         height, width = ref_shape\n",
        "#         # x coords (cols)\n",
        "#         xs = np.array([rio_xy(ref_transform, 0, col, offset='center')[0] for col in range(width)])\n",
        "#         # y coords (rows) - note rasterio returns y per (row, col); rows increase downward\n",
        "#         ys = np.array([rio_xy(ref_transform, row, 0, offset='center')[1] for row in range(height)])\n",
        "\n",
        "#         # xarray DataArray\n",
        "#         da = xr.DataArray(\n",
        "#             data_stack,\n",
        "#             dims=('time', 'y', 'x'),\n",
        "#             coords={'time': times, 'y': ys, 'x': xs},\n",
        "#             name=variable\n",
        "#         )\n",
        "\n",
        "#         ds = xr.Dataset({variable: da})\n",
        "#         ds.attrs['source'] = f\"GEE {coll_daily} ({dataset})\" # Note: still using daily collection ID in source attr\n",
        "#         # Convert region_geojson to a string for NetCDF compatibility\n",
        "#         ds.attrs['region'] = json.dumps(region_geojson)\n",
        "#         ds.attrs['scale_m'] = scale\n",
        "\n",
        "#         # Persist to a temporary netCDF file, then load bytes into memory\n",
        "#         tmpf = tempfile.NamedTemporaryFile(suffix=f\"_{variable}_{year}_{cadence}.nc\", delete=False)\n",
        "#         tmpf.close()\n",
        "#         try:\n",
        "#             ds.to_netcdf(tmpf.name, engine='netcdf4')\n",
        "#         except Exception as e:\n",
        "#             os.unlink(tmpf.name)\n",
        "#             raise RuntimeError(f\"Failed to write NetCDF for year {year}: {e}\")\n",
        "\n",
        "#         # Read bytes into memory BytesIO\n",
        "#         with open(tmpf.name, 'rb') as f:\n",
        "#             nc_bytes = f.read()\n",
        "\n",
        "#         # Optionally save a local persistent copy\n",
        "#         local_path = None\n",
        "#         if save_local_copy:\n",
        "#             os.makedirs(local_folder, exist_ok=True)\n",
        "#             local_path = os.path.join(local_folder, f\"{variable}_{year}_{cadence}.nc\")\n",
        "#             with open(local_path, 'wb') as f:\n",
        "#                 f.write(nc_bytes)\n",
        "\n",
        "#         # Cleanup temp file\n",
        "#         os.unlink(tmpf.name)\n",
        "\n",
        "#         results[year] = {'nc_bytes': io.BytesIO(nc_bytes), 'local_path': local_path}\n",
        "\n",
        "#         print(f\"Year {year} done: NetCDF in memory ({len(nc_bytes)/1e6:.2f} MB).\")\n",
        "\n",
        "#     return results\n",
        "\n",
        "\n",
        "# roi = ee.Geometry.Polygon(eac_region)\n",
        "\n",
        "# out = era5_yearly_to_inmemory_netcdf(\n",
        "#     variable='temperature_2m',\n",
        "#     start_year=1982,\n",
        "#     end_year=2024,\n",
        "#     region_ee_geometry=roi,\n",
        "#     dataset='ERA5',        # or 'ERA5_LAND'\n",
        "#     cadence='monthly',\n",
        "#     scale=27830,           # use native-ish scale for ERA5 (meters)\n",
        "#     save_local_copy=False\n",
        "# )\n",
        "\n",
        "# # Access the NetCDF bytes for 2023:\n",
        "# nc_bytesio = out[2023]['nc_bytes']      # io.BytesIO\n",
        "# # To load into xarray directly from memory:\n",
        "# nc_bytesio.seek(0)\n",
        "# ds = xr.open_dataset(nc_bytesio)\n",
        "# print(ds)\n",
        "\n",
        "\n",
        "# load the data\n",
        "era5_long_term = xr.open_dataset('/content/drive/Shareddrives/NOAA-workshop/Datasets/reanalysis/era5/era5_temperature_monthly_1982_2024_combined.nc')\n",
        "\n",
        "# subset to the xmin max\n",
        "# era5_long_term = era5_long_term.sel(x=slice(ymin, ymax), y=slice(xmin, xmax))\n",
        "\n",
        "# Subtract 273.15 from the data\n",
        "era5_long_term['temperature_2m'] = era5_long_term['temperature_2m'] - 273.15\n",
        "\n",
        "\n",
        "# @title Plot ERA5 Monthly Temperature Heatmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def plot_era5_monthly_heatmap(era5_data, variable='temperature_2m', cmap='coolwarm', fig_title=\"Monthly Average Temperature Heatmap (ERA5)\"):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of monthly average temperature for ERA5 data over the years.\n",
        "\n",
        "    Args:\n",
        "        era5_data (xr.Dataset): Xarray Dataset containing monthly ERA5 temperature data.\n",
        "        variable (str): The variable to plot from the ERA5 dataset (e.g., 'temperature_2m').\n",
        "        cmap (str): Colormap to use for the heatmap.\n",
        "        fig_title (str): Title of the heatmap.\n",
        "    \"\"\"\n",
        "    # Calculate the monthly average temperature over the spatial dimensions\n",
        "    monthly_mean_temp = era5_data[variable].mean(dim=['y', 'x'])\n",
        "\n",
        "    # Convert to pandas Series for easier reshaping\n",
        "    monthly_mean_temp_series = monthly_mean_temp.to_series()\n",
        "\n",
        "    # Create a MultiIndex from the time index for unstacking into Year x Month\n",
        "    monthly_mean_temp_series.index = pd.MultiIndex.from_arrays([\n",
        "        monthly_mean_temp_series.index.year,\n",
        "        monthly_mean_temp_series.index.month\n",
        "    ], names=['year', 'month'])\n",
        "\n",
        "    # Reshape for heatmap (Year as columns, Month as index)\n",
        "    heatmap_data = monthly_mean_temp_series.unstack(level='year')\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(heatmap_data, cmap=cmap, cbar_kws={'label': f'Average {variable} (°C)'})\n",
        "    plt.title(fig_title)\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Month')\n",
        "    plt.yticks(ticks=np.arange(12) + 0.5, labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the plotting function with the loaded ERA5 long-term data\n",
        "plot_era5_monthly_heatmap(era5_long_term)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nP-4yRRNGeMb"
      },
      "id": "nP-4yRRNGeMb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}