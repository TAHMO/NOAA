{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43eda2f0",
      "metadata": {
        "id": "43eda2f0"
      },
      "source": [
        "# Focus Area 3 — Temperature Quality &amp; Microclimates\n",
        "**Core Objective**: To demonstrate the advantages of high-resolution temperature data in\n",
        "capturing microclimates and computing derived metrics like PET, for better assessment of heat-\n",
        "related risks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa97e7ce",
      "metadata": {
        "id": "fa97e7ce"
      },
      "source": [
        "## Temperature data\n",
        "- GHCN data\n",
        "- CBAM data\n",
        "- ERA5 data\n",
        "- TAHMO data\n",
        "<br>\n",
        "\n",
        "EA: March 2024 heatwave\n",
        "\n",
        "\n",
        "Choose the current location and get the nearest GHCNd weather station and visualise the temperature over the last half a century\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6df409a",
      "metadata": {
        "id": "f6df409a"
      },
      "source": [
        "Require 2 files\n",
        "- The Metadata file: Ground_Metadata.csv\n",
        "- The Ground_station data file: Ground_data.csv\n",
        "\n",
        "For TAHMO data we shall extract the data during this workshop period.\n",
        "\n",
        "Metadata file format (Columns):\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>TAHMO Metadata</title>\n",
        "</head>\n",
        "<body>\n",
        "    <table border=\"1\">\n",
        "        <tr>\n",
        "            <th>Code</th>\n",
        "            <th>lat</th>\n",
        "            <th>lon</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>TA00283</td>\n",
        "            <td>1.2345</td>\n",
        "            <td>36.7890</td>\n",
        "        </tr>\n",
        "        <!-- More rows as needed -->\n",
        "    </table>\n",
        "</html>\n",
        "\n",
        "Data file format (Columns): Temperature / Precipitation data for multiple stations\n",
        "<html>\n",
        "<head>\n",
        "    <title>TAHMO Data</title>\n",
        "</head>\n",
        "<body>\n",
        "    <table border=\"1\">\n",
        "        <tr>\n",
        "            <th>Date</th>\n",
        "            <th>TA00283</th>\n",
        "            <th>TA00284</th>\n",
        "            <th>TA00285</th>\n",
        "            <!-- More station codes as needed -->\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>2023-01-01</td>\n",
        "            <td>25.3</td>\n",
        "            <td>26.1</td>\n",
        "            <td>24.8</td>\n",
        "        </tr>\n",
        "        <!-- More rows as needed -->\n",
        "    </table>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps Breakdown\n",
        "- Step 1: Setting up environment and Authentication\n",
        "\n",
        "- Step2: Select country between Kenya, Uganda and Rwanda\n",
        "- Step 3: Extract TAHMO temperature data and get to visualise the data\n",
        "- Step 4: Detect flatlines in the temperature data\n",
        "- Step 5: Extract ERA5 data and compare with ground data\n",
        "- Step 6: Extract CBAM data and compare with ground data\n",
        "- Step 7: A comparison of CBAM and ERA5 (Get to look at the granularity)\n",
        "- Step 8: Extract GHCNd temperature data and visualise the nearest station from the capital\n",
        "- Step 9: Compute PET and stress days with CBAM and ERA5 over March\n",
        "- Step 10: Visualise the heat change over the last half a century with GHCNd and ERA5\n"
      ],
      "metadata": {
        "id": "NpbU68UkNNRY"
      },
      "id": "NpbU68UkNNRY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XWw3IATm-_J9",
      "metadata": {
        "id": "XWw3IATm-_J9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Step 1a: Setting up environment installing required Dependencies\n",
        "# @markdown This cell installs the required dependencies for the workshop. It may take a few minutes <br>\n",
        "# @markdown If you encounter any errors, please restart the runtime and try again. <br>\n",
        "# @markdown If the error persists, please seek help.\n",
        "\n",
        "\n",
        "print(\"Installing required dependencies...\")\n",
        "!pip install git+https://github.com/TAHMO/NOAA.git > /dev/null 2>&1\n",
        "\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "\n",
        "# check there was no error\n",
        "import sys\n",
        "if not sys.argv[0].endswith(\"kernel_launcher.py\"):\n",
        "    print(\"❌ Errors occurred during installation. Please restart the runtime and try again.\")\n",
        "else:\n",
        "    print(\"✅ Dependencies installed successfully.\")\n",
        "\n",
        "print(\"Importing required libraries...\")\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import ee\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr, ttest_rel\n",
        "import random\n",
        "\n",
        "# import os\n",
        "# os.chdir('NOAA-workshop')\n",
        "\n",
        "from utils.ground_stations import plot_stations_folium\n",
        "from utils.helpers import get_region_geojson\n",
        "from utils.CHIRPS_helpers import get_chirps_pentad_gee\n",
        "from utils.CBAM_helpers import CBAMClient, extract_cbam_data # CBAM helper functions\n",
        "from utils.plotting import select, scale, plot_xarray_data, plot_xarray_data2, compare_xarray_datasets, compare_xarray_datasets2 # Plotting helper functionsfrom utils.IMERG_helpers import get_imerg_raw\n",
        "from utils.ERA5_helpers import era5_data_extracts, era5_var_handling\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import pandas as pd\n",
        "import json\n",
        "import ee\n",
        "from scipy.stats import pearsonr\n",
        "import seaborn as sns\n",
        "from utils.filter_stations import RetrieveData\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"✅ Libraries imported successfully.\")\n",
        "\n",
        "def build_xr_from_stations(ds, stations_metadata, var_name=None):\n",
        "    # Auto-detect variable if not provided\n",
        "    if var_name is None:\n",
        "        candidate_vars = ['total_precipitation', 'total_rainfall', 'precipitation']\n",
        "        found = [v for v in candidate_vars if v in ds.data_vars]\n",
        "        if not found:\n",
        "            raise ValueError(f\"None of expected precipitation variable names {candidate_vars} found in dataset vars: {list(ds.data_vars)}\")\n",
        "        var_name = found[0]\n",
        "\n",
        "    # Determine dimension names\n",
        "    if {'x', 'y'}.issubset(ds.dims):\n",
        "        lon_dim, lat_dim = 'x', 'y'\n",
        "    elif {'lon', 'lat'}.issubset(ds.dims):\n",
        "        lon_dim, lat_dim = 'lon', 'lat'\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset dims {list(ds.dims)} do not contain expected (x,y) or (lon,lat).\")\n",
        "\n",
        "    all_stations_data = {}\n",
        "    for _, row in stations_metadata.iterrows():\n",
        "        station_code = row['code']\n",
        "        lat = float(row['lat'])\n",
        "        lon = float(row['lon'])\n",
        "        # Skip stations outside domain (quick bounds check)\n",
        "        if not (ds[lon_dim].min() <= lon <= ds[lon_dim].max() and ds[lat_dim].min() <= lat <= ds[lat_dim].max()):\n",
        "            continue\n",
        "        station_da = ds[var_name].sel({lon_dim: lon, lat_dim: lat}, method=\"nearest\")\n",
        "        station_df = station_da.to_dataframe(name=station_code)\n",
        "        all_stations_data[station_code] = station_df[station_code]\n",
        "\n",
        "    combined_df = pd.DataFrame(all_stations_data)\n",
        "    return combined_df\n",
        "\n",
        "\n",
        "\n",
        "def plot_temperatures(tmin_df, tavg_df, tmax_df, station_code=None):\n",
        "    \"\"\"\n",
        "    Plots the daily minimum, average, and maximum temperatures for a specified TAHMO station.\n",
        "\n",
        "    Args:\n",
        "        tmin_df (pd.DataFrame): DataFrame containing daily minimum temperatures.\n",
        "        tavg_df (pd.DataFrame): DataFrame containing daily average temperatures.\n",
        "        tmax_df (pd.DataFrame): DataFrame containing daily maximum temperatures.\n",
        "        station_code (str, optional): The code of the station to plot. If None, a random station from the DataFrame is plotted.\n",
        "    \"\"\"\n",
        "    if station_code is None:\n",
        "        station_code = random.choice(tmin_df.columns.tolist())\n",
        "        print(f\"Randomly selected station: {station_code}\")\n",
        "    elif station_code not in tmin_df.columns:\n",
        "        print(f\"Station code {station_code} not found in the data.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(tmin_df.index, tmin_df[station_code], label='Min Temp', linestyle='-')\n",
        "    plt.plot(tavg_df.index, tavg_df[station_code], label='Avg Temp', linestyle='-')\n",
        "    plt.plot(tmax_df.index, tmax_df[station_code], label='Max Temp', linestyle='-')\n",
        "\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Temperature (°C)')\n",
        "    plt.title(f'Daily Temperatures for Station {station_code}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Step 1b: Authentication Step\n",
        "# @markdown This step is used to authenticate you as a user and there will be a popups that will be doing this.\n",
        "# @markdown 1. **Authentication to Google Drive** - This is where we shall be loading the data after we have extracted it\n",
        "# his workshop, we have created the ```noaa-tahmo``` project that you can input as your project id<br><br><br>\n",
        "\n",
        "print(\"Authenticating to Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"✅ Google Drive authenticated successfully.\")\n",
        "\n",
        "# import ee\n",
        "\n",
        "# # Authenticate and initialise Google Earth Engine\n",
        "# # This will open a link in your browser to grant permissions if necessary.\n",
        "# try:\n",
        "#     print(\"Authenticating Google Earth Engine. Please follow the instructions in your browser.\")\n",
        "#     ee.Authenticate()\n",
        "#     print(\"✅ Authentication successful.\")\n",
        "# except ee.auth.scopes.MissingScopeError:\n",
        "#     print(\"Authentication scopes are missing. Please re-run the cell and grant the necessary permissions.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Authentication failed: {e}\")\n",
        "\n",
        "# # Initialize Earth Engine with your project ID\n",
        "# # Replace 'your-project-id' with your actual Google Cloud Project ID\n",
        "# # You need to create an unpaid project manually through the Google Cloud Console\n",
        "# print(\"\\nIf you already have a project id paste it below. If you do not have a project You need to create an unpaid project manually through the Google Cloud Console\")\n",
        "# print(\"💡 You can create a new project here: https://console.cloud.google.com/projectcreate and copy the project id\")\n",
        "# try:\n",
        "#     # It's recommended to use a project ID associated with your Earth Engine account.\n",
        "#     print(\"\\nEnter your Google Cloud Project ID: \")\n",
        "#     project_id = input(\"\")\n",
        "#     ee.Initialize(project=project_id)\n",
        "#     print(\"✅ Google Earth Engine initialized successfully.\")\n",
        "# except ee.EEException as e:\n",
        "#     if \"PERMISSION_DENIED\" in str(e):\n",
        "#         print(f\"Earth Engine initialization failed due to PERMISSION_DENIED.\")\n",
        "#         print(\"Please ensure the Earth Engine API is enabled for your project:\")\n",
        "#         print(\"Enable the Earth Engine API here: https://console.developers.google.com/apis/api/earthengine.googleapis.com/overview?project=elated-capsule-471808-k1\")\n",
        "#     else:\n",
        "#         print(f\"Earth Engine initialization failed: {e}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An unexpected error occurred during initialization: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lY_TL02ishPn"
      },
      "id": "lY_TL02ishPn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A config file is provided with the api keys to access TAHMO Data\n",
        "```json\n",
        "{\n",
        "    \"apiKey\": \"\",\n",
        "    \"apiSecret\": \"\",\n",
        "    \"location_keys\": \"\",\n",
        "    \"cbam_username\" : \"\",\n",
        "    \"cbam_password\" : \"\"\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "Z0C8VQhrM-Ul"
      },
      "id": "Z0C8VQhrM-Ul"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1c: Loading the config files\n",
        "config_file_path = '/content/drive/Shareddrives/NOAA-workshop2/config.json'\n",
        "\n",
        "# check if path exists\n",
        "if not os.path.exists(config_file_path):\n",
        "    print(\"❌ Config file not found. Please upload it first.\")\n",
        "else:\n",
        "    print(\"✅ Config file loaded successfully.\")\n",
        "\n",
        "# Loading the config file and parsing from uploaded incase it comes with a different name\n",
        "import json\n",
        "with open(config_file_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dXJBF3FUM4B4"
      },
      "id": "dXJBF3FUM4B4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2a: Select country\n",
        "# @markdown This will be the country that we will get the data\n",
        "\n",
        "\n",
        "import time\n",
        "import json\n",
        "import plotly.graph_objects as go\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Polygon\n",
        "import sys\n",
        "import importlib\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# --- Environment Detection ---\n",
        "def in_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "IS_COLAB = in_colab()\n",
        "print(f\"💡 Running in {'Google Colab' if IS_COLAB else 'Local Jupyter'} environment.\")\n",
        "\n",
        "try:\n",
        "    with open('/content/config.json', 'r') as f:\n",
        "        config = json.load(f)\n",
        "    location_key = config.get('location_keys', None)\n",
        "except Exception:\n",
        "    location_key = None\n",
        "    # print(\"⚠️ Warning: No API key found. Fallback modes will be used.\")\n",
        "\n",
        "\n",
        "\n",
        "# Define the dropdown widget\n",
        "country_dropdown = widgets.Dropdown(\n",
        "    options=['Kenya', 'Uganda', 'Rwanda'],\n",
        "    value='Kenya',\n",
        "    description='Select Country:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "# Reactive variable to store selection\n",
        "region_query = country_dropdown.value\n",
        "region_query = region_query.lower()\n",
        "\n",
        "def on_country_change(change):\n",
        "    \"\"\"Trigger downstream updates when user changes country selection\"\"\"\n",
        "    global region_query\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        region_query = change['new']\n",
        "        print(f\"🌍 Country selected: {region_query}\")\n",
        "\n",
        "# Bind the event listener\n",
        "country_dropdown.observe(on_country_change)\n",
        "\n",
        "# Display the widget\n",
        "display(country_dropdown)\n",
        "\n",
        "print(\"💡 Use the dropdown above to select your country of interest.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7JvxHhHtNEQ7"
      },
      "id": "7JvxHhHtNEQ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2b: Visualise your selected region\n",
        "def xmin_ymin_xmax_ymax(polygon):\n",
        "    lons = [pt[0] for pt in polygon]\n",
        "    lats = [pt[1] for pt in polygon]\n",
        "    return min(lons), min(lats), max(lons), max(lats)\n",
        "\n",
        "def fetch_region_google(query):\n",
        "    \"\"\"Primary: Fetch polygon geometry via Google Maps API\"\"\"\n",
        "    if not location_key:\n",
        "        raise RuntimeError(\"Missing Google Maps API key.\")\n",
        "    region_geom = get_region_geojson(query, location_key)['geometry']['coordinates'][0]\n",
        "    return region_geom\n",
        "\n",
        "def fetch_region_osm(query):\n",
        "    \"\"\"Fallback: Fetch geometry from OSM (Nominatim) via GeoPandas\"\"\"\n",
        "    url = f\"https://nominatim.openstreetmap.org/search?country={query}&format=geojson&polygon_geojson=1\"\n",
        "    gdf = gpd.read_file(url)\n",
        "    if gdf.empty:\n",
        "        raise ValueError(\"No OSM data found for that query.\")\n",
        "    geom = gdf.iloc[0].geometry\n",
        "    if geom.geom_type == \"Polygon\":\n",
        "        return list(geom.exterior.coords)\n",
        "    elif geom.geom_type == \"MultiPolygon\":\n",
        "        return list(list(geom.geoms)[0].exterior.coords)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported geometry type from OSM.\")\n",
        "\n",
        "def draw_region_interactively():\n",
        "    \"\"\"Manual fallback: let the user draw their ROI\"\"\"\n",
        "    print(\"🖱️ Draw your region on the map (double-click to finish).\")\n",
        "\n",
        "    if IS_COLAB:\n",
        "        # ✅ Folium backend (Colab-compatible)\n",
        "        import geemap.foliumap as geemap\n",
        "        from geemap.foliumap import plugins\n",
        "\n",
        "        m = geemap.Map(center=[0, 20], zoom=3)\n",
        "        draw = plugins.Draw(export=True)\n",
        "        draw.add_to(m)\n",
        "        m.add_child(plugins.Fullscreen())\n",
        "        m.add_child(plugins.MeasureControl(primary_length_unit='kilometers'))\n",
        "        m  # Display map in Colab output cell\n",
        "\n",
        "        print(\"✅ Use the draw tools on the left to mark your region.\")\n",
        "        print(\"💾 After drawing, click 'Export' to download your GeoJSON.\")\n",
        "        return m\n",
        "\n",
        "    else:\n",
        "        # ✅ ipyleaflet backend (Local Jupyter)\n",
        "        import geemap\n",
        "        m = geemap.Map(center=[0, 20], zoom=3)\n",
        "        m.add_draw_control()\n",
        "        display(m)\n",
        "        print(\"✅ After drawing, access your shape via `m.user_rois`.\")\n",
        "        return m\n",
        "\n",
        "\n",
        "def show_region_plotly(polygon, region_name=\"Region\", margin=0.05):\n",
        "    \"\"\"Plot polygon with Plotly Mapbox\"\"\"\n",
        "    lons = [pt[0] for pt in polygon]\n",
        "    lats = [pt[1] for pt in polygon]\n",
        "    fig = go.Figure(go.Scattermapbox(\n",
        "        lon=lons + [lons[0]],\n",
        "        lat=lats + [lats[0]],\n",
        "        mode=\"lines\",\n",
        "        fill=\"toself\",\n",
        "        fillcolor=\"rgba(0,0,255,0.3)\",\n",
        "        name=region_name\n",
        "    ))\n",
        "    fig.update_layout(\n",
        "        mapbox_style=\"open-street-map\",\n",
        "        mapbox=dict(center={\"lat\": sum(lats)/len(lats), \"lon\": sum(lons)/len(lons)}, zoom=5),\n",
        "        margin=dict(r=0, t=30, l=0, b=0),\n",
        "        title=f\"Region of Interest: {region_name}\",\n",
        "        height=500,\n",
        "        width=900\n",
        "    )\n",
        "    fig.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "region_geom = None\n",
        "try:\n",
        "    region_geom = fetch_region_google(region_query)\n",
        "    print(f\"✅ Geometry fetched via Google Maps API for {region_query}\")\n",
        "except Exception as e1:\n",
        "    # print(f\"⚠️ Google Maps API failed: {e1}\")\n",
        "    try:\n",
        "        region_geom = fetch_region_osm(region_query)\n",
        "        print(f\"✅ Geometry fetched via OpenStreetMap for {region_query.title()}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"⚠️ OSM fallback failed: {e2}\")\n",
        "        print(\"🔁 Launching interactive map draw mode...\")\n",
        "        map_widget = draw_region_interactively()\n",
        "\n",
        "if region_geom:\n",
        "    xmin, ymin, xmax, ymax = xmin_ymin_xmax_ymax(region_geom)\n",
        "    show_region_plotly(region_geom, region_name=region_query)\n",
        "    print(f\"📦 Bounding box -> xmin: {xmin}, ymin: {ymin}, xmax: {xmax}, ymax: {ymax}\")\n",
        "else:\n",
        "    print(\"🛑 No geometry available. Please draw manually or retry another query.\")\n",
        "\n",
        "region_query = region_query.lower()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vBpSJl9Kf_XS"
      },
      "id": "vBpSJl9Kf_XS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction"
      ],
      "metadata": {
        "id": "BAxHyuqb4Z6H"
      },
      "id": "BAxHyuqb4Z6H"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3a: Extract TAHMO Metadata\n",
        "# @title Step 3a: Metadata Extraction/Loading and Visualisation\n",
        "# @markdown At this step we shall continue storing the datasets we keep on extracting on Google Drive to easily access and minimize API requests<br>\n",
        "\n",
        "\n",
        "from utils.filter_stations import RetrieveData\n",
        "import os\n",
        "import time\n",
        "\n",
        "dir_path = '/content/drive/MyDrive/NOAA-workshop-data'\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "# check if the path was created successfully\n",
        "if not os.path.exists(dir_path):\n",
        "    print(\"❌ Path not created successfully.\")\n",
        "else:\n",
        "    print(\"✅ Path created successfully.\")\n",
        "\n",
        "# check if the config exists\n",
        "# if not os.path.exists('/content/config.json'):\n",
        "#     print(\"❌ Config file not found. Please upload it first.\")\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "def plot_stations_plotly(dataframes, colors=None, zoom=5, height=500,\n",
        "                         width=900, legend_title='Station Locations', ghcnd_coords=False):\n",
        "    \"\"\"\n",
        "    Plot stations from one or more dataframes on a Plotly mapbox.\n",
        "\n",
        "    Each dataframe must have 'location.latitude' and 'location.longitude' columns.\n",
        "    'colors' is a list specifying marker colors for each dataframe respectively.\n",
        "    \"\"\"\n",
        "    if colors is None:\n",
        "        colors = [\"blue\", \"red\", \"green\", \"purple\", \"orange\"]\n",
        "\n",
        "    frames = []\n",
        "    for i, df in enumerate(dataframes):\n",
        "        temp = df.copy()\n",
        "        temp[\"color\"] = colors[i % len(colors)]  # cycle colors if more dfs than colors\n",
        "        frames.append(temp)\n",
        "\n",
        "    combined = pd.concat(frames, ignore_index=True)\n",
        "    if ghcnd_coords:\n",
        "      lat, lon, station_id = 'lat', 'lon', 'station'\n",
        "    else:\n",
        "      lat, lon, station_id = 'location.latitude', 'location.longitude', 'code'\n",
        "\n",
        "    fig = px.scatter_mapbox(\n",
        "        combined,\n",
        "        lat=lat,\n",
        "        lon=lon,\n",
        "        color=\"color\",\n",
        "        hover_name=station_id,\n",
        "        zoom=zoom,\n",
        "        height=height,\n",
        "        width=width\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        mapbox_style=\"open-street-map\",\n",
        "        legend_title=legend_title,\n",
        "        margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 0}\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n",
        "api_key = config['apiKey']\n",
        "api_secret = config['apiSecret']\n",
        "\n",
        "# Initialize the class\n",
        "rd = RetrieveData(apiKey=api_key,\n",
        "                  apiSecret=api_secret)\n",
        "\n",
        "# Extracting TAHMO data\n",
        "print(\"Extracting TAHMO data...\")\n",
        "info = rd.get_stations_info()\n",
        "info = info[(info['location.longitude'] >= xmin) &\n",
        "                        (info['location.longitude'] <= xmax) &\n",
        "                        (info['location.latitude'] >= ymin) &\n",
        "                        (info['location.latitude'] <= ymax)]\n",
        "print(\"✅ TAHMO data extracted successfully.\")\n",
        "# Print the total number of stations\n",
        "print(f\"Total number of stations: {len(info)}\")\n",
        "\n",
        "\n",
        "# save the data as csv to the created directory\n",
        "info.to_csv(f'{dir_path}/tahmo_metadata_{region_query}.csv')\n",
        "\n",
        "# wait for 5 seconds before visual\n",
        "time.sleep(5)\n",
        "\n",
        "# Visualise the data\n",
        "plot_stations_plotly([info])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-fpHD0dR6vDm"
      },
      "id": "-fpHD0dR6vDm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7213219a",
      "metadata": {
        "id": "7213219a",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Step 3b: Extract the TAHMO temperature 5 minute data for 2024 and get the tmin, tavg and tmax\n",
        "# Load TAHMO EAC stations previously extracted\n",
        "eac_metadata = pd.read_csv(f'{dir_path}/tahmo_metadata_{region_query}.csv')\n",
        "eac_metadata = eac_metadata[['code',\n",
        "                             'location.latitude',\n",
        "                             'location.longitude']].rename(columns={'location.latitude': 'lat',\n",
        "                                                                    'location.longitude': 'lon'})\n",
        "\n",
        "# Initialize the class\n",
        "rd = RetrieveData(apiKey=api_key,\n",
        "                  apiSecret=api_secret)\n",
        "\n",
        "print('Extracting Temperature data ...')\n",
        "# # Get the temperature data for the EAC stations in 5min intervals\n",
        "# eac_temp = rd.multiple_measurements(stations_list=eac_metadata['code'].tolist(),\n",
        "#                                      startDate=start_date,\n",
        "#                                      endDate=end_date,\n",
        "#                                      variables=['te'],\n",
        "#                                      csv_file = f'{dir_path}/tahmo_temp_{region_query}.csv',\n",
        "#                                      aggregate='5min'\n",
        "#                                      )\n",
        "\n",
        "\n",
        "# # Aggregate the values to get the min, mean and max for the day\n",
        "# tahmo_eac_tmin = rd.aggregate_variables(\n",
        "#     eac_temp,\n",
        "#     freq='1D',\n",
        "#     method='min'\n",
        "# )\n",
        "# tahmo_eac_tavg = rd.aggregate_variables(\n",
        "#     eac_temp,\n",
        "#     freq='1D',\n",
        "#     method='mean'\n",
        "# )\n",
        "# tahmo_eac_tmax = rd.aggregate_variables(\n",
        "#     eac_temp,\n",
        "#     freq='1D',\n",
        "#     method='max'\n",
        "# )\n",
        "\n",
        "\n",
        "# # plot_temperatures(tahmo_eac_tmin, tahmo_eac_tavg, tahmo_eac_tmax)\n",
        "\n",
        "\n",
        "# # Save the variables\n",
        "# tahmo_eac_tmin.to_csv(f'{dir_path}/tahmo_tmin_{region_query}.csv', index=True)\n",
        "# tahmo_eac_tavg.to_csv(f'{dir_path}/tahmo_tmin_{region_query}.csv', index=True)\n",
        "# tahmo_eac_tmax.to_csv(f'{dir_path}/tahmo_tmin_{region_query}.csv', index=True)\n",
        "\n",
        "# Method to cleanup the data\n",
        "def format_cleanup(df, localize_none=True):\n",
        "  # Rename Unnamed: 0 to Date\n",
        "  df.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
        "  # Set Date as index\n",
        "  df.set_index('Date', inplace=True)\n",
        "\n",
        "  # convert Date to datetime\n",
        "  df.index = pd.to_datetime(df.index)\n",
        "  if localize_none:\n",
        "    # set tz_localize to None\n",
        "    df.index = df.index.tz_localize(None)\n",
        "  return df\n",
        "\n",
        "# get only stations that have the metadata\n",
        "def match_with_metadata(df, metadata, column='code', localize_none=True):\n",
        "  # Format the data\n",
        "  df = format_cleanup(df, localize_none=localize_none)\n",
        "\n",
        "  # get the stations list\n",
        "  stations_list = metadata[column].to_list()\n",
        "\n",
        "  # Subset the columns from the dataframe with the data\n",
        "  df = df[df.columns.intersection(stations_list)]\n",
        "\n",
        "  return df\n",
        "\n",
        "# Load the tahmo data\n",
        "base_data_path = '/content/drive/Shareddrives/NOAA-workshop/Datasets/ground'\n",
        "tahmo_tmin = pd.read_csv(os.path.join(base_data_path,'eac_tmin_march_2024.csv' ))\n",
        "tahmo_tmin = match_with_metadata(tahmo_tmin, info)\n",
        "tahmo_tmax = pd.read_csv(os.path.join(base_data_path,'eac_tmax_march_2024.csv' ))\n",
        "tahmo_tmax = match_with_metadata(tahmo_tmax, info)\n",
        "tahmo_tavg = pd.read_csv(os.path.join(base_data_path,'eac_tavg_march_2024.csv' ))\n",
        "tahmo_tavg = match_with_metadata(tahmo_tavg, info)\n",
        "\n",
        "# check if the data is well loaded\n",
        "start_date = tahmo_tavg.index.min().strftime('%Y-%m-%d')\n",
        "end_date = tahmo_tavg.index.max().strftime('%Y-%m-%d')\n",
        "\n",
        "print('✅ Tahmo data loaded')\n",
        "# visualise the tavg data\n",
        "print('Printing the first 5 rows of the tavg data')\n",
        "tahmo_tavg.head().dropna(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3c: Randomly visualise the station data\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def plot_random_station_subplots(tavg_data, tmin_data, tmax_data):\n",
        "    \"\"\"\n",
        "    Randomly select one station from each dataset (tavg, tmin, tmax)\n",
        "    and plot them in vertically stacked subplots.\n",
        "    \"\"\"\n",
        "    datasets = {\n",
        "        \"Average Temperature (°C)\": tavg_data,\n",
        "        \"Minimum Temperature (°C)\": tmin_data,\n",
        "        \"Maximum Temperature (°C)\": tmax_data\n",
        "    }\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 10), sharex=True)\n",
        "    fig.suptitle(\"Random TAHMO Station Data — TAVG, TMIN, TMAX\", fontsize=14, weight='bold')\n",
        "\n",
        "    # Define station_codes here, before using it to pick a random station\n",
        "    station_codes = tavg_data.columns.to_list()\n",
        "    if not station_codes:\n",
        "        print(\"No stations found in the data.\")\n",
        "        plt.close(fig) # Close the figure if no stations are found\n",
        "        return\n",
        "\n",
        "    random_station = random.choice(station_codes)\n",
        "\n",
        "    for ax, (label, data) in zip(axes, datasets.items()):\n",
        "        # Ensure the random_station exists in the current data's columns\n",
        "        if random_station in data.columns:\n",
        "            station_data = data[random_station]\n",
        "\n",
        "            ax.plot(station_data.index, station_data.values, marker='o', linestyle='-')\n",
        "            ax.set_title(f\"{label} — Station {random_station}\", fontsize=11)\n",
        "            ax.set_ylabel(label)\n",
        "            ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "            # Print summary for each subplot\n",
        "            print(f\"📊 {label}\")\n",
        "            print(f\"   Station Code: {random_station}\")\n",
        "            print(f\"   Data Range: {station_data.min():.2f} to {station_data.max():.2f}\")\n",
        "            print(f\"   Number of Records: {len(station_data)}\\n\")\n",
        "        else:\n",
        "            print(f\"Station {random_station} not found in {label} data.\")\n",
        "\n",
        "\n",
        "    # Shared x-label and rotate ticks\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "plot_random_station_subplots(tahmo_tavg, tahmo_tmin, tahmo_tmax)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tRv2094lKgtV"
      },
      "id": "tRv2094lKgtV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 4: Detect flatlines in the TAHMO data\n",
        "# @markdown A flatline is defined when the station does not change in data for 3 days continuously\n",
        "\n",
        "from utils.flatline import detect_flatlines, plot_flatline_stations\n",
        "\n",
        "flatline_info_tmax = detect_flatlines(tahmo_tmax, window_size=3)\n",
        "flatline_info_tmin = detect_flatlines(tahmo_tmin, window_size=3)\n",
        "flatline_info_tavg = detect_flatlines(tahmo_tavg, window_size=3)\n",
        "\n",
        "# plot the flatline\n",
        "plot_flatline_stations(tahmo_tmax, flatline_info_tmax, window_size=3)\n",
        "# plot_flatline_stations(tahmo_tmin, flatline_info_tmin)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XlZcYQBkDm44"
      },
      "id": "XlZcYQBkDm44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5: ERA5 daily data for the month of March\n",
        "# @markdown The ERA5 equivalent variable is temperature_2m <br>\n",
        "# @markdown We will visualise the station against the ground data\n",
        "\n",
        "# # @title ERA5 builder\n",
        "# import ee\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import xarray as xr\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.animation as animation\n",
        "# import matplotlib.colors\n",
        "# import math\n",
        "# import datetime\n",
        "# import io\n",
        "# from tqdm import tqdm\n",
        "# from datetime import datetime, timedelta\n",
        "# from IPython.display import HTML, display\n",
        "# import cartopy.crs as ccrs\n",
        "# import cartopy.feature as cfeature\n",
        "# from filter_stations import retreive_data, Filter\n",
        "# import base64\n",
        "# import json\n",
        "# import requests\n",
        "# import datetime\n",
        "# from utils.helpers import get_region_geojson, df_to_xarray\n",
        "\n",
        "\n",
        "\n",
        "# def extract_era5_daily(start_date_str, end_date_str, bbox=None, polygon=None, era5_l=False, aggregate='mean'):\n",
        "#     \"\"\"\n",
        "#     Extract ERA5 reanalysis data (daily aggregated) from Google Earth Engine for a given bounding box or polygon and time range.\n",
        "#     The extraction is performed on a daily basis by aggregating hourly images (using the mean) for each day.\n",
        "#     For each day, the function retrieves the ERA5 HOURLY images, aggregates them, adds pixel coordinate bands (longitude\n",
        "#     and latitude), and uses sampleRectangle to extract a grid of pixel values. The results for each variable (band) are then\n",
        "#     organized into pandas DataFrames with the following columns:\n",
        "#       - date: The daily timestamp (ISO formatted)\n",
        "#       - latitude: The latitude coordinate of the pixel center\n",
        "#       - longitude: The longitude coordinate of the pixel center\n",
        "#       - value: The aggregated pixel value for that variable\n",
        "\n",
        "#     Args:\n",
        "#         start_date_str (str): Start datetime in ISO format, e.g., '2020-01-01T00:00:00'.\n",
        "#         end_date_str (str): End datetime in ISO format, e.g., '2020-01-02T00:00:00'.\n",
        "#         bbox (list or tuple, optional): Bounding box specified as [minLon, minLat, maxLon, maxLat]. Default is None.\n",
        "#         polygon (list, optional): Polygon specified as a list of coordinate pairs (e.g., [[lon, lat], ...]).\n",
        "#                                   If provided, the polygon geometry will be used instead of the bounding box.\n",
        "#                                   Default is None.\n",
        "#         era5_l (bool, optional): If True, use ERA5_LAND instead of ERA5. Default is False.\n",
        "#         aggregate (str, optional): Aggregation method ('mean' or 'sum' or 'min', or 'max'). Default is 'mean'.\n",
        "\n",
        "#     Returns:\n",
        "#         dict: A dictionary where keys are variable (band) names and values are pandas DataFrames containing\n",
        "#               the daily aggregated data.\n",
        "#     \"\"\"\n",
        "#     # Convert input datetime strings to Python datetime objects.\n",
        "#     start_date = datetime.datetime.strptime(start_date_str, '%Y-%m-%dT%H:%M:%S')\n",
        "#     end_date   = datetime.datetime.strptime(end_date_str, '%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "#     # Define the geometry: Use polygon if provided, otherwise use bbox.\n",
        "#     if polygon is not None:\n",
        "#         region = ee.Geometry.Polygon(polygon)\n",
        "#     elif bbox is not None:\n",
        "#         region = ee.Geometry.Rectangle(bbox)\n",
        "#     else:\n",
        "#         raise ValueError(\"Either bbox or polygon must be provided.\")\n",
        "\n",
        "#     # Define a scale in meters corresponding approximately to 0.25° (at the equator, 1° ≈ 111320 m).\n",
        "#     scale_m = 27830\n",
        "\n",
        "#     # This dictionary will accumulate extracted records for each variable (band).\n",
        "#     results = {}\n",
        "\n",
        "#     # Loop over each day in the specified time range.\n",
        "#     current = start_date\n",
        "#     while current < end_date:\n",
        "#         next_day = current + datetime.timedelta(days=1)\n",
        "\n",
        "#         # Format the current time window in ISO format.\n",
        "#         t0_str = current.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "#         t1_str = next_day.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "#         print(f\"Processing {t0_str} to {t1_str}\")\n",
        "\n",
        "#         # If ER5 Land (0.1) or ERA5 (0.25)\n",
        "#         if era5_l:\n",
        "#             # Get the ERA5 Land hourly image collection for the current day.\n",
        "#             collection = ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY') \\\n",
        "#                             .filterDate(ee.Date(t0_str), ee.Date(t1_str))\n",
        "#         else:\n",
        "#             # Get the ERA5 hourly image collection for the current day.\n",
        "#             collection = ee.ImageCollection('ECMWF/ERA5/HOURLY') \\\n",
        "#                             .filterDate(ee.Date(t0_str), ee.Date(t1_str))\n",
        "\n",
        "#         # Aggregate the hourly images into a single daily image using the mean.\n",
        "#         if aggregate == 'mean':\n",
        "#             image = collection.mean()\n",
        "#         elif aggregate == 'sum':\n",
        "#             image = collection.sum()\n",
        "#         elif aggregate == 'min':\n",
        "#             image = collection.min()\n",
        "#         elif aggregate == 'max':\n",
        "#             image = collection.max()\n",
        "#         else:\n",
        "#             raise ValueError(f\"Invalid aggregation method: {aggregate} can either be sum, min, max or mean\")\n",
        "\n",
        "#         # Add bands containing the pixel longitude and latitude.\n",
        "#         image = image.addBands(ee.Image.pixelLonLat())\n",
        "\n",
        "#         # Use sampleRectangle to extract a grid of pixel values over the region.\n",
        "#         region_data = image.sampleRectangle(region=region, defaultValue=0).getInfo()\n",
        "\n",
        "#         # The pixel values for each band are in the \"properties\" dictionary.\n",
        "#         props = region_data['properties']\n",
        "\n",
        "#         # Extract the coordinate arrays from the added pixelLonLat bands.\n",
        "#         lon_array = props['longitude']  # 2D array of longitudes\n",
        "#         lat_array = props['latitude']   # 2D array of latitudes\n",
        "\n",
        "#         # Determine the dimensions of the extracted grid.\n",
        "#         nrows = len(lon_array)\n",
        "#         ncols = len(lon_array[0]) if nrows > 0 else 0\n",
        "\n",
        "#         # Identify the names of the bands that hold ERA5 variables, excluding the coordinate bands.\n",
        "#         band_names = [key for key in props.keys() if key not in ['longitude', 'latitude']]\n",
        "\n",
        "#         # Initialize results lists for each band if not already present.\n",
        "#         for band in band_names:\n",
        "#             if band not in results:\n",
        "#                 results[band] = []\n",
        "\n",
        "#         # Loop over each pixel in the grid.\n",
        "#         for i in range(nrows):\n",
        "#             for j in range(ncols):\n",
        "#                 pixel_lon = lon_array[i][j]\n",
        "#                 pixel_lat = lat_array[i][j]\n",
        "#                 # For each ERA5 variable band, extract the pixel value and create a record.\n",
        "#                 for band in band_names:\n",
        "#                     pixel_value = props[band][i][j]\n",
        "#                     record = {\n",
        "#                         'date': t0_str,  # daily timestamp as a string\n",
        "#                         'latitude': pixel_lat,\n",
        "#                         'longitude': pixel_lon,\n",
        "#                         'value': pixel_value\n",
        "#                     }\n",
        "#                     results[band].append(record)\n",
        "\n",
        "#         # Advance to the next day.\n",
        "#         current = next_day\n",
        "\n",
        "#     # Convert the accumulated results for each band into pandas DataFrames.\n",
        "#     dataframes = {band: pd.DataFrame(records) for band, records in results.items()}\n",
        "#     return dataframes\n",
        "\n",
        "\n",
        "\n",
        "# # ERA5 helper expects ISO-like datetime strings with time component (%Y-%m-%dT%H:%M:%S)\n",
        "# iso_start_date = f\"{start_date}T00:00:00\"\n",
        "# iso_end_date = f\"{end_date}T23:59:59\"\n",
        "\n",
        "# era5_region_tmin = extract_era5_daily(iso_start_date, iso_end_date, era5_l=False,\n",
        "#                                    polygon=region_geom, aggregate='min')\n",
        "# era5_region_tavg = extract_era5_daily(iso_start_date, iso_end_date, era5_l=False,\n",
        "#                                    polygon=region_geom, aggregate='mean')\n",
        "# era5_region_tmax = extract_era5_daily(iso_start_date, iso_end_date, era5_l=False,\n",
        "#                                    polygon=region_geom, aggregate='max')\n",
        "\n",
        "# # xarray for tempersture 2m\n",
        "# era5_tmin = era5_var_handling(era5_region_tmin, 'temperature_2m', xarray_ds=True)\n",
        "# era5_tavg = era5_var_handling(era5_region_tavg, 'temperature_2m', xarray_ds=True)\n",
        "# era5_tmax = era5_var_handling(era5_region_tmax, 'temperature_2m', xarray_ds=True)\n",
        "\n",
        "# # save to xarray\n",
        "# era5_tmin.to_netcdf(f'{dir_path}/ERA5_tmin_{region_query}.nc')\n",
        "# era5_tavg.to_netcdf(f'{dir_path}/ERA5_tavg_{region_query}.nc')\n",
        "# era5_tmax.to_netcdf(f'{dir_path}/ERA5_tmax_{region_query}.nc')\n",
        "\n",
        "\n",
        "# Load the data\n",
        "era5_base_path = '/content/drive/Shareddrives/NOAA-workshop/Datasets/reanalysis/era5'\n",
        "\n",
        "# tavg\n",
        "era5_tavg = xr.open_dataset(os.path.join(era5_base_path,'era5_tavg_march_2024.nc'))\n",
        "era5_tavg\n",
        "era5_tmin = xr.open_dataset(os.path.join(era5_base_path,'era5_tmin_march_2024.nc'))\n",
        "era5_tmin\n",
        "era5_tmax = xr.open_dataset(os.path.join(era5_base_path,'era5_tmax_march_2024.nc'))\n",
        "era5_tmax\n",
        "\n",
        "# select the region within xmin, xmax ymin and ymax\n",
        "def subset_within_x_ymin_max(ds, xmin, xmax, ymin, ymax):\n",
        "  return ds.sel(lat=slice(ymin, ymax), lon=slice(xmin, xmax))\n",
        "\n",
        "era5_tavg = subset_within_x_ymin_max(era5_tavg, xmin, xmax, ymin, ymax)\n",
        "era5_tmin = subset_within_x_ymin_max(era5_tmin, xmin, xmax, ymin, ymax)\n",
        "era5_tmax = subset_within_x_ymin_max(era5_tmax, xmin, xmax, ymin, ymax)\n",
        "\n",
        "\n",
        "from utils.plotting_point import point_plot\n",
        "\n",
        "import xarray as xr\n",
        "\n",
        "# Weather points\n",
        "# Slice the resampled ground station data to match the number of time steps in chirps_ds\n",
        "# num_chirps_timesteps = len(chirps_ds.time)\n",
        "# ground_data_for_plot = region_precip_data.resample('5D').sum().iloc[:num_chirps_timesteps]\n",
        "\n",
        "\n",
        "html_anim = point_plot(\n",
        "    tahmo_tmax,\n",
        "    info,\n",
        "    variable_name=\"Ground Temperature\", # This is the point data variable name\n",
        "    metadata_columns=['code', 'location.latitude', 'location.longitude'],\n",
        "    cmap=\"plasma\",\n",
        "    grid_da=era5_tmax,\n",
        "    grid_cmap=\"coolwarm\",\n",
        "    grid_alpha=0.5,\n",
        "    fig_title=\"Station Temperature vs ERA5 Background\",\n",
        "    grid_da_var='max_temperature'\n",
        ")\n",
        "\n",
        "html_anim"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MFMIcmxE8Kgk"
      },
      "id": "MFMIcmxE8Kgk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations_with_replacement\n",
        "# @title Step 6: CBAM data for the month of March\n",
        "# @markdown Load and compare with the TAHMO data\n",
        "# Data from 2018-2024\n",
        "cbam_eac = xr.open_dataset('/content/drive/Shareddrives/NOAA-workshop/Datasets/reanalysis/CBAM_temp2018_2024.nc')\n",
        "\n",
        "# Subset for march 2024\n",
        "# cbam_eac = cbam_eac.sel(date=slice('2024-03-01', '2024-03-31'))\n",
        "\n",
        "# # Agreegate the data from daiy to monthly\n",
        "# cbam_eac_monthly = cbam_eac.resample(time='M').mean()\n",
        "\n",
        "# cbam_eac_monthly\n",
        "\n",
        "\n",
        "# select the month of march 2024\n",
        "cbam_data = cbam_eac.sel(date=slice('2024-03-01', '2024-03-31')).sel(lat=slice(ymin, ymax), lon=slice(xmin, xmax))\n",
        "\n",
        "del cbam_eac\n",
        "\n",
        "# compute the avg_temperature from the min and max temperature by computing the sum and dividing by 2\n",
        "cbam_data['avg_temperature'] = (cbam_data['max_temperature'] + cbam_data['min_temperature']) / 2\n",
        "\n",
        "# subset to the region xmin, ymin, xmax, ymax\n",
        "\n",
        "# rename date to time\n",
        "cbam_data = cbam_data.rename({'date': 'time'})\n",
        "\n",
        "html_anim = point_plot(\n",
        "    tahmo_tmax,\n",
        "    info,\n",
        "    variable_name=\"Ground Temperature\", # This is the point data variable name\n",
        "    metadata_columns=['code', 'location.latitude', 'location.longitude'],\n",
        "    cmap=\"plasma\",\n",
        "    grid_da=cbam_data,\n",
        "    grid_cmap=\"coolwarm\",\n",
        "    grid_alpha=0.5,\n",
        "    fig_title=\"Station Temperature vs CBAM Background\",\n",
        "    grid_da_var='max_temperature'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "html_anim"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xMXf4WnSm2UW"
      },
      "id": "xMXf4WnSm2UW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 7: CBAM vs ERA5 Comparison\n",
        "# @markdown This gives a visual comparison on the two datasets. <br>\n",
        "# @markdown ERA5 has a pixel grid of ~28km x 28km while CBAM ~4km x 4km <br>\n",
        "# @markdown We shall compare the tmin and tmax for ERA5 and CBAM\n",
        "\n",
        "# subset to cbam min and max\n",
        "cbam_tmin = cbam_data['min_temperature'].to_dataset()\n",
        "cbam_tmax = cbam_data['max_temperature'].to_dataset()\n",
        "\n",
        "# convert lat lon to y, x\n",
        "def convert_lat_lon_to_xy(cbam_tmin, cbam_tmax, eera5_tmin, era5_tmax,\n",
        "                          lat_name='lat', lon_name='lon'):\n",
        "  # rename the datasets\n",
        "  cbam_tmin = cbam_tmin.rename({lat_name: 'y', lon_name: 'x'})\n",
        "  cbam_tmax = cbam_tmax.rename({lat_name: 'y', lon_name: 'x'})\n",
        "  eera5_tmin = eera5_tmin.rename({lat_name: 'y', lon_name: 'x'})\n",
        "  era5_tmax = era5_tmax.rename({lat_name: 'y', lon_name: 'x'})\n",
        "  return cbam_tmin, cbam_tmax, eera5_tmin, era5_tmax\n",
        "\n",
        "cbam_tmin, cbam_tmax, era5_tmin, era5_tmax = convert_lat_lon_to_xy(cbam_tmin, cbam_tmax, era5_tmin, era5_tmax)\n",
        "\n",
        "\n",
        "\n",
        "compare_xarray_datasets2(\n",
        "    [era5_tmin, era5_tmax,\n",
        "     cbam_tmin, cbam_tmax],\n",
        "    labels=['ERA5 Minimum Temperature',\n",
        "            'ERA5 Maximum Temperature',\n",
        "            'CBAM Minimum Temperature',\n",
        "            'CBAM Maximum Temperature'],\n",
        "    fig_title='PET Comparison (ERA5 vs CBAM) - March 2024',\n",
        "    bboxes=[[xmin, ymin, xmax, ymax],\n",
        "            [xmin, ymin, xmax, ymax],\n",
        "            [xmin, ymin, xmax, ymax],\n",
        "            [xmin, ymin, xmax, ymax]],\n",
        "    save=False\n",
        ")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iucp4rCz2Wyz"
      },
      "id": "iucp4rCz2Wyz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bdcaf02",
      "metadata": {
        "id": "0bdcaf02",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Step 8a: Extract and visualise GHCNd weather stations locations\n",
        "# @markdown They are distributed globally but we shall visualize only the selected region\n",
        "\n",
        "# if path does not exist clone into repository\n",
        "if not os.path.exists('get-station-data'):\n",
        "    print('Setting up the tools to extract GHCNd weather stations ...')\n",
        "    !git clone https://github.com/scotthosking/get-station-data.git > /dev/null\n",
        "    print('✅ Tools set up successfully.')\n",
        "\n",
        "# !git clone https://github.com/scotthosking/get-station-data.git > /dev/null\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append('get-station-data')\n",
        "\n",
        "from get_station_data import ghcnd\n",
        "from get_station_data.util import nearest_stn\n",
        "\n",
        "from utils.GHCN_stations import subset_stations_in_bbox, get_nearest_wmo_station, subset_noaa_stations_by_country, subset_weather_data_by_variable # GHCN station helper functions\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import folium\n",
        "\n",
        "!pip install -U countrycode  > /dev/null\n",
        "\n",
        "\n",
        "stn_md = ghcnd.get_stn_metadata()\n",
        "# stn_md\n",
        "\n",
        "# Format the data rename lat and lon to latitude and longitude\n",
        "# stn_md = stn_md.rename(columns={'lat': 'latitude', 'lon': 'longitude'})\n",
        "\n",
        "# map from country to  country code\n",
        "def map2code(region_query):\n",
        "\n",
        "\n",
        "  from countrycode import countrycode\n",
        "\n",
        "\n",
        "  country_code = countrycode([region_query],\n",
        "                             origin='country.name',\n",
        "                             destination ='iso2c')[0]\n",
        "  # check if countrycode was obtained\n",
        "  if country_code:\n",
        "    print(f'✅ Filtered to the specific country')\n",
        "\n",
        "  return country_code\n",
        "\n",
        "# input the data get the country subset\n",
        "def country_subset(ghcnd_metadata, region_query):\n",
        "  # get the country code from the country name\n",
        "  code_c = map2code(region_query)\n",
        "\n",
        "  # subset to this countrycode\n",
        "  wmo_subset = subset_noaa_stations_by_country(ghcnd_metadata, code_c)\n",
        "  return wmo_subset\n",
        "\n",
        "\n",
        "# concatenate eac stations\n",
        "# eac_wmo_stations = pd.concat([wmo_ke_stations, wmo_ug_stations, wmo_rw_stations])\n",
        "\n",
        "ghcn_subset_md = country_subset(stn_md, region_query)\n",
        "plot_stations_plotly([ghcn_subset_md],\n",
        "                     ghcnd_coords=True)\n",
        "\n",
        "# Get the data\n",
        "# eac_wmo_data = ghcnd.get_data(eac_wmo_stations)\n",
        "\n",
        "# eac_wmo_data[eac_wmo_data.station == 'KEM00063741']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 8b: Get the GHCNd data for the selected region\n",
        "\n",
        "# base path\n",
        "wmo_base_path = '/content/drive/Shareddrives/NOAA-workshop/Datasets/ground'\n",
        "\n",
        "# format cleanup\n",
        "def format_ghcnd(df, localize_none=True):\n",
        "  # rename date to Date\n",
        "  df.rename(columns={'date': 'Date'}, inplace=True)\n",
        "  # Set Date as index\n",
        "  df.set_index('Date', inplace=True)\n",
        "\n",
        "  # convert Date to datetime\n",
        "  df.index = pd.to_datetime(df.index)\n",
        "\n",
        "  if localize_none:\n",
        "    # set tz_localize to None\n",
        "    df.index = df.index.tz_localize(None)\n",
        "  return df\n",
        "\n",
        "# get the common stations from the metadata\n",
        "def match_stations(df, metadata, column='station', localize_none=True):\n",
        "  # Format the data\n",
        "  df = format_ghcnd(df, localize_none=localize_none)\n",
        "\n",
        "  # get the data subset\n",
        "  stations_list = metadata[column].to_list()\n",
        "\n",
        "  # Subset the columns from the dataframe with the data\n",
        "  df = df[df.columns.intersection(stations_list)]\n",
        "\n",
        "  return df\n",
        "\n",
        "# load the data\n",
        "ghcnd_tmin = pd.read_csv(os.path.join(wmo_base_path,'eac_wmo_tmin_march_2024.csv' ))\n",
        "ghcnd_tmin = match_stations(ghcnd_tmin, ghcn_subset_md)\n",
        "ghcnd_tmax = pd.read_csv(os.path.join(wmo_base_path,'eac_wmo_tmax_march_2024.csv' ))\n",
        "ghcnd_tmax = match_stations(ghcnd_tmax, ghcn_subset_md)\n",
        "ghcnd_tavg = pd.read_csv(os.path.join(wmo_base_path,'eac_wmo_tavg_march_2024.csv' ))\n",
        "ghcnd_tavg = match_stations(ghcnd_tavg, ghcn_subset_md)\n",
        "\n",
        "print('✅ GHCNd data loaded')\n",
        "\n",
        "\n",
        "# visualise the tavg data\n",
        "print('Printing the first 5 rows of the tavg data')\n",
        "ghcnd_tavg.head().dropna(axis=1)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O0nmKz4B6HsA"
      },
      "id": "O0nmKz4B6HsA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 8c: Visualize the nearest station from the capital city\n",
        "# @markdown The visual is for the nearest station from the capital city of the country selected earlier on\n",
        "\n",
        "\n",
        "# Dropdown for the capitals\n",
        "# Set capital coordinates\n",
        "CAPITALS={'Kenya':{'name':'Nairobi','lat':-1.2921,'lon':36.8219},\n",
        "          'Uganda':{'name':'Kampala','lat':0.3476,'lon':32.5825},\n",
        "          'Rwanda':{'name':'Kigali','lat':-1.9579,'lon':30.1127}}\n",
        "\n",
        "# Map the country to the capital and obtain the coords\n",
        "capital_query = CAPITALS[region_query.title()]\n",
        "capital_city, lat, lon = capital_query['name'], capital_query['lat'], capital_query['lon']\n",
        "\n",
        "# Get the nearest station\n",
        "# Reset the index of ghcn_subset_md before passing it to nearest_stn\n",
        "nearest_capital_stn = nearest_stn(ghcn_subset_md.reset_index(),\n",
        "            my_x=lon,\n",
        "            my_y=lat)\n",
        "\n",
        "# plot the visuals for this station\n",
        "\n",
        "\n",
        "def plot_ghcnd_station_subplots(tavg_data, tmin_data, tmax_data, station_code):\n",
        "    \"\"\"\n",
        "    Plots the daily minimum, average, and maximum temperatures for a specified GHCNd station\n",
        "    in vertically stacked subplots.\n",
        "\n",
        "    Args:\n",
        "        tavg_data (pd.DataFrame): DataFrame containing daily average temperatures for GHCNd stations.\n",
        "        tmin_data (pd.DataFrame): DataFrame containing daily minimum temperatures for GHCNd stations.\n",
        "        tmax_data (pd.DataFrame): DataFrame containing daily maximum temperatures for GHCNd stations.\n",
        "        station_code (str): The code of the station to plot.\n",
        "    \"\"\"\n",
        "    datasets = {\n",
        "        \"Average Temperature (°C)\": tavg_data,\n",
        "        \"Minimum Temperature (°C)\": tmin_data,\n",
        "        \"Maximum Temperature (°C)\": tmax_data\n",
        "    }\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 10), sharex=True)\n",
        "    fig.suptitle(f\"GHCNd Station Data — {station_code}\", fontsize=14, weight='bold')\n",
        "\n",
        "    for ax, (label, data) in zip(axes, datasets.items()):\n",
        "        if station_code in data.columns:\n",
        "            station_data = data[station_code].dropna() # Drop NaN values for plotting\n",
        "            if not station_data.empty:\n",
        "                ax.plot(station_data.index, station_data.values, linestyle='-')\n",
        "                ax.set_title(f\"{label}\", fontsize=11)\n",
        "                ax.set_ylabel(label)\n",
        "                ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "                # Print summary for each subplot\n",
        "                print(f\"📊 {label}\")\n",
        "                print(f\"   Station Code: {station_code}\")\n",
        "                print(f\"   Data Range: {station_data.min():.2f} to {station_data.max():.2f}\")\n",
        "                print(f\"   Number of Records: {len(station_data)}\\n\")\n",
        "            else:\n",
        "                print(f\"No valid data for {label} for station {station_code}.\")\n",
        "        else:\n",
        "            print(f\"Station {station_code} not found in {label} data.\")\n",
        "\n",
        "    # Shared x-label and rotate ticks\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "plot_ghcnd_station_subplots(ghcnd_tavg, ghcnd_tmin, ghcnd_tmax, nearest_capital_stn.station.to_list()[0])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SZsdeNll_hmM"
      },
      "id": "SZsdeNll_hmM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 8d: Visualising the annual temperature over the years\n",
        "\n",
        "# visualise the cumulative average monthly data over the years\n",
        "def compute_average_data(temp_data, agg='1M'):\n",
        "  # aggregate either monthly or annually\n",
        "  if agg == '1M':\n",
        "    return temp_data.resample('1M').mean()\n",
        "  elif agg == '1Y':\n",
        "    return temp_data.resample('1Y').mean()\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid aggregation method: {agg}. Use '1M' for monthly or '1Y' for annual.\")\n",
        "\n",
        "tavg_annual = compute_average_data(ghcnd_tavg, agg='1Y').loc['1980':]\n",
        "tmin_annual = compute_average_data(ghcnd_tmin, agg='1Y').loc['1980':]\n",
        "tmax_annual = compute_average_data(ghcnd_tmax, agg='1Y').loc['1980':]\n",
        "\n",
        "plot_ghcnd_station_subplots(tavg_annual, tmin_annual, tmax_annual, nearest_capital_stn.station.to_list()[0])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5sdtVoIVErDg"
      },
      "id": "5sdtVoIVErDg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ###  Step 9a: Hargreaves Equation for Potential Evapotranspiration (PET) Definition\n",
        "# @markdown The Hargreaves method estimates daily potential evapotranspiration (PET)\n",
        "# @markdown based on temperature range and incoming solar radiation:\n",
        "# @markdown\n",
        "# @markdown $$\n",
        "# @markdown \\text{PET} = 0.0023 \\times R_a \\times (T_{mean} + 17.8) \\times \\sqrt{T_{max} - T_{min}}\n",
        "# @markdown $$\n",
        "# @markdown\n",
        "# @markdown **Where:**\n",
        "# @markdown - $PET$: Potential Evapotranspiration (mm day⁻¹)\n",
        "# @markdown - $R_a$: Extraterrestrial radiation (MJ m⁻² day⁻¹)\n",
        "# @markdown - $T_{mean}$: Mean daily air temperature (°C)\n",
        "# @markdown - $T_{max}$: Maximum daily air temperature (°C)\n",
        "# @markdown - $T_{min}$: Minimum daily air temperature (°C)\n",
        "# @markdown\n",
        "# @markdown 💡 *In this function, a default value of $R_a = 15.0$ MJ m⁻² day⁻¹ is used* <br>\n",
        "\n",
        "def pet_hargreaves(tmin, tmax, tmean, Ra=15.0):\n",
        "    dtr = np.maximum(tmax - tmin, 0)\n",
        "    return 0.0023 * Ra * (tmean + 17.8) * np.sqrt(dtr)\n",
        "\n",
        "def rmse(a,b): return float(np.sqrt(np.nanmean((np.asarray(a)-np.asarray(b))**2)))\n",
        "\n",
        "print('✅ Formula loaded')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "veZZQab97em0"
      },
      "id": "veZZQab97em0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 9b: Compute PET for CBAM and ERA5\n",
        "\n",
        "pet_era5 = pet_hargreaves(era5_tmin.min_temperature, era5_tmax.max_temperature.values, era5_tavg.avg_temperature.values).to_dataset(name='pet')\n",
        "\n",
        "# pet_era5 = pet_era5.to_dataset(name='pet')\n",
        "# pet_era5\n",
        "\n",
        "# # drop lat and lon columns\n",
        "# pet_era5 = pet_era5.drop(['lat', 'lon'])\n",
        "# pet_era5\n",
        "\n",
        "\n",
        "# pet cbam\n",
        "pet_cbam = pet_hargreaves(cbam_data['min_temperature'], cbam_data['max_temperature'], cbam_data['avg_temperature'])\n",
        "\n",
        "pet_cbam = pet_cbam.to_dataset(name='pet')\n",
        "pet_cbam\n",
        "\n",
        "# rename lat lon to x y\n",
        "pet_cbam = pet_cbam.rename({'lat': 'y', 'lon': 'x'})\n",
        "\n",
        "# drop lat and lon columns\n",
        "# pet_cbam = pet_cbam.drop(['lat', 'lon'])\n",
        "pet_cbam\n",
        "\n",
        "# compare_xarray_datasets2(\n",
        "#     [pet_era5, pet_cbam],\n",
        "#     labels=['PET ERA5', 'PET CBAM'],\n",
        "#     fig_title='PET Comparison (ERA5 vs CBAM) - March 2024',\n",
        "#     bboxes=[[xmin, ymin, xmax, ymax], [xmin, ymin, xmax, ymax]],\n",
        "#     save=False\n",
        "# )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YtSzlDBo52PY"
      },
      "id": "YtSzlDBo52PY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 9c: Compute Heat Stress Conditions\n",
        "# @markdown ### **Stress Condition Criteria**\n",
        "# @markdown A grid cell or station is considered under **heat stress** when:\n",
        "# @markdown\n",
        "# @markdown - $PET > 5$ mm day⁻¹  <br>\n",
        "# @markdown **and**\n",
        "# @markdown - Maximum temperature ($T_{max} > 32\\,°C$)\n",
        "\n",
        "stress_cbam = (pet_cbam>5) &(cbam_data['max_temperature']>32)\n",
        "stress_era5 = (pet_era5>5) &(era5_tmax['max_temperature']>32)\n",
        "\n",
        "# get the stress days\n",
        "print(\"Heat/Agri stress days (CBAM): \", )\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RevvODcw86Vx"
      },
      "id": "RevvODcw86Vx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the true to 1 and false to 0\n",
        "stress_cbam = stress_cbam.astype(int).to_dataset(name='stress')\n",
        "stress_era5 = stress_era5.astype(int).to_dataset(name='stress')\n",
        "\n",
        "# Get the days where stress is True\n",
        "# stress_days_cbam = stress_cbam.where(stress_cbam, drop=True).time.values\n",
        "# stress_days_era5 = stress_era5.where(stress_era5, drop=True).time.values\n",
        "\n",
        "# print(\"Days with heat/agri stress (CBAM):\")\n",
        "# if len(stress_days_cbam) > 0:\n",
        "#     for day in stress_days_cbam:\n",
        "#         print(day)\n",
        "# else:\n",
        "#     print(\"No stress days found for CBAM.\")\n",
        "\n",
        "# print(\"\\nDays with heat/agri stress (ERA5):\")\n",
        "# if len(stress_days_era5) > 0:\n",
        "#     for day in stress_days_era5:\n",
        "#         print(day)\n",
        "# else:\n",
        "#     print(\"No stress days found for ERA5.\")"
      ],
      "metadata": {
        "id": "TQC7GnC9GeRF"
      },
      "id": "TQC7GnC9GeRF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_xarray_datasets2(\n",
        "    [stress_era5, stress_cbam],\n",
        "    labels=['Mean Stress (ERA5)', 'Mean Stress (CBAM)'],\n",
        "    fig_title='Mean Heat/Agri Stress Frequency - March 2024',\n",
        "    bboxes=[[xmin, ymin, xmax, ymax], [xmin, ymin, xmax, ymax]]\n",
        ")"
      ],
      "metadata": {
        "id": "6gO2-ZZTHcqr"
      },
      "id": "6gO2-ZZTHcqr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 10: Load ERA5 data from 1982 to 2024 and visualise the heat change over the years\n",
        "# # @title ERA5 extract data from 1982 -2024\n",
        "\n",
        "\n",
        "\n",
        "# import ee\n",
        "# import io\n",
        "# import os\n",
        "# import tempfile\n",
        "# import requests\n",
        "# import datetime\n",
        "# import numpy as np\n",
        "# import xarray as xr\n",
        "# import rasterio\n",
        "# from rasterio.io import MemoryFile\n",
        "# from rasterio.transform import xy as rio_xy\n",
        "\n",
        "# # Authenticate / initialize once (uncomment in interactive runtime)\n",
        "# # ee.Authenticate()\n",
        "# # ee.Initialize()\n",
        "\n",
        "# def era5_yearly_to_inmemory_netcdf(\n",
        "#     variable,\n",
        "#     start_year=1982,\n",
        "#     end_year=None,\n",
        "#     region_ee_geometry=None,\n",
        "#     dataset='ERA5_LAND',   # 'ERA5' or 'ERA5_LAND'\n",
        "#     cadence='monthly',       # 'hourly' or 'daily' or 'monthly'\n",
        "#     scale=None,            # meters (defaults used below)\n",
        "#     crs='EPSG:4326',\n",
        "#     save_local_copy=False, # also save .nc to local disk (path returned)\n",
        "#     local_folder='./',\n",
        "#     max_images_per_year=4000  # safety cutoff\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     For each year in [start_year, end_year], download the ERA5 images in that year,\n",
        "#     aggregate them according to the specified cadence, build a time-x-y-xarray dataset\n",
        "#     and write a NetCDF file for that year, then return the NetCDF as an in-memory\n",
        "#     BytesIO object.\n",
        "\n",
        "#     Returns:\n",
        "#         dict: { year (int) : { 'nc_bytes': io.BytesIO, 'local_path': str or None } }\n",
        "#     \"\"\"\n",
        "\n",
        "#     if end_year is None:\n",
        "#         end_year = datetime.datetime.utcnow().year\n",
        "\n",
        "#     # Dataset selection and default scale (meters)\n",
        "#     ds_upper = dataset.upper()\n",
        "#     if ds_upper == 'ERA5_LAND' or ds_upper == 'ERA5-LAND' or ds_upper == 'ERA5LAND':\n",
        "#         coll_hourly = 'ECMWF/ERA5_LAND/HOURLY'\n",
        "#         coll_daily = 'ECMWF/ERA5_LAND/DAILY_AGGR'\n",
        "#         default_scale = 11132\n",
        "#     elif ds_upper == 'ERA5':\n",
        "#         coll_hourly = 'ECMWF/ERA5/HOURLY'\n",
        "#         coll_daily = 'ECMWF/ERA5/DAILY'\n",
        "#         default_scale = 27830\n",
        "#     else:\n",
        "#         raise ValueError(\"dataset must be 'ERA5' or 'ERA5_LAND'\")\n",
        "\n",
        "#     if scale is None:\n",
        "#         scale = default_scale\n",
        "\n",
        "#     if region_ee_geometry is None:\n",
        "#         raise ValueError(\"region_ee_geometry (an ee.Geometry) is required (keep it small!)\")\n",
        "\n",
        "#     # turn region into a geojson / coordinates object for getDownloadURL\n",
        "#     # getInfo() here calls the server once\n",
        "#     region_geojson = region_ee_geometry.getInfo()\n",
        "\n",
        "#     results = {}\n",
        "\n",
        "#     for year in range(start_year, end_year + 1):\n",
        "#         print(f\"\\n--- Processing year {year} ---\")\n",
        "#         start_date_year = f'{year}-01-01'\n",
        "#         end_date_year = f'{year+1}-01-01'\n",
        "\n",
        "#         if cadence == 'hourly':\n",
        "#             coll = ee.ImageCollection(coll_hourly).filterDate(start_date_year, end_date_year).select(variable)\n",
        "#         elif cadence == 'daily':\n",
        "#             coll = ee.ImageCollection(coll_daily).filterDate(start_date_year, end_date_year).select(variable)\n",
        "#         elif cadence == 'monthly':\n",
        "#              # Process month by month for monthly aggregation\n",
        "#             monthly_images = []\n",
        "#             current_month_start = datetime.datetime.strptime(start_date_year, '%Y-%m-%d')\n",
        "#             while current_month_start.year == year:\n",
        "#                 next_month_start = (current_month_start.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)\n",
        "#                 coll_hourly_month = ee.ImageCollection(coll_hourly).filterDate(current_month_start, next_month_start).select(variable)\n",
        "#                 monthly_image = coll_hourly_month.mean() # Aggregate hourly to monthly mean\n",
        "#                 monthly_images.append(monthly_image.set('system:time_start', ee.Date(current_month_start)))\n",
        "#                 current_month_start = next_month_start\n",
        "#             coll = ee.ImageCollection(monthly_images)\n",
        "#         else:\n",
        "#             raise ValueError(\"cadence must be 'hourly', 'daily', or 'monthly'\")\n",
        "\n",
        "\n",
        "#         try:\n",
        "#             n_images = int(coll.size().getInfo())\n",
        "#         except Exception as e:\n",
        "#             raise RuntimeError(f\"Could not fetch collection size for {year}: {e}\")\n",
        "\n",
        "#         if n_images == 0:\n",
        "#             print(f\"No images found for {year} (variable '{variable}', cadence '{cadence}'). Skipping.\")\n",
        "#             continue\n",
        "\n",
        "#         if n_images > max_images_per_year and cadence != 'monthly': # Allow more images for monthly aggregation\n",
        "#              raise RuntimeError(f\"Year {year} has {n_images} images > max_images_per_year ({max_images_per_year}). Aborting for safety.\")\n",
        "\n",
        "#         print(f\"Found {n_images} images for {year}. Downloading each to memory (this may be slow).\")\n",
        "\n",
        "#         # Build lists to stack\n",
        "#         img_arrays = []\n",
        "#         times = []\n",
        "#         ref_shape = None\n",
        "#         ref_transform = None\n",
        "#         ref_crs = None\n",
        "\n",
        "#         # Convert collection to server list and iterate\n",
        "#         coll_list = coll.toList(n_images)\n",
        "\n",
        "#         for i in range(n_images):\n",
        "#             ee_img = ee.Image(coll_list.get(i))\n",
        "#             # time string\n",
        "#             try:\n",
        "#                 time_start_ms = ee.Date(ee_img.get('system:time_start')).getInfo()['value']\n",
        "#                 time_str = datetime.datetime.fromtimestamp(time_start_ms / 1000.0).strftime('%Y-%m-%d')\n",
        "#             except Exception:\n",
        "#                 # fallback: use index-based date\n",
        "#                 time_str = f'{year}-unknown-{i}'\n",
        "#             print(f\"  - image {i+1}/{n_images} date {time_str} ...\", end=' ', flush=True)\n",
        "\n",
        "#             # Request a GeoTIFF download URL (format GEO_TIFF to get raw .tif bytes)\n",
        "#             params = {\n",
        "#                 'bands': [variable],\n",
        "#                 'region': region_geojson,   # geojson-like mapping or coordinates (small)\n",
        "#                 'scale': int(scale),\n",
        "#                 'format': 'GEO_TIFF',\n",
        "#                 'filePerBand': False\n",
        "#             }\n",
        "\n",
        "#             try:\n",
        "#                 url = ee_img.getDownloadURL(params)\n",
        "#             except Exception as e:\n",
        "#                 raise RuntimeError(f\"getDownloadURL failed for {year} image idx {i}: {e}\")\n",
        "\n",
        "#             # Download bytes (may be zipped or raw GeoTIFF depending on params; we asked GEO_TIFF)\n",
        "#             r = requests.get(url, timeout=600)\n",
        "#             if r.status_code != 200:\n",
        "#                 raise RuntimeError(f\"HTTP error {r.status_code} when downloading image: {r.text[:200]}\")\n",
        "\n",
        "#             # Load into rasterio MemoryFile\n",
        "#             with MemoryFile(r.content) as mem:\n",
        "#                 with mem.open() as src:\n",
        "#                     arr = src.read(1)           # single-band image\n",
        "#                     transform = src.transform\n",
        "#                     crs_src = src.crs\n",
        "#                     h, w = src.height, src.width\n",
        "\n",
        "#             # check shape consistency\n",
        "#             if ref_shape is None:\n",
        "#                 ref_shape = (h, w)\n",
        "#                 ref_transform = transform\n",
        "#                 ref_crs = crs_src\n",
        "#             else:\n",
        "#                 if (h, w) != ref_shape:\n",
        "#                     raise RuntimeError(f\"Image {i} shape {h,w} differs from first image shape {ref_shape}. Reprojection/resampling not implemented - aborting.\")\n",
        "\n",
        "#             img_arrays.append(arr)\n",
        "#             times.append(np.datetime64(time_str))\n",
        "#             print(\"OK\")\n",
        "\n",
        "#         # Stack into ndarray (time, y, x)\n",
        "#         data_stack = np.stack(img_arrays, axis=0)  # shape (time, H, W)\n",
        "#         print(f\"Stacked data: {data_stack.shape}\")\n",
        "\n",
        "#         # Build coordinate vectors from transform\n",
        "#         height, width = ref_shape\n",
        "#         # x coords (cols)\n",
        "#         xs = np.array([rio_xy(ref_transform, 0, col, offset='center')[0] for col in range(width)])\n",
        "#         # y coords (rows) - note rasterio returns y per (row, col); rows increase downward\n",
        "#         ys = np.array([rio_xy(ref_transform, row, 0, offset='center')[1] for row in range(height)])\n",
        "\n",
        "#         # xarray DataArray\n",
        "#         da = xr.DataArray(\n",
        "#             data_stack,\n",
        "#             dims=('time', 'y', 'x'),\n",
        "#             coords={'time': times, 'y': ys, 'x': xs},\n",
        "#             name=variable\n",
        "#         )\n",
        "\n",
        "#         ds = xr.Dataset({variable: da})\n",
        "#         ds.attrs['source'] = f\"GEE {coll_daily} ({dataset})\" # Note: still using daily collection ID in source attr\n",
        "#         # Convert region_geojson to a string for NetCDF compatibility\n",
        "#         ds.attrs['region'] = json.dumps(region_geojson)\n",
        "#         ds.attrs['scale_m'] = scale\n",
        "\n",
        "#         # Persist to a temporary netCDF file, then load bytes into memory\n",
        "#         tmpf = tempfile.NamedTemporaryFile(suffix=f\"_{variable}_{year}_{cadence}.nc\", delete=False)\n",
        "#         tmpf.close()\n",
        "#         try:\n",
        "#             ds.to_netcdf(tmpf.name, engine='netcdf4')\n",
        "#         except Exception as e:\n",
        "#             os.unlink(tmpf.name)\n",
        "#             raise RuntimeError(f\"Failed to write NetCDF for year {year}: {e}\")\n",
        "\n",
        "#         # Read bytes into memory BytesIO\n",
        "#         with open(tmpf.name, 'rb') as f:\n",
        "#             nc_bytes = f.read()\n",
        "\n",
        "#         # Optionally save a local persistent copy\n",
        "#         local_path = None\n",
        "#         if save_local_copy:\n",
        "#             os.makedirs(local_folder, exist_ok=True)\n",
        "#             local_path = os.path.join(local_folder, f\"{variable}_{year}_{cadence}.nc\")\n",
        "#             with open(local_path, 'wb') as f:\n",
        "#                 f.write(nc_bytes)\n",
        "\n",
        "#         # Cleanup temp file\n",
        "#         os.unlink(tmpf.name)\n",
        "\n",
        "#         results[year] = {'nc_bytes': io.BytesIO(nc_bytes), 'local_path': local_path}\n",
        "\n",
        "#         print(f\"Year {year} done: NetCDF in memory ({len(nc_bytes)/1e6:.2f} MB).\")\n",
        "\n",
        "#     return results\n",
        "\n",
        "\n",
        "# roi = ee.Geometry.Polygon(eac_region)\n",
        "\n",
        "# out = era5_yearly_to_inmemory_netcdf(\n",
        "#     variable='temperature_2m',\n",
        "#     start_year=1982,\n",
        "#     end_year=2024,\n",
        "#     region_ee_geometry=roi,\n",
        "#     dataset='ERA5',        # or 'ERA5_LAND'\n",
        "#     cadence='monthly',\n",
        "#     scale=27830,           # use native-ish scale for ERA5 (meters)\n",
        "#     save_local_copy=False\n",
        "# )\n",
        "\n",
        "# # Access the NetCDF bytes for 2023:\n",
        "# nc_bytesio = out[2023]['nc_bytes']      # io.BytesIO\n",
        "# # To load into xarray directly from memory:\n",
        "# nc_bytesio.seek(0)\n",
        "# ds = xr.open_dataset(nc_bytesio)\n",
        "# print(ds)\n",
        "\n",
        "\n",
        "# load the data\n",
        "era5_long_term = xr.open_dataset('/content/drive/Shareddrives/NOAA-workshop/Datasets/reanalysis/era5/era5_temperature_monthly_1982_2024_combined.nc')\n",
        "\n",
        "# subset to the xmin max\n",
        "# era5_long_term = era5_long_term.sel(x=slice(ymin, ymax), y=slice(xmin, xmax))\n",
        "\n",
        "# Subtract 273.15 from the data\n",
        "era5_long_term['temperature_2m'] = era5_long_term['temperature_2m'] - 273.15\n",
        "\n",
        "\n",
        "# @title Plot ERA5 Monthly Temperature Heatmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def plot_era5_monthly_heatmap(era5_data, variable='temperature_2m', cmap='coolwarm', fig_title=\"Monthly Average Temperature Heatmap (ERA5)\"):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of monthly average temperature for ERA5 data over the years.\n",
        "\n",
        "    Args:\n",
        "        era5_data (xr.Dataset): Xarray Dataset containing monthly ERA5 temperature data.\n",
        "        variable (str): The variable to plot from the ERA5 dataset (e.g., 'temperature_2m').\n",
        "        cmap (str): Colormap to use for the heatmap.\n",
        "        fig_title (str): Title of the heatmap.\n",
        "    \"\"\"\n",
        "    # Calculate the monthly average temperature over the spatial dimensions\n",
        "    monthly_mean_temp = era5_data[variable].mean(dim=['y', 'x'])\n",
        "\n",
        "    # Convert to pandas Series for easier reshaping\n",
        "    monthly_mean_temp_series = monthly_mean_temp.to_series()\n",
        "\n",
        "    # Create a MultiIndex from the time index for unstacking into Year x Month\n",
        "    monthly_mean_temp_series.index = pd.MultiIndex.from_arrays([\n",
        "        monthly_mean_temp_series.index.year,\n",
        "        monthly_mean_temp_series.index.month\n",
        "    ], names=['year', 'month'])\n",
        "\n",
        "    # Reshape for heatmap (Year as columns, Month as index)\n",
        "    heatmap_data = monthly_mean_temp_series.unstack(level='year')\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(heatmap_data, cmap=cmap, cbar_kws={'label': f'Average {variable} (°C)'})\n",
        "    plt.title(fig_title)\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Month')\n",
        "    plt.yticks(ticks=np.arange(12) + 0.5, labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the plotting function with the loaded ERA5 long-term data\n",
        "plot_era5_monthly_heatmap(era5_long_term)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nP-4yRRNGeMb"
      },
      "id": "nP-4yRRNGeMb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "558f87d4",
        "cellView": "form"
      },
      "source": [
        "# method to create a heatmap of maximum temperature for WMO stations\n",
        "def plot_wmo_heatmap(annual_max_temp_stations, fig_title=\"Annual Maximum Temperature Heatmap (WMO Stations)\"):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of annual maximum temperature for WMO stations.\n",
        "\n",
        "    Args:\n",
        "        annual_max_temp_stations (pd.DataFrame): DataFrame with annual maximum temperatures per station.\n",
        "        fig_title (str): Title of the heatmap.\n",
        "    \"\"\"\n",
        "    # localise to None date\n",
        "    # annual_max_temp_stations = annual_max_temp_stations.T\n",
        "    annual_max_temp_stations.index = pd.to_datetime(annual_max_temp_stations.index)\n",
        "    annual_max_temp_stations = annual_max_temp_stations.tz_localize(None)\n",
        "    annual_max_temp_stations = annual_max_temp_stations.T\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(annual_max_temp_stations,\n",
        "                cmap='coolwarm',\n",
        "                cbar_kws={'label': 'Maximum Temperature (°C)'}) # Fixed cbar_label\n",
        "    plt.title(fig_title)\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Station')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# method to create a heatmap of temperature for ERA5 monthly data\n",
        "def plot_era5_heatmap(era5_monthly_data, variable='temperature_2m', fig_title=\"Monthly Temperature Heatmap (ERA5)\"):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of monthly temperature for ERA5 data.\n",
        "\n",
        "    Args:\n",
        "        era5_monthly_data (xr.Dataset): Xarray Dataset containing monthly ERA5 temperature data.\n",
        "        variable (str): The variable to plot from the ERA5 dataset.\n",
        "        fig_title (str): Title of the heatmap.\n",
        "    \"\"\"\n",
        "    # Assuming the ERA5 data has dimensions 'time', 'latitude', 'longitude'\n",
        "    monthly_mean_temp = era5_monthly_data[variable].mean(dim=['latitude', 'longitude'])\n",
        "\n",
        "    # Convert to pandas Series for plotting\n",
        "    monthly_mean_temp_series = monthly_mean_temp.to_series()\n",
        "\n",
        "    # Create a MultiIndex from the time index for unstacking\n",
        "    monthly_mean_temp_series.index = pd.MultiIndex.from_arrays([\n",
        "        monthly_mean_temp_series.index.year,\n",
        "        monthly_mean_temp_series.index.month\n",
        "    ], names=['year', 'month'])\n",
        "\n",
        "\n",
        "    # Reshape for heatmap (Month x Year)\n",
        "    heatmap_data = monthly_mean_temp_series.unstack().T\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Mean Temperature (°C)'}) # Fixed cbar_label\n",
        "    plt.title(fig_title)\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Month')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# method to visualize heat change comparison (this will depend on the specific comparison needed)\n",
        "def compare_heatmaps(heatmap1_data, heatmap2_data, label1, label2, fig_title=\"Heatmap Comparison\"):\n",
        "    \"\"\"\n",
        "    Visualizes the comparison of two heatmaps.\n",
        "\n",
        "    Args:\n",
        "        heatmap1_data (pd.DataFrame): Data for the first heatmap.\n",
        "        heatmap2_data (pd.DataFrame): Data for the second heatmap.\n",
        "        label1 (str): Label for the first heatmap.\n",
        "        label2 (str): Label for the second heatmap.\n",
        "        fig_title (str): Title for the comparison plot.\n",
        "    \"\"\"\n",
        "    # This is a placeholder. Actual comparison visualization will depend on the data structure and desired comparison.\n",
        "    print(f\"Comparison visualization between {label1} and {label2} needs to be implemented based on the specific comparison method (e.g., difference, correlation).\")\n",
        "    # Example: Plotting the difference (requires both heatmaps to have the same structure)\n",
        "    # if heatmap1_data.shape == heatmap2_data.shape:\n",
        "    #     difference_heatmap = heatmap1_data - heatmap2_data\n",
        "    #     plt.figure(figsize=(15, 8))\n",
        "    #     sns.heatmap(difference_heatmap, cmap='coolwarm', center=0, cbar_label='Temperature Difference (°C)')\n",
        "    #     plt.title(f'{fig_title} (Difference: {label1} - {label2})')\n",
        "    #     plt.xlabel('Year')\n",
        "    #     plt.ylabel('Month/Station') # Adjust label based on heatmap structure\n",
        "    #     plt.tight_layout()\n",
        "    #     plt.show()\n",
        "    # else:\n",
        "    #     print(\"Heatmaps have different shapes and cannot be directly subtracted for difference visualization.\")\n",
        "\n",
        "# Set your own coordinates (example using Nairobi coordinates)\n",
        "# my_lat = nairobi_coords[0]\n",
        "# my_lon = nairobi_coords[1]\n",
        "\n",
        "# # Use WMO Stations and get the overall heat in the nearest station\n",
        "# distance, nearest_wmo_data = get_nearest_wmo_station_data(stn_md, eac_wmo_data, my_lat, my_lon)\n",
        "\n",
        "# Extract TMIN, TMAX, TAVG for the nearest station\n",
        "# nearest_wmo_tmin, nearest_wmo_tmax, nearest_wmo_tavg = extract_tmin_tmax_tvg(nearest_wmo_data)\n",
        "\n",
        "# print(\"\\nTemperature data for the nearest WMO station:\")\n",
        "# display(nearest_wmo_tavg.head())\n",
        "\n",
        "\n",
        "# # Visualise the heat change comparison of the 2\n",
        "# # For comparison, let's use the mean annual maximum temperature from WMO stations\n",
        "# # and the mean annual temperature from ERA5\n",
        "# wmo_mean_annual_max = annual_max_temp_stations.mean(axis=1).to_frame(name='WMO Mean Annual Max Temp')\n",
        "\n",
        "# # For ERA5, let's calculate the mean annual temperature over the region\n",
        "# era5_mean_annual = era5_monthly['temperature_2m'].mean(dim=['latitude', 'longitude']).resample(time='Y').mean().to_series().to_frame(name='ERA5 Mean Annual Temp')\n",
        "\n",
        "# # Align the dataframes by year\n",
        "# comparison_df = pd.concat([wmo_mean_annual_max, era5_mean_annual], axis=1)\n",
        "\n",
        "\n",
        "# # Look at all the stations maximum temperature reached over the years as a heatmap\n",
        "# # Ensure annual_max_temp_stations is calculated (if not already)\n",
        "# if 'annual_max_temp_stations' not in locals():\n",
        "#     annual_max_temp_stations = eac_wmo_tmax.resample('Y').max()\n",
        "\n",
        "# plot_wmo_heatmap(annual_max_temp_stations)\n",
        "# # Plot the comparison\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# comparison_df.plot(ax=plt.gca())\n",
        "# plt.title('Mean Annual Temperature Comparison (WMO vs ERA5)')\n",
        "# plt.xlabel('Year')\n",
        "# plt.ylabel('Temperature (°C)')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# Call the compare_heatmaps function (placeholder)\n",
        "# compare_heatmaps(annual_max_temp_stations, heatmap_data, 'WMO Annual Max Temp', 'ERA5 Monthly Mean Temp')\n",
        "\n",
        "# @title Plot WMO Heatmap\n",
        "# Ensure annual_max_temp_stations is calculated (if not already)\n",
        "if 'annual_max_temp_stations' not in locals():\n",
        "    annual_max_temp_stations = ghcnd_tmax.resample('Y').mean()\n",
        "\n",
        "plot_wmo_heatmap(annual_max_temp_stations, 'Annual Average Maximum Temperature Heatmap (WMO Stations)')"
      ],
      "id": "558f87d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qE-1vabxKPpk"
      },
      "id": "qE-1vabxKPpk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}