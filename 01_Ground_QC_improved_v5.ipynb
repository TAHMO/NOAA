{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22dd3d5",
   "metadata": {
    "id": "a22dd3d5"
   },
   "source": [
    "# **Focus Area 1 — Ground Observations Monitoring &amp; QC**\n",
    "**Core Objective**: To equip NMHS participants with tools for quality-controlling ground station\n",
    "data and validating it against satellite products, enabling identification of network issues and\n",
    "building confidence in observational networks."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Overall notebook steps**\n",
    "Set up\n",
    "  - Step 1: Define variables to use in the analysis\n",
    "  - Step 2: Setup – Install dependencies, authenticate Google Drive & Earth Engine\n",
    "\n",
    "Data Extraction\n",
    "  - Step 3: Visualise the area of interest\n",
    "  - Step 4: Upload custom data or extract TAHMO data (Ground stations)\n",
    "  - Step 5: Extract CHIRPS data\n",
    "\n",
    "Data Processing\n",
    "  - Step 6: Visualise data availability for ground stations and handle missing data\n",
    "\n",
    "Data QC and comparison\n",
    "  - Step 7: Aggregate the Ground data to Pentads\n",
    "  - Step 8: Build CHIRPS Nearest data for the ground stations\n",
    "  - Step 9: Visualise CHIRPS vs the Ground stations\n",
    "  - Step 10: Compute station confidence scores"
   ],
   "metadata": {
    "id": "zj_l_eCHoi4C"
   },
   "id": "zj_l_eCHoi4C"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpIwYynL7pRc"
   },
   "source": [
    "### **Station Scoring – Formula (0–100)**\n",
    "We compute per-station metrics on **pentad totals** and convert them to a 0–100 score:\n",
    "\n",
    "- **corr (r)** – higher is better\n",
    "- **rmse** (mm/pentad) – lower is better\n",
    "- **bias** (|gauge − CHIRPS|, mm/pentad) – lower is better\n",
    "- **complete** – fraction of pentads that meet `MIN_PENTAD_DAYS` – higher is better\n",
    "- **outlier** – fraction of pentads outside tolerance – lower is better\n",
    "\n",
    "Weights are set in `SCORING_WEIGHTS` in the Config cell. The final score is:\n",
    "\n",
    "```\n",
    "score = 100 * [\n",
    "  w_corr * norm_corr +\n",
    "  w_rmse * (1 - norm_rmse) +\n",
    "  w_bias * (1 - norm_bias) +\n",
    "  w_complete * complete +\n",
    "  w_outlier * (1 - outlier)\n",
    "]\n",
    "```\n",
    "with simple min–max normalization of rmse/bias over stations.\n"
   ],
   "id": "cpIwYynL7pRc"
  },
  {
   "cell_type": "markdown",
   "id": "qQCR_RarYikB",
   "metadata": {
    "id": "qQCR_RarYikB"
   },
   "source": [
    "### **Instructions to use custom data**\n",
    "\n",
    "To use custom data you require 2 files\n",
    "- The Metadata file: Ground_Metadata.csv\n",
    "- The Ground_station data file: Ground_data.csv\n",
    "\n",
    "**Recommended file format and structure (Columns)**:\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Ground_Metadata.csv</title>\n",
    "</head>\n",
    "<body>\n",
    "    <table border=\"1\">\n",
    "        <tr>\n",
    "            <th>Code</th>\n",
    "            <th>lat</th>\n",
    "            <th>lon</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Station1</td>\n",
    "            <td>1.2345</td>\n",
    "            <td>36.7890</td>\n",
    "        </tr>\n",
    "        <!-- More rows as needed -->\n",
    "    </table>\n",
    "    <p><p>\n",
    "</html>\n",
    "\n",
    "Data file format (Columns): Precipitation data for multiple stations\n",
    "<html>\n",
    "<head>\n",
    "    <title>Ground_data.csv</title>\n",
    "</head>\n",
    "<body>\n",
    "    <table border=\"1\">\n",
    "        <tr>\n",
    "            <th>Date</th>\n",
    "            <th>Station1</th>\n",
    "            <th>Station2</th>\n",
    "            <th>Station3</th>\n",
    "            <!-- More station codes as needed -->\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2023-01-01</td>\n",
    "            <td>25.3</td>\n",
    "            <td>26.1</td>\n",
    "            <td>24.8</td>\n",
    "        </tr>\n",
    "        <!-- More rows as needed -->\n",
    "    </table>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 1: Define variables to use in the analysis**"
   ],
   "metadata": {
    "id": "ScRrrDP7EQ74"
   },
   "id": "ScRrrDP7EQ74"
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "id": "34u1Mya-7pRZ",
    "cellView": "code"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "## === CONFIG (edit here) ======================================================\n",
    "\n",
    "import os, numpy as np\n",
    "from datetime import date\n",
    "from google.colab import drive\n",
    "\n",
    "# Define data source to be used for ground stations data\n",
    "Ground_data_source = \"TAHMO\" # use custom or TAHMO\n",
    "\n",
    "# Country / ROI\n",
    "COUNTRY = 'Uganda'            # 'Kenya' | 'Uganda' | 'Rwanda'\n",
    "ROI_METHOD = 'OSM'           # 'OSM' (default) or 'GMAPS' if API key available\n",
    "\n",
    "# Dates\n",
    "start_date = '2025-04-01'    # ISO format\n",
    "end_date   = '2025-05-31'\n",
    "\n",
    "# Output directory (will be created under Google Drive if mounted)\n",
    "RUN_STAMP = date.today().isoformat()\n",
    "drive.mount('/content/drive')\n",
    "BASE_OUT = f'/content/drive/MyDrive/NOAA_QC_RUN_{COUNTRY}_{RUN_STAMP}'\n",
    "os.makedirs(BASE_OUT, exist_ok=True)\n",
    "\n",
    "# QC thresholds\n",
    "QC_LIMITS = {\n",
    "    'rain_min': 0.0,\n",
    "    'rain_spike_mm': 300.0,    # flag if a single day exceeds this\n",
    "    'flatline_days': 7,        # flag if >=N days unchanged\n",
    "}\n",
    "\n",
    "# Aggregation & matching\n",
    "CHIRPS_AGG = 'pentad'        # currently pentad-based comparison\n",
    "MIN_PENTAD_DAYS = 3          # minimum valid daily values to accept a pentad\n",
    "NEIGHBORHOOD = 3             # 1 (nearest pixel), 3 or 5 for mean of NxN neighborhood\n",
    "\n",
    "# Scoring weights (must sum to 1.0)\n",
    "SCORING_WEIGHTS = {\n",
    "    'corr': 0.5,   # Pearson r (higher is better)\n",
    "    'rmse': 0.25,   # RMSE (lower is better)\n",
    "    'bias': 0.3,   # absolute bias (lower is better)\n",
    "    'complete': 0.1, # pentad completeness (higher is better)\n",
    "    'outlier': 0.1,  # outlier rate (lower is better)\n",
    "}\n",
    "\n",
    "# Correlation method\n",
    "CORR_METHOD='spearman' # Use pearson or spearman\n",
    "\n",
    "# Confidence definition\n",
    "High_confidence_threshold = \"70\"\n",
    "Low_confidence_threshold = \"30\"\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "print('✅ CONFIG set. Outputs will be written to:', BASE_OUT)\n"
   ],
   "id": "34u1Mya-7pRZ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 2: Setup – Install dependencies, authenticate Google Drive & Earth Engine**"
   ],
   "metadata": {
    "id": "ETudoF3fFm0q"
   },
   "id": "ETudoF3fFm0q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1rWq-ngbBVM1",
   "metadata": {
    "id": "1rWq-ngbBVM1",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title ###2a) Install dependencies\n",
    "# @markdown This cell installs the required dependencies for the workshop. It may take a few minutes <br>\n",
    "# @markdown If you encounter any errors, please restart the runtime and try again. <br>\n",
    "# @markdown If the error persists, please seek help.\n",
    "\n",
    "\n",
    "print(\"Installing required dependencies...\")\n",
    "!pip install git+https://github.com/TAHMO/NOAA.git > /dev/null 2>&1\n",
    "!pip install scipy --upgrade > /dev/null 2>&1 # Added to upgrade scipy\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# check there was no error\n",
    "import sys\n",
    "if not sys.argv[0].endswith(\"kernel_launcher.py\"):\n",
    "    print(\"❌ Errors occurred during installation. Please restart the runtime and try again.\")\n",
    "else:\n",
    "    print(\"✅ Dependencies installed successfully.\")\n",
    "\n",
    "print(\"Importing required libraries...\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import ee\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "\n",
    "# import os\n",
    "# os.chdir('NOAA-workshop')\n",
    "\n",
    "from utils.ground_stations import plot_stations_folium\n",
    "from utils.helpers import get_region_geojson\n",
    "from utils.CHIRPS_helpers import get_chirps_pentad_gee\n",
    "from utils.plotting import plot_xarray_data, plot_xarray_data2\n",
    "from google.colab import drive\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✅ Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g0SJyAURCL0u",
   "metadata": {
    "id": "g0SJyAURCL0u"
   },
   "outputs": [],
   "source": [
    "# @title ### 2b) Authenticate Google Drive & Earth Engine {\"display-mode\":\"form\"}\n",
    "# @markdown This step is used to authenticate you as a user and there will be two popups that will be doing this.\n",
    "# @markdown 1. **Authentication to Google Drive** - This is where we shall be loading the data after we have extracted it\n",
    "# @markdown 2. **Authentication to Google Earth Engine** - This will be used to extract the CHIRPS data and any other satellite product we shall be extracting in the future.\n",
    "# @markdown Please check your email we shared an invitation to our Google Cloud Project that we shall be using to extract the data<br>\n",
    "# @markdown *PS: In the future, to create your own project, please refer to [Google Cloud's Documentation](https://developers.google.com/earth-engine/guides/access) that shows the step by step breakdown of creating a Google Cloud Project and enabling Google Earth Engine*\n",
    "# @markdown  Link to configure noncommercial use of Google Earth Engine: https://console.cloud.google.com/earth-engine/configuration <br><br>\n",
    "# @markdown For this workshop, we have created the ```noaa-tahmo``` project that you can input as your project id<br><br><br>\n",
    "\n",
    "print(\"Authenticating to Google Drive...\")\n",
    "import os\n",
    "if not os.path.exists('/content/drive'):\n",
    "  drive.mount('/content/drive')\n",
    "  print(\"✅ Google Drive authenticated successfully.\")\n",
    "else:\n",
    "  print(\"✅ Google Drive already mounted.\")\n",
    "\n",
    "import ee\n",
    "\n",
    "# Authenticate and initialise Google Earth Engine\n",
    "# This will open a link in your browser to grant permissions if necessary.\n",
    "try:\n",
    "    print(\"Authenticating Google Earth Engine. Please follow the instructions in your browser.\")\n",
    "    ee.Authenticate()\n",
    "    print(\"✅ Authentication successful.\")\n",
    "except ee.auth.scopes.MissingScopeError:\n",
    "    print(\"Authentication scopes are missing. Please re-run the cell and grant the necessary permissions.\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication failed: {e}\")\n",
    "\n",
    "# Initialize Earth Engine with your project ID\n",
    "# Replace 'your-project-id' with your actual Google Cloud Project ID\n",
    "# You need to create an unpaid project manually through the Google Cloud Console\n",
    "print(\"\\nIf you already have a project id paste it below. If you do not have a project You need to create an unpaid project manually through the Google Cloud Console\")\n",
    "print(\"💡 You can create a new project here: https://console.cloud.google.com/projectcreate and copy the project id\")\n",
    "try:\n",
    "    # It's recommended to use a project ID associated with your Earth Engine account.\n",
    "    print(\"\\nEnter your Google Cloud Project ID: \")\n",
    "    project_id = input(\"\")\n",
    "    ee.Initialize(project=project_id)\n",
    "    print(\"✅ Google Earth Engine initialized successfully.\")\n",
    "except ee.EEException as e:\n",
    "    if \"PERMISSION_DENIED\" in str(e):\n",
    "        print(f\"Earth Engine initialization failed due to PERMISSION_DENIED.\")\n",
    "        print(\"Please ensure the Earth Engine API is enabled for your project:\")\n",
    "        print(\"Enable the Earth Engine API here: https://console.developers.google.com/apis/api/earthengine.googleapis.com/overview?project=elated-capsule-471808-k1\")\n",
    "    else:\n",
    "        print(f\"Earth Engine initialization failed: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during initialization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "helpers"
    ],
    "id": "uhNs1HS37pRc"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title ### 2c) Loading helper functions {\"display-mode\":\"form\"}\n",
    "\n",
    "# @markdown This cell loads helper functions for the Quality control section\n",
    "\n",
    "# === HELPERS ================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def interquartile_mean(arr):\n",
    "    \"\"\"Return the mean of the central 50% (25th–75th percentile).\"\"\"\n",
    "    a = np.sort(np.asarray(arr).astype(float))\n",
    "    n = len(a)\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    lo = int(np.floor(0.25*n))\n",
    "    hi = int(np.ceil(0.75*n))\n",
    "    hi = max(hi, lo+1)\n",
    "    return float(np.mean(a[lo:hi]))\n",
    "\n",
    "def representative_member(arr):\n",
    "    \"\"\"Return value from arr closest to the interquartile mean.\"\"\"\n",
    "    iqm = interquartile_mean(arr)\n",
    "    if np.isnan(iqm):\n",
    "        return np.nan\n",
    "    arr = np.asarray(arr).astype(float)\n",
    "    return float(arr[np.argmin(np.abs(arr - iqm))])\n",
    "\n",
    "def flag_qc(df, precip_col='precip'):\n",
    "    \"\"\"Add simple QC flags to a daily dataframe with a precipitation column.\"\"\"\n",
    "    out = df.copy()\n",
    "    out['neg_rain'] = out[precip_col] < QC_LIMITS['rain_min']\n",
    "    out['daily_spike'] = out[precip_col] > QC_LIMITS['rain_spike_mm']\n",
    "    # flatline: same value N days in a row\n",
    "    runs = (out[precip_col].diff()!=0).cumsum()\n",
    "    run_lengths = runs.map(runs.value_counts())\n",
    "    out['flatline_Ndays'] = run_lengths >= QC_LIMITS['flatline_days']\n",
    "    return out\n",
    "\n",
    "def qc_summary(df, station_col='station_id', date_col='date', precip_col='precip'):\n",
    "    \"\"\"Summarize QC flags per station.\"\"\"\n",
    "    grp = df.groupby(station_col)\n",
    "    summ = grp.agg(\n",
    "        n_obs=(precip_col, 'size'),\n",
    "        n_missing=(precip_col, lambda s: int(s.isna().sum())),\n",
    "        neg_rain=('neg_rain', 'sum'),\n",
    "        daily_spike=('daily_spike', 'sum'),\n",
    "        flatline=('flatline_Ndays', 'sum'),\n",
    "    ).reset_index()\n",
    "    summ['missing_pct'] = (summ['n_missing'] / summ['n_obs']).round(3)\n",
    "    return summ\n",
    "\n",
    "def classify_score(s):\n",
    "    if s >= 70: return 'High'\n",
    "    if s >= 50: return 'Medium'\n",
    "    return 'Low'\n",
    "\n",
    "print('✅ Helpers loaded')\n",
    "\n",
    "# Loading the config file and parsing from uploaded incase it comes with a different name\n",
    "import json\n",
    "# Updated path to read from the shared drive\n",
    "config_path = '/content/drive/Shareddrives/NOAA-workshop2/config.json'\n",
    "try:\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(f\"✅ Config file loaded successfully from {config_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Config file not found at {config_path}. Please ensure the file exists.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"❌ Error: Could not decode JSON from {config_path}. Please check the file format.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An unexpected error occurred while reading the config file: {e}\")\n"
   ],
   "id": "uhNs1HS37pRc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 3: Visualise the area of interest**"
   ],
   "metadata": {
    "id": "f4Qk0sVnGsk2"
   },
   "id": "f4Qk0sVnGsk2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iIzzKRfS6KMt",
   "metadata": {
    "id": "iIzzKRfS6KMt"
   },
   "outputs": [],
   "source": [
    "# @title ####The Google Maps API will be used to fetch geometry, bounding box, and show the polygon on a map. {\"display-mode\":\"form\"}\n",
    "\n",
    "import time\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# --- Environment Detection ---\n",
    "def in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "IS_COLAB = in_colab()\n",
    "print(f\"💡 Running in {'Google Colab' if IS_COLAB else 'Local Jupyter'} environment.\")\n",
    "\n",
    "try:\n",
    "    with open('/content/config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    location_key = config.get('location_keys', None)\n",
    "except Exception:\n",
    "    location_key = None\n",
    "    print(\"⚠️ Warning: No API key found. Fallback modes will be used.\")\n",
    "\n",
    "def xmin_ymin_xmax_ymax(polygon):\n",
    "    lons = [pt[0] for pt in polygon]\n",
    "    lats = [pt[1] for pt in polygon]\n",
    "    return min(lons), min(lats), max(lons), max(lats)\n",
    "\n",
    "def fetch_region_google(query):\n",
    "    \"\"\"Primary: Fetch polygon geometry via Google Maps API\"\"\"\n",
    "    if not location_key:\n",
    "        raise RuntimeError(\"Missing Google Maps API key.\")\n",
    "    region_geom = get_region_geojson(query, location_key)['geometry']['coordinates'][0]\n",
    "    return region_geom\n",
    "\n",
    "def fetch_region_osm(query):\n",
    "    \"\"\"Fallback: Fetch geometry from OSM (Nominatim) via GeoPandas\"\"\"\n",
    "    url = f\"https://nominatim.openstreetmap.org/search?country={query}&format=geojson&polygon_geojson=1\"\n",
    "    gdf = gpd.read_file(url)\n",
    "    if gdf.empty:\n",
    "        raise ValueError(\"No OSM data found for that query.\")\n",
    "    geom = gdf.iloc[0].geometry\n",
    "    if geom.geom_type == \"Polygon\":\n",
    "        return list(geom.exterior.coords)\n",
    "    elif geom.geom_type == \"MultiPolygon\":\n",
    "        # Select the polygon with the most coordinates\n",
    "        polygons = list(geom.geoms)\n",
    "        if polygons:\n",
    "            largest_polygon = max(polygons, key=lambda p: len(p.exterior.coords))\n",
    "            return list(largest_polygon.exterior.coords)\n",
    "        else:\n",
    "             raise ValueError(\"No polygons found in MultiPolygon from OSM.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported geometry type from OSM.\")\n",
    "\n",
    "\n",
    "def draw_region_interactively():\n",
    "    \"\"\"Manual fallback: let the user draw their ROI\"\"\"\n",
    "    print(\"🖱️ Draw your region on the map (double-click to finish).\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        # ✅ Folium backend (Colab-compatible)\n",
    "        import geemap.foliumap as geemap\n",
    "        from geemap.foliumap import plugins\n",
    "\n",
    "        m = geemap.Map(center=[0, 20], zoom=3)\n",
    "        draw = plugins.Draw(export=True)\n",
    "        draw.add_to(m)\n",
    "        m.add_child(plugins.Fullscreen())\n",
    "        m.add_child(plugins.MeasureControl(primary_length_unit='kilometers'))\n",
    "        m  # Display map in Colab output cell\n",
    "\n",
    "        print(\"✅ Use the draw tools on the left to mark your region.\")\n",
    "        print(\"💾 After drawing, click 'Export' to download your GeoJSON.\")\n",
    "        return m\n",
    "\n",
    "    else:\n",
    "        # ✅ ipyleaflet backend (Local Jupyter)\n",
    "        import geemap\n",
    "        m = geemap.Map(center=[0, 20], zoom=3)\n",
    "        m.add_draw_control()\n",
    "        display(m)\n",
    "        print(\"✅ After drawing, access your shape via `m.user_rois`.\")\n",
    "        return m\n",
    "\n",
    "\n",
    "def show_region_plotly(polygon, region_name=\"Region\", margin=0.05):\n",
    "    \"\"\"Plot polygon with Plotly Mapbox\"\"\"\n",
    "    lons = [pt[0] for pt in polygon]\n",
    "    lats = [pt[1] for pt in polygon]\n",
    "    fig = go.Figure(go.Scattermapbox(\n",
    "        lon=lons + [lons[0]],\n",
    "        lat=lats + [lats[0]],\n",
    "        mode=\"lines\",\n",
    "        fill=\"toself\",\n",
    "        fillcolor=\"rgba(0,0,255,0.3)\",\n",
    "        name=region_name\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        mapbox=dict(center={\"lat\": sum(lats)/len(lats), \"lon\": sum(lons)/len(lons)}, zoom=5),\n",
    "        margin=dict(r=0, t=30, l=0, b=0),\n",
    "        title=f\"Region of Interest: {region_name}\",\n",
    "        height=500,\n",
    "        width=900\n",
    "    )\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "time.sleep(1) # avoid jumps in the input\n",
    "region_query = COUNTRY\n",
    "\n",
    "region_geom = None\n",
    "try:\n",
    "    region_geom = fetch_region_osm(region_query)\n",
    "    print(f\"✅ Geometry fetched via OpenStreetMap for {region_query}\")\n",
    "    # Print the polygon coordinates if fetch_region_osm is successful\n",
    "    print(\"\\nPolygon Coordinates:\")\n",
    "except Exception as e1:\n",
    "    print(f\"⚠️ OSM failed: {e1}\")\n",
    "    try:\n",
    "        region_geom = fetch_region_google(region_query)\n",
    "        print(f\"✅ Geometry fetched via Google Maps API for {region_query}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"⚠️ Google Maps API failed: {e2}\")\n",
    "        print(\"🔁 Launching interactive map draw mode...\")\n",
    "        map_widget = draw_region_interactively()\n",
    "\n",
    "\n",
    "if region_geom:\n",
    "    xmin, ymin, xmax, ymax = xmin_ymin_xmax_ymax(region_geom)\n",
    "    show_region_plotly(region_geom, region_name=region_query)\n",
    "    print(f\"📦 Bounding box -> xmin: {xmin}, ymin: {ymin}, xmax: {xmax}, ymax: {ymax}\")\n",
    "else:\n",
    "    print(\"🛑 No geometry available. Please draw manually or retry another query.\")\n",
    "\n",
    "region_query = region_query.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 4: Upload custom data or extract TAHMO data (Ground stations)**"
   ],
   "metadata": {
    "id": "G_cM56VveUIE"
   },
   "id": "G_cM56VveUIE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c2a8c",
   "metadata": {
    "id": "0d8c2a8c",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title ### 4a_1: Extract and visualise TAHMO data using filter stations\n",
    "\n",
    "# @markdown ❌ SKIP this step if you have your own data\n",
    "\n",
    "# @markdown Documentation: https://filter-stations.netlify.app/\n",
    "\n",
    "# @markdown Using the method ```get_stations_info()``` to extract the metadata and ```multiple_measurements()``` to extract the precipitation data from multiple stations\n",
    "\n",
    "# @markdown At this step we shall also create a directory to start storing the datasets we keep on extracting on Google Drive to easily access and minimize API requests<br>\n",
    "\n",
    "\n",
    "from utils.filter_stations import RetrieveData\n",
    "import os\n",
    "import time\n",
    "\n",
    "dir_path = '/content/drive/MyDrive/NOAA-workshop-data'\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "# check if the path was created successfully\n",
    "if not os.path.exists(dir_path):\n",
    "    print(\"❌ Path not created successfully.\")\n",
    "else:\n",
    "    print(\"✅ Path created successfully.\")\n",
    "\n",
    "# check if the config exists\n",
    "if not os.path.exists('/content/drive/Shareddrives/NOAA-workshop2/config.json'):\n",
    "    print(\"❌ Config file not found. Please upload it first.\")\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def plot_stations_plotly(dataframes, colors=None, zoom=5, height=500,\n",
    "                         width=900, legend_title='Station Locations'):\n",
    "    \"\"\"\n",
    "    Plot stations from one or more dataframes on a Plotly mapbox.\n",
    "\n",
    "    Each dataframe must have 'location.latitude' and 'location.longitude' columns.\n",
    "    'colors' is a list specifying marker colors for each dataframe respectively.\n",
    "    \"\"\"\n",
    "    if colors is None:\n",
    "        colors = [\"blue\", \"red\", \"green\", \"purple\", \"orange\"]\n",
    "\n",
    "    frames = []\n",
    "    for i, df in enumerate(dataframes):\n",
    "        temp = df.copy()\n",
    "        temp[\"color\"] = colors[i % len(colors)]  # cycle colors if more dfs than colors\n",
    "        frames.append(temp)\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    fig = px.scatter_mapbox(\n",
    "        combined,\n",
    "        lat=\"location.latitude\",\n",
    "        lon=\"location.longitude\",\n",
    "        color=\"color\",\n",
    "        hover_name=\"code\",\n",
    "        zoom=zoom,\n",
    "        height=height,\n",
    "        width=width\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        legend_title=legend_title,\n",
    "        margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 0}\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "api_key = config['apiKey']\n",
    "api_secret = config['apiSecret']\n",
    "\n",
    "# Initialize the class\n",
    "rd = RetrieveData(apiKey=api_key,\n",
    "                  apiSecret=api_secret)\n",
    "\n",
    "# Extracting TAHMO data\n",
    "print(\"Extracting TAHMO data...\")\n",
    "info = rd.get_stations_info()\n",
    "\n",
    "# Use region_geom to filter stations\n",
    "from shapely.geometry import Point, shape\n",
    "from shapely.prepared import prep\n",
    "\n",
    "# Create a shapely polygon from region_geom\n",
    "polygon = shape({'type': 'Polygon', 'coordinates': [region_geom]})\n",
    "prepared_polygon = prep(polygon)\n",
    "\n",
    "# Filter the dataframe\n",
    "info = info[info.apply(lambda row: prepared_polygon.contains(Point(row['location.longitude'], row['location.latitude'])), axis=1)].reset_index(drop=True)\n",
    "\n",
    "print(\"✅ TAHMO data extracted successfully.\")\n",
    "# Print the total number of stations\n",
    "print(f\"Total number of stations: {len(info)}\")\n",
    "\n",
    "\n",
    "# save the data as csv to the created directory\n",
    "info.to_csv(f'{dir_path}/Ground_Metadata.csv')\n",
    "\n",
    "# wait for 5 seconds before visual\n",
    "time.sleep(5)\n",
    "\n",
    "# Visualise the data\n",
    "plot_stations_plotly([info], colors=[\"blue\"])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title ### 4a_2: Extracting TAHMO Precipitation data and preview\n",
    "# @markdown Using filter-stations to extract precipitation data <br>\n",
    "# @title ### 4a_1: Extract and visualise TAHMO data using filter stations\n",
    "\n",
    "# @markdown ❌ SKIP this step if you have your own data\n",
    "\n",
    "# save to the directory\n",
    "\n",
    "# print('Extracting Precipitation Data ...')\n",
    "region_precip = rd.multiple_measurements(stations_list=info['code'].tolist(),\n",
    "                                     startDate=start_date,\n",
    "                                     endDate=end_date,\n",
    "                                     variables=['pr'],\n",
    "                                         csv_file=f'{dir_path}/Ground_data')\n",
    "\n",
    "# check if the file exist to know if it was successful\n",
    "if not os.path.exists(f'{dir_path}/Ground_data.csv'):\n",
    "    print(\"❌ Precipitation data not extracted successfully.\")\n",
    "else:\n",
    "    print(\"✅ Precipitation data extracted successfully.\")\n",
    "\n",
    "# Visualise any random station data\n",
    "def plot_stations_data_randomly(eac_data):\n",
    "    import random\n",
    "    station_codes = eac_data.columns.to_list()\n",
    "    random_station = random.choice(station_codes)\n",
    "    station_data = eac_data[random_station]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(station_data.index, station_data.values, marker='o')\n",
    "    plt.title(f'Precipitation Data for Station {random_station}')\n",
    "    plt.xlabel('Date')\n",
    "\n",
    "    # Rotate date labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.ylabel('Precipitation (mm)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Station Code: {random_station}\")\n",
    "    print(f\"Data Range: {station_data.min()} mm to {station_data.max()} mm\")\n",
    "    print(f\"Number of Records: {len(station_data)}\")\n",
    "\n",
    "# plot_stations_data_randomly(region_precip)\n",
    "\n",
    "region_precip"
   ],
   "metadata": {
    "cellView": "form",
    "id": "-Pek2caFhRZ1"
   },
   "id": "-Pek2caFhRZ1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title ### 4b_1) Upload custom data {\"display-mode\":\"form\"}\n",
    "# @markdown ❌ SKIP this step if you are using TAHMO data\n",
    "\n",
    "# @markdown Please upload your Ground_Metadata.csv and Ground_data.csv files.\n",
    "\n",
    "from google.colab import files\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dir_path = '/content/drive/MyDrive/NOAA-workshop-data'\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# check if the path was created successfully\n",
    "if not os.path.exists(dir_path):\n",
    "    print(\"❌ Path not created successfully.\")\n",
    "else:\n",
    "    print(\"✅ Path created successfully.\")\n",
    "\n",
    "# Delete existing files if they exist\n",
    "metadata_path = os.path.join(dir_path, 'Ground_Metadata.csv')\n",
    "data_path = os.path.join(dir_path, 'Ground_data.csv')\n",
    "\n",
    "if os.path.exists(metadata_path):\n",
    "    os.remove(metadata_path)\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    os.remove(data_path)\n",
    "\n",
    "# Change working directory to the target path\n",
    "os.chdir(dir_path)\n",
    "print(f\"✅ Working directory set to: {os.getcwd()}\")\n",
    "\n",
    "# Upload Ground_Metadata.csv\n",
    "print(\"\\nPlease upload Ground_Metadata.csv:\")\n",
    "uploaded_metadata = files.upload()\n",
    "if not uploaded_metadata:\n",
    "    print(\"❌ No file uploaded for Ground_Metadata.csv\")\n",
    "    metadata_file_name = None\n",
    "else:\n",
    "    metadata_file_name = list(uploaded_metadata.keys())[0]\n",
    "    # Rename if needed\n",
    "    if metadata_file_name != 'Ground_Metadata.csv':\n",
    "        os.rename(metadata_file_name, 'Ground_Metadata.csv')\n",
    "        metadata_file_name = 'Ground_Metadata.csv'\n",
    "    print(f\"✅ File saved as 'Ground_Metadata.csv'\")\n",
    "\n",
    "\n",
    "# Upload Ground_data.csv\n",
    "print(\"\\nPlease upload Ground_data.csv:\")\n",
    "uploaded_data = files.upload()\n",
    "if not uploaded_data:\n",
    "    print(\"❌ No file uploaded for Ground_data.csv.\")\n",
    "    data_file_name = None\n",
    "else:\n",
    "    data_file_name = list(uploaded_data.keys())[0]\n",
    "    # Rename if needed\n",
    "    if data_file_name != 'Ground_data.csv':\n",
    "        os.rename(data_file_name, 'Ground_data.csv')\n",
    "        data_file_name = 'Ground_data.csv'\n",
    "    print(f\"✅ File saved as 'Ground_data.csv'\")\n",
    "\n",
    "\n",
    "# --- Validation ---\n",
    "\n",
    "region_metadata = None\n",
    "region_precip_data = None\n",
    "\n",
    "# Validate Ground_Metadata.csv if uploaded\n",
    "if metadata_file_name:\n",
    "    try:\n",
    "        region_metadata = pd.read_csv(metadata_file_name)\n",
    "        required_metadata_cols = ['Code', 'lat', 'lon']\n",
    "        if not all(col in region_metadata.columns for col in required_metadata_cols):\n",
    "            print(f\"❌ Error: Ground_Metadata.csv must contain the columns: {required_metadata_cols}\")\n",
    "            region_metadata = None\n",
    "        else:\n",
    "            print(\"✅ Ground_Metadata.csv columns validated.\")\n",
    "            # Rename columns to match expected format later in the notebook\n",
    "            region_metadata = region_metadata.rename(columns={'Code': 'code'})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading Ground_Metadata.csv: {e}\")\n",
    "        region_metadata = None\n",
    "\n",
    "# Validate Ground_data.csv if uploaded\n",
    "if data_file_name:\n",
    "    try:\n",
    "        region_precip_data = pd.read_csv(data_file_name)\n",
    "        if 'Date' not in region_precip_data.columns:\n",
    "            print(\"❌ Error: Ground_data.csv must contain a 'Date' column.\")\n",
    "            region_precip_data = None\n",
    "        else:\n",
    "            print(\"✅ Ground_data.csv 'Date' column validated.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading Ground_data.csv: {e}\")\n",
    "        region_precip_data = None\n",
    "\n",
    "\n",
    "# Display number of stations if data is loaded\n",
    "if region_precip_data is not None:\n",
    "    # Assuming all columns except 'Date' are stations\n",
    "    station_columns = [col for col in region_precip_data.columns if col != 'Date']\n",
    "    print(f\"\\nTotal number of stations in Ground_data.csv: {len(station_columns)}\")"
   ],
   "metadata": {
    "id": "CThViTecIInu"
   },
   "id": "CThViTecIInu",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title ### 4b_2) Visualise and preview custom data {\"display-mode\":\"form\"}\n",
    "# @markdown ❌ SKIP this step if you are using TAHMO data\n",
    "\n",
    "# @markdown Please upload your Ground_Metadata.csv and Ground_data.csv files.\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def plot_stations_plotly(dataframes, colors=None, zoom=5, height=500,\n",
    "                         width=900, legend_title='Station Locations'):\n",
    "    \"\"\"\n",
    "    Plot stations from one or more dataframes on a Plotly mapbox.\n",
    "\n",
    "    Each dataframe must have 'lat' and 'lon' columns.\n",
    "    'colors' is a list specifying marker colors for each dataframe respectively.\n",
    "    \"\"\"\n",
    "    if colors is None:\n",
    "        colors = [\"blue\", \"red\", \"green\", \"purple\", \"orange\"]\n",
    "\n",
    "    frames = []\n",
    "    for i, df in enumerate(dataframes):\n",
    "        temp = df.copy()\n",
    "        temp[\"color\"] = colors[i % len(colors)]  # cycle colors if more dfs than colors\n",
    "        frames.append(temp)\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    fig = px.scatter_mapbox(\n",
    "        combined,\n",
    "        lat=\"lat\",\n",
    "        lon=\"lon\",\n",
    "        color=\"color\",\n",
    "        hover_name=\"Code\",\n",
    "        zoom=zoom,\n",
    "        height=height,\n",
    "        width=width\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        legend_title=legend_title,\n",
    "        margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 0}\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Load the custom data files\n",
    "try:\n",
    "    region_metadata = pd.read_csv('/content/drive/MyDrive/NOAA-workshop-data/Ground_Metadata.csv')\n",
    "    region_precip_data = pd.read_csv('/content/drive/MyDrive/NOAA-workshop-data/Ground_data.csv')\n",
    "\n",
    "    # Display the first 5 rows of the ground data\n",
    "    print(\"Preview of first 5 rows of Ground_data.csv:\")\n",
    "    display(region_precip_data.head())\n",
    "\n",
    "    # Visualise the custom metadata\n",
    "    print(\"\\nVisualizing custom station locations:\")\n",
    "\n",
    "    # Check if metadata is loaded and has required columns and data\n",
    "    if region_metadata is None or region_metadata.empty or not all(col in region_metadata.columns for col in ['lat', 'lon']):\n",
    "        print(\"❌ Metadata not loaded correctly or is empty or missing 'lat' or 'lon' columns. Please check your Ground_Metadata.csv file.\")\n",
    "    elif region_metadata[['lat', 'lon']].isnull().any().any():\n",
    "         print(\"❌ Metadata contains missing latitude or longitude values. Please check your Ground_Metadata.csv file.\")\n",
    "    else:\n",
    "        fig = plot_stations_plotly([region_metadata], colors=[\"blue\"])\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Custom data files not found. Please upload Ground_Metadata.csv and Ground_data.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred while processing custom data: {e}\")"
   ],
   "metadata": {
    "id": "cH1UHZUUTLY-"
   },
   "id": "cH1UHZUUTLY-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 5: Extract CHIRPS data**"
   ],
   "metadata": {
    "id": "i9qeFKFSJ5j-"
   },
   "id": "i9qeFKFSJ5j-"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ab5da",
   "metadata": {
    "id": "df9ab5da",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title ### Extract the data from Google Earth engine project\n",
    "\n",
    "# @markdown **Note**: For this workshop, we have done this process for you when we shared the Google Cloud invitation to the project\n",
    "\n",
    "# --- 4. Convert downloaded tiffs to xarray ---\n",
    "\n",
    "# import geemap\n",
    "# import glob\n",
    "# from tqdm.notebook import tqdm\n",
    "#\n",
    "# # Define region of interest (Kenya)\n",
    "# roi = ee.Geometry.Polygon(region_geom)\n",
    "#\n",
    "# # --- 2. Set up CHIRPS collection ---\n",
    "# collection_id = \"UCSB-CHG/CHIRPS/PENTAD\"\n",
    "# chirps = ee.ImageCollection(collection_id).filterDate(start_date, end_date).select(\"precipitation\")\n",
    "# chirps = chirps.map(lambda img: img.clip(roi))\n",
    "#\n",
    "# export_dir = \"chirps_temp\"\n",
    "# os.makedirs(export_dir, exist_ok=True)\n",
    "#\n",
    "# # Convert to Python list\n",
    "# image_list = chirps.toList(chirps.size())\n",
    "# n_images = image_list.size().getInfo()\n",
    "#\n",
    "# # --- 3. Download with progress bar ---\n",
    "# pbar = tqdm(total=n_images, desc=\"Exporting CHIRPS\", unit=\"file\")\n",
    "#\n",
    "# for i in range(n_images):\n",
    "#     img = ee.Image(image_list.get(i))\n",
    "#     date_str = img.date().format(\"yyyyMMdd\").getInfo()\n",
    "#     out_file = os.path.join(export_dir, f\"{date_str}.tif\")\n",
    "#\n",
    "#     geemap.ee_export_image(\n",
    "#         img,\n",
    "#         filename=out_file,\n",
    "#         scale=5500,\n",
    "#         region=roi\n",
    "#     )\n",
    "#\n",
    "#     pbar.update(1)\n",
    "#     pbar.refresh()  # force redraw in Colab\n",
    "#\n",
    "# pbar.close()\n",
    "#\n",
    "# # --- 4. Convert downloaded tiffs to xarray ---\n",
    "# tiff_files = sorted(glob.glob(os.path.join(export_dir, \"*.tif\")))\n",
    "# ds = xr.open_mfdataset(tiff_files, combine=\"nested\", concat_dim=\"time\", engine=\"rasterio\")\n",
    "#\n",
    "# dates = [os.path.basename(f).split(\".\")[0] for f in tiff_files]\n",
    "# ds = ds.assign_coords(time=pd.to_datetime(dates, format=\"%Y%m%d\"))\n",
    "# ds = ds.squeeze(\"band\", drop=True).rename({\"band_data\": \"precipitation\"})\n",
    "# ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "#\n",
    "# # --- 5. Save as NetCDF ---\n",
    "# export_path = f'{dir_path}/chirps_pentad_{region_query}.nc'\n",
    "# ds.to_netcdf(export_path)\n",
    "#\n",
    "# # --- 6. Clean data (remove imputed values) ---\n",
    "# chirps_ds = ds.where(ds != -9999)\n",
    "\n",
    "chirps_ds = xr.open_dataset(f'/content/drive/Shared drives/NOAA-workshop2/Datasets/{COUNTRY}/chirps_pentad_{region_query}.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 6: Visualise data availability for ground stations and handle missing data**\n"
   ],
   "metadata": {
    "id": "lBqtW_XpMBJh"
   },
   "id": "lBqtW_XpMBJh"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df83e99",
   "metadata": {
    "id": "4df83e99",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title ### 6a) Load the extracted CHIRPS and Ground Precipitation and Metadata\n",
    "\n",
    "# from scipy.stats import pearsonr, ttest_rel\n",
    "# handle runtime disconnected issues to begin from this step\n",
    "# 1. check if region query is defined\n",
    "if not 'region_query' in locals():\n",
    "    region_query = input(\"🌍 It seems the runtime was disconnected please enter the region you had selected again: \")\n",
    "    print('Re-Installing required dependencies ...')\n",
    "    !pip install git+https://github.com/TAHMO/NOAA.git > /dev/null 2>&1\n",
    "    import xarray as xr\n",
    "    import time\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    # import ee\n",
    "    import numpy as np\n",
    "    # check if config.json file is already available\n",
    "    if not os.path.exists('/content/config.json'):\n",
    "      from google.colab import files\n",
    "      # Upload the TAHMO config file\n",
    "      print('Please upload the config file provided ...')\n",
    "      uploaded = files.upload()\n",
    "\n",
    "    # load the config.json file\n",
    "    with open('/content/config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    location_key = config['location_keys']  # Google Maps API key\n",
    "    api_key = config['apiKey']\n",
    "    api_secret = config['apiSecret']\n",
    "\n",
    "\n",
    "    time.sleep(2)\n",
    "    # region_query = input(\"🌍 It seems the runtime was disconnected please write your region you had put againe: \")\n",
    "    # print(f\"Selected: {region_query}\")\n",
    "    # region_query = region_query.lower()\n",
    "\n",
    "    # re importing the modules\n",
    "\n",
    "\n",
    "    # import os\n",
    "    # os.chdir('NOAA-workshop')\n",
    "\n",
    "\n",
    "    from utils.ground_stations import plot_stations_folium\n",
    "    from utils.helpers import get_region_geojson\n",
    "    from utils.CHIRPS_helpers import get_chirps_pentad_gee\n",
    "    from utils.plotting import plot_xarray_data, plot_xarray_data2\n",
    "    from utils.filter_stations import RetrieveData\n",
    "\n",
    "    from google.colab import drive\n",
    "\n",
    "    def xmin_ymin_xmax_ymax(polygon):\n",
    "      lons = [pt[0] for pt in polygon]\n",
    "      lats = [pt[1] for pt in polygon]\n",
    "      return min(lons), min(lats), max(lons), max(lats)\n",
    "\n",
    "    def fetch_region(query):\n",
    "      \"\"\"Fetch polygon geometry and bounding box for region name\"\"\"\n",
    "      region_geom = get_region_geojson(query, location_key)['geometry']['coordinates'][0]\n",
    "      xmin, ymin, xmax, ymax = xmin_ymin_xmax_ymax(region_geom)\n",
    "      print(f\"✅ Selected: {query}\")\n",
    "      print(f\"Bounding box -> xmin: {xmin}, ymin: {ymin}, xmax: {xmax}, ymax: {ymax}\")\n",
    "      return region_geom, (xmin, ymin, xmax, ymax)\n",
    "\n",
    "    # remounting drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    # check if the drve folder is available in /content/drive\n",
    "    if not os.path.exists('/content/drive/MyDrive/NOAA-workshop-data'):\n",
    "      print(\"❌ Google Drive not mounted successfully.\")\n",
    "    else:\n",
    "      print(\"✅ Google Drive mounted successfully.\")\n",
    "\n",
    "    rd = RetrieveData(api_key,\n",
    "                    api_secret)\n",
    "\n",
    "    region_geom, bbox = fetch_region(region_query)\n",
    "\n",
    "    xmin, ymin, xmax, ymax = xmin_ymin_xmax_ymax(region_geom)\n",
    "\n",
    "    dir_path = '/content/drive/MyDrive/NOAA-workshop-data'\n",
    "\n",
    "    # check if the metadata, chirps and precip data is available\n",
    "    if not os.path.exists(f'{dir_path}/tahmo_precip_{region_query}.csv'):\n",
    "      print(\"❌ Precipitation data not extracted successfully.\")\n",
    "    else:\n",
    "      print(\"✅ Precipitation data extracted successfully.\")\n",
    "\n",
    "    if not os.path.exists(f'{dir_path}/tahmo_metadata_{region_query}.csv'):\n",
    "      print(\"❌ Metadata not extracted successfully.\")\n",
    "    else:\n",
    "      print(\"✅ Metadata extracted successfully.\")\n",
    "\n",
    "    if not os.path.exists(f'{dir_path}/chirps_pentad_{region_query}.nc'):\n",
    "      print(\"❌ CHIRPS data not extracted successfully.\")\n",
    "    else:\n",
    "      print(\"✅ CHIRPS data extracted successfully.\")\n",
    "\n",
    "import xarray as xr\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import ee\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "region_precip_data = pd.read_csv(f'{dir_path}/Ground_data.csv')\n",
    "# eac metadata\n",
    "region_metadata = pd.read_csv(f\"{dir_path}/Ground_Metadata.csv\")\n",
    "if Ground_data_source.lower() == 'tahmo':\n",
    "  region_metadata = region_metadata[['code', 'location.latitude', 'location.longitude']].rename(columns={'location.latitude': 'lat', 'location.longitude': 'lon'})\n",
    "elif Ground_data_source.lower() == 'custom':\n",
    "  region_metadata = region_metadata.rename(columns={'Code': 'code'})\n",
    "  region_metadata = region_metadata[['code', 'lat', 'lon']]\n",
    "  # Rename the date column for consistency\n",
    "  region_precip_data = region_precip_data.rename(columns={\"Date\": \"Unnamed: 0\"})\n",
    "\n",
    "\n",
    "# CHIRPS Xarray dataset\n",
    "chirps_ds = xr.open_dataset(f'/content/drive/Shared drives/NOAA-workshop2/Datasets/{COUNTRY}/chirps_pentad_{region_query}.nc')\n",
    "chirps_ds = chirps_ds.where(chirps_ds != -9999)\n",
    "\n",
    "# Format the TAHMO data\n",
    "region_precip_data = region_precip_data.rename(columns={\"Unnamed: 0\": \"Date\"})\n",
    "region_precip_data['Date'] = pd.to_datetime(region_precip_data['Date'], format='mixed', dayfirst=True)\n",
    "region_precip_data = region_precip_data.set_index('Date')\n",
    "region_precip_data.index = region_precip_data.index.tz_localize(None)\n",
    "multiple_sensors = [i for i in region_precip_data.columns if len(i.split('_')) > 1]\n",
    "region_precip_data = region_precip_data.drop(columns=multiple_sensors)\n",
    "# region_precip_data = region_precip_data.dropna(how='all', axis=1)\n",
    "\n",
    "print(\"✅ Step successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706999fe",
   "metadata": {
    "cellView": "form",
    "id": "706999fe"
   },
   "outputs": [],
   "source": [
    "# @title ### 6b) Check the range of the data and plot any of the stations randomly\n",
    "# @markdown Rerun this cell to visualise the next random station in the region <br>\n",
    "def plot_stations_data_randomly(eac_data):\n",
    "    import random\n",
    "    station_codes = eac_data.columns.to_list()\n",
    "    random_station = random.choice(station_codes)\n",
    "    station_data = eac_data[random_station]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(station_data.index, station_data.values, marker='o')\n",
    "    plt.title(f'Precipitation Data for Station {random_station}')\n",
    "    plt.xlabel('Date')\n",
    "\n",
    "    # Rotate date labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.ylabel('Precipitation (mm)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Station Code: {random_station}\")\n",
    "    print(f\"Data Range: {station_data.min()} mm to {station_data.max()} mm\")\n",
    "    print(f\"Number of Records: {len(station_data)}\")\n",
    "\n",
    "plot_stations_data_randomly(region_precip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba309f",
   "metadata": {
    "id": "3dba309f"
   },
   "outputs": [],
   "source": [
    "# @title ### 6c) Visualise missing data for each station {\"display-mode\":\"form\"}\n",
    "# @markdown The generated chart shows the available data on y axis and a heatmap of available\n",
    "# Get the stations with missing data\n",
    "# missing_stations = region_precip_data.columns[region_precip_data.isna().any()].tolist()\n",
    "# # print(f\"Stations with missing data: {missing_stations}\")\n",
    "# # drop stations with all missing data\n",
    "# region_precip_data = region_precip_data.dropna(how='all', axis=1)\n",
    "# region_precip_data.isna().sum().sort_values().plot(kind='bar', title='Missing Data Count per Variable', figsize=(40, 10))\n",
    "# # Get the stations with missing data\n",
    "# missing_stations = region_precip_data.columns[region_precip_data.isna().any()].tolist()\n",
    "# print(f\"Stations with missing data: {missing_stations}\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Drop stations with multiple sensors station_sensor columns\n",
    "multiple_sensors = [i for i in region_precip_data.columns if len(i.split('_')) > 1]\n",
    "region_precip_data_filtered = region_precip_data.drop(columns=multiple_sensors)\n",
    "\n",
    "# Calculate missing data percentage per station over time\n",
    "missing_data = region_precip_data_filtered.isnull().T\n",
    "missing_data.columns = list(missing_data.columns.tz_localize(None).astype(str))\n",
    "\n",
    "# Calculate overall completeness percentage for each station\n",
    "overall_completeness = (1 - region_precip_data_filtered.isnull().mean(axis=0)) * 100\n",
    "\n",
    "# Sort stations by overall completeness in descending order\n",
    "sorted_stations = overall_completeness.sort_values(ascending=False).index\n",
    "missing_data_sorted = missing_data.loc[sorted_stations]\n",
    "\n",
    "# Determine the number of stations to dynamically adjust height\n",
    "num_stations = len(missing_data_sorted)\n",
    "# Define a base height and an additional height per station\n",
    "base_height = 5  # Base height in inches\n",
    "height_per_station = 0.05 # Additional height per station in inches\n",
    "fig_height = base_height + num_stations * height_per_station\n",
    "\n",
    "# Create a custom colormap: Blue for non-missing (False), Red for missing (True)\n",
    "cmap = matplotlib.colors.ListedColormap(['blue', 'red'])\n",
    "\n",
    "plt.figure(figsize=(12, fig_height))\n",
    "# Use the custom colormap and specify levels for the two colors, and remove the legend\n",
    "sns.heatmap(missing_data_sorted, cbar=False, cmap=cmap, vmin=0, vmax=1)\n",
    "\n",
    "# Create legend patches\n",
    "blue_patch = mpatches.Patch(color='blue', label='Available Data')\n",
    "red_patch = mpatches.Patch(color='red', label='Missing Data')\n",
    "\n",
    "# Add legend to the plot\n",
    "plt.legend(handles=[blue_patch, red_patch], title=\"Data Status\", loc='upper right')\n",
    "\n",
    "# Update y-axis labels to include completeness percentage\n",
    "ax = plt.gca()\n",
    "y_labels = [f\"{label.get_text()} ({overall_completeness.loc[label.get_text()]:.1f}%)\" for label in ax.get_yticklabels()]\n",
    "ax.set_yticklabels(y_labels)\n",
    "\n",
    "plt.title('Missing Data Heatmap by Station and Date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Station (Completeness)')\n",
    "plt.xticks(rotation=90) # Rotate x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title ### 6d) Ground Satellite Map visual comparison\n",
    "from utils.plotting_point import point_plot\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "# Weather points\n",
    "# Slice the resampled ground station data to match the number of time steps in chirps_ds\n",
    "num_chirps_timesteps = len(chirps_ds.time)\n",
    "ground_data_for_plot = region_precip_data.resample('5D').sum().iloc[:num_chirps_timesteps]\n",
    "\n",
    "\n",
    "html_anim = point_plot(\n",
    "    ground_data_for_plot,\n",
    "    region_metadata,\n",
    "    variable_name=\"Rainfall (mm)\", # This is the point data variable name\n",
    "    metadata_columns=['code', 'lat', 'lon'],\n",
    "    cmap=\"plasma\",\n",
    "    grid_da=chirps_ds,\n",
    "    grid_cmap=\"coolwarm\",\n",
    "    grid_alpha=0.5,\n",
    "    fig_title=\"Station Precipitation vs CHIRPS Background\",\n",
    "    grid_da_var='precipitation'\n",
    ")\n",
    "\n",
    "html_anim"
   ],
   "metadata": {
    "cellView": "form",
    "id": "YlS3W08fKHfP"
   },
   "id": "YlS3W08fKHfP",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x2M5zFfKDl61",
   "metadata": {
    "id": "x2M5zFfKDl61"
   },
   "source": [
    "# **Step 7: Aggregate the Ground data to Pentads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c165c6",
   "metadata": {
    "id": "38c165c6"
   },
   "outputs": [],
   "source": [
    "# @title Aggregate station rain to pentads to match CHIRPS. {\"display-mode\":\"form\"}\n",
    "# @markdown In order to match CHIRPS and how it is extracted we will aggregate the data to pentads (5 days)\n",
    "region_precip_pentad = rd.aggregate_variables(region_precip_data, freq='5D', method='sum')[:-1]\n",
    "plot_stations_data_randomly(region_precip_pentad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 8: Build CHIRPS Nearest data for the ground stations**"
   ],
   "metadata": {
    "id": "dXZU2pZnOzYC"
   },
   "id": "dXZU2pZnOzYC"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4d0a3",
   "metadata": {
    "id": "0af4d0a3",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title ### At this step we are building the CHIRPS station dataset by using the station metadata (latitude and longitude) to get the nearest value of CHIRPS\n",
    "# filter to the required stations\n",
    "req_stations = region_precip_pentad.columns.tolist()\n",
    "\n",
    "region_metadata = region_metadata[region_metadata['code'].isin(req_stations)]\n",
    "\n",
    "def build_chirps_from_stations(chirps_ds, stations_metadata, k=NEIGHBORHOOD):\n",
    "    \"\"\"\n",
    "    Build CHIRPS station dataset by sampling CHIRPS values at station locations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chirps_ds : xarray.Dataset\n",
    "        CHIRPS dataset with coordinates ('x', 'y', 'time')\n",
    "    stations_metadata : pd.DataFrame\n",
    "        Must contain columns ['code', 'lat', 'lon']\n",
    "    k : int, optional\n",
    "        Number of nearest CHIRPS grid points to average. Default is 1 (just nearest pixel).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    all_stations_data = {}\n",
    "\n",
    "    # Extract coordinate arrays for CHIRPS grid\n",
    "    chirps_lons = chirps_ds['x'].values\n",
    "    chirps_lats = chirps_ds['y'].values\n",
    "\n",
    "    for _, row in stations_metadata.iterrows():\n",
    "        station_code = row['code']\n",
    "        lat = row['lat']\n",
    "        lon = row['lon']\n",
    "\n",
    "        # Compute distances (simple Euclidean approximation — fine for small areas)\n",
    "        dist = np.sqrt((chirps_lats[:, None] - lat)**2 + (chirps_lons[None, :] - lon)**2)\n",
    "        dist_flat = dist.ravel()\n",
    "\n",
    "        # Get the k nearest pixel indices\n",
    "        nearest_indices = np.argsort(dist_flat)[:k]\n",
    "\n",
    "        # Convert flat indices back to 2D (lat, lon)\n",
    "        lat_idx, lon_idx = np.unravel_index(nearest_indices, dist.shape)\n",
    "\n",
    "        # Extract data for those k pixels\n",
    "        da_values = [chirps_ds.precipitation.isel(y=lat_i, x=lon_i) for lat_i, lon_i in zip(lat_idx, lon_idx)]\n",
    "\n",
    "        # Compute mean across k nearest points\n",
    "        station_da = sum(da_values) / len(da_values)\n",
    "        station_df = station_da.to_dataframe().rename(columns={'precipitation': station_code})\n",
    "\n",
    "        all_stations_data[station_code] = station_df[station_code]\n",
    "\n",
    "    combined_df = pd.DataFrame(all_stations_data)\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "chirps_stations = build_chirps_from_stations(chirps_ds, region_metadata)\n",
    "# plot_stations_data_randomly(chirps_stations)\n",
    "chirps_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 9: Visualise CHIRPS vs the Ground stations**"
   ],
   "metadata": {
    "id": "muIgelKyPLQ7"
   },
   "id": "muIgelKyPLQ7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfe184",
   "metadata": {
    "id": "92bfe184",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title ### Plot the CHIRPS data and TAHMO equivalents for a random station\n",
    "# @markdown Every single time we run this cell we get to visualise the CHIRPS v Ground data comparison for a different station within the region\n",
    "def plot_chirps_vs_tahmo_randomly(eac_pentad, chirps_stations, station_code=None):\n",
    "    if station_code is None:\n",
    "      import random\n",
    "      station_codes = eac_pentad.columns.intersection(chirps_stations.columns).tolist()\n",
    "      if not station_codes:\n",
    "          print(\"No common stations between EAC pentad data and CHIRPS data.\")\n",
    "          return\n",
    "\n",
    "      random_station = random.choice(station_codes)\n",
    "    else:\n",
    "      random_station = station_code\n",
    "\n",
    "    tahmo_data = eac_pentad[random_station]\n",
    "    chirps_data = chirps_stations[random_station]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(tahmo_data.index, tahmo_data.values, marker='o', label='TAHMO', color='blue')\n",
    "    plt.plot(chirps_data.index, chirps_data.values, marker='x', label='CHIRPS', color='orange')\n",
    "    plt.title(f'Precipitation Comparison for Station {random_station}')\n",
    "    plt.xlabel('Date')\n",
    "\n",
    "    # Rotate date labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.ylabel('Precipitation (mm)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Station Code: {random_station}\")\n",
    "    print(f\"TAHMO Data Range: {tahmo_data.min()} mm to {tahmo_data.max()} mm\")\n",
    "    print(f\"CHIRPS Data Range: {chirps_data.min()} mm to {chirps_data.max()} mm\")\n",
    "    print(f\"Number of TAHMO Records: {len(tahmo_data)}\")\n",
    "    print(f\"Number of CHIRPS Records: {len(chirps_data)}\")\n",
    "\n",
    "plot_chirps_vs_tahmo_randomly(region_precip_pentad, chirps_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 10: Compute station confidence scores**"
   ],
   "metadata": {
    "id": "lHrtMusDPXPo"
   },
   "id": "lHrtMusDPXPo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758dbbaf",
   "metadata": {
    "cellView": "form",
    "id": "758dbbaf"
   },
   "outputs": [],
   "source": [
    "# @title ### 10a) Run this cell to compute station scores\n",
    "# @title Confidence Score Formula\n",
    "# @markdown ### **Confidence Score Formula**\n",
    "# @markdown For each station, the confidence score is computed as:\n",
    "# @markdown\n",
    "# @markdown $$\n",
    "# @markdown \\text{Confidence Score} = 100 \\times \\big(0.1 \\cdot C \\;+\\; 0.5 \\cdot r \\;+\\; 0.1 \\cdot (1 - O) \\;+\\; 0.3 \\cdot(1- b) \\big)\n",
    "# @markdown $$\n",
    "# @markdown\n",
    "# @markdown Where:\n",
    "# @markdown - $C = 1 -$ (missing fraction of observations), i.e. **completeness**\n",
    "# @markdown - $r =$ Spearman correlation coefficient between observed and CHIRPS pentads\n",
    "# @markdown - $O =$ fraction of outliers (observed values < 0)\n",
    "# @markdown - $b =$ Bias Significance - paired $t$-test between observed and simulated values (significant if $p < 0.05$)\n",
    "# @markdown\n",
    "# @markdown ---\n",
    "# @markdown\n",
    "# @markdown **Additional metrics per station:**\n",
    "# @markdown\n",
    "# @markdown - **RMSE**\n",
    "# @markdown $$\n",
    "# @markdown \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum (obs - sim)^2}\n",
    "# @markdown $$\n",
    "# @markdown\n",
    "# @markdown - **Adjusted confidence score**\n",
    "# @markdown $$\n",
    "# @markdown \\text{Adjusted Confidence Score} = \\text{Confidence Score} - \\text{RMSE}\n",
    "# @markdown $$\n",
    "# @markdown\n",
    "\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "# spearman correlation\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_station_scores(eac_pentad, chirps_stations): # returns a dataframe of scores 'station_id','confidence_score','pearson_r','rmse','bias_signif'\n",
    "    # get the union stations\n",
    "    stations = eac_pentad.columns.intersection(chirps_stations.columns)\n",
    "    if stations.empty:\n",
    "        print(\"No common stations between EAC pentad data and CHIRPS data.\")\n",
    "        return None\n",
    "    # choose only the matching dates by checking the index\n",
    "    union_dates = eac_pentad.index.intersection(chirps_stations.index)\n",
    "    # print()\n",
    "    eac_pentad = eac_pentad.loc[union_dates]\n",
    "    chirps_stations = chirps_stations.loc[union_dates]\n",
    "    scores = []\n",
    "    for station in stations:\n",
    "        obs = eac_pentad[station]\n",
    "        sim = chirps_stations[station]\n",
    "\n",
    "        if CORR_METHOD.lower()=='pearson':\n",
    "            corr, _ = pearsonr(obs, sim)\n",
    "        else:\n",
    "            corr, _ = spearmanr(obs, sim)\n",
    "\n",
    "        # Compute metrics\n",
    "        error = rmse(obs, sim)\n",
    "        t_stat, p_val = ttest_rel(obs, sim)\n",
    "        bias_signif = 'Significant' if p_val < 0.05 else 'Not Significant'\n",
    "        completeness = 1 - obs.isna().mean()\n",
    "        outlier_rate = (obs < 0).mean()\n",
    "        #         confidence_score = 100 * (0.1 * completeness + 0.5 * np.nan_to_num(r, nan=0) + 0.1 * (1 - outlier_rate)) previous\n",
    "\n",
    "        confidence_score = 100 * (0.1 * completeness + 0.5 * np.nan_to_num(corr, nan=0) + 0.1 * (1 - outlier_rate)+ 0.3*(1-p_val))\n",
    "        scores.append(\n",
    "            {\n",
    "                'station_id': station,\n",
    "                'confidence_score': confidence_score,\n",
    "                f'{CORR_METHOD}_r': corr,\n",
    "                'rmse': error,\n",
    "                'bias_signif': bias_signif,\n",
    "                'completeness': completeness,\n",
    "                'outlier_rate': outlier_rate\n",
    "            }\n",
    "        )\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    return scores_df\n",
    "\n",
    "# @title RMSE\n",
    "def rmse(a,b):\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    return float(np.sqrt(np.nanmean((a-b)**2)))\n",
    "\n",
    "\n",
    "station_scores = compute_station_scores(region_precip_pentad, chirps_stations)\n",
    "# to the confidence scores subtract the rmse\n",
    "station_scores['adjusted_confidence_score'] = station_scores['confidence_score'] - station_scores['rmse']\n",
    "station_scores.sort_values(by='adjusted_confidence_score', ascending=False).head(20)\n",
    "\n",
    "# save the stations with the confidence scores\n",
    "station_scores.to_csv(f'{dir_path}/station_scores_{region_query}.csv', index=False)\n",
    "\n",
    "\n",
    "'''\n",
    "1. Compute the RMSE\n",
    "2. Compute the Pearson correlation coefficient\n",
    "3. Compute the ttest for bias\n",
    "    bias significance (p<0.05) ['Significant' if p_val < 0.05 else 'Not Significant']\n",
    "4. Compute completeness (% of non-missing values)\n",
    "        completeness = 1 - g['station_pentad'].isna().mean()\n",
    "        outlier_rate = (g['station_pentad']<0).mean()\n",
    "5. confidence_score = 100*(0.4*completeness + 0.4*np.nan_to_num(r, nan=0) + 0.2*(1-outlier_rate))\n",
    "'''\n",
    "station_scores.sort_values(by='confidence_score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title 10b) Export the confidence scores results to excel file\n",
    "from google.colab import files\n",
    "print('Exporting station scores to Excel ...')\n",
    "station_scores.to_excel(f'{dir_path}/station_scores_{region_query}.xlsx', index=False)\n",
    "files.download(f'{dir_path}/station_scores_{region_query}.xlsx')\n",
    "\n",
    "# print('✅ Su')"
   ],
   "metadata": {
    "cellView": "form",
    "id": "ppduUJOJBDAT"
   },
   "id": "ppduUJOJBDAT",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87721cd4",
   "metadata": {
    "cellView": "form",
    "id": "87721cd4"
   },
   "outputs": [],
   "source": [
    "# @title 10c) Visualise station confidence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_station_vs_chirps_and_confidence_interactive(\n",
    "    station_pentad_df,\n",
    "    chirps_pentad_df,\n",
    "    station_scores_df,\n",
    "    metadata_df,\n",
    "    country_label=\"EAC (Kenya/Uganda/Rwanda)\",\n",
    "    high_thr=70,\n",
    "    low_thr=40,\n",
    "    min_pairs=3,\n",
    "    scatter_color=\"#3366cc\",\n",
    "    width=900, # Add width parameter\n",
    "    height=600 # Add height parameter\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter: single color (no category legend)\n",
    "    Geo map: always shows High / Medium / Low categories\n",
    "    Returns:\n",
    "      m_long, score_map, scatter_fig, map_fig\n",
    "    \"\"\"\n",
    "    # -------- 1. Build long paired dataset --------\n",
    "    common = station_pentad_df.columns.intersection(chirps_pentad_df.columns)\n",
    "    rows = []\n",
    "    for sid in common:\n",
    "        s_obs = station_pentad_df[sid]\n",
    "        s_sat = chirps_pentad_df[sid]\n",
    "        pair = pd.DataFrame({'station_pentad': s_obs, 'sat_pentad': s_sat}).dropna()\n",
    "        if len(pair) >= min_pairs:\n",
    "            pair['station_id'] = sid\n",
    "            pair['time'] = pair.index\n",
    "            rows.append(pair)\n",
    "    if not rows:\n",
    "        print(\"No stations with sufficient paired data.\")\n",
    "        return None, None, None, None\n",
    "    m_long = pd.concat(rows, axis=0).reset_index(drop=True)\n",
    "\n",
    "    # -------- 2. Always compute classification for map --------\n",
    "    sc = station_scores_df.copy()\n",
    "    if 'class' not in sc.columns:\n",
    "        def classify_fn(v):\n",
    "            if v >= high_thr: return 'High'\n",
    "            if v >= low_thr: return 'Medium'\n",
    "            return 'Low'\n",
    "        sc['class'] = sc['confidence_score'].apply(classify_fn)\n",
    "    # enforce category ordering even if some missing\n",
    "    cls_order = ['High','Medium','Low']\n",
    "    sc['class'] = pd.Categorical(sc['class'], categories=cls_order, ordered=True)\n",
    "\n",
    "    # -------- 3. Merge with metadata --------\n",
    "    meta = metadata_df.rename(columns={'code':'station_id'})\n",
    "    score_map = sc.merge(meta, on='station_id', how='left').dropna(subset=['lat','lon'])\n",
    "    score_map['class'] = pd.Categorical(score_map['class'], categories=cls_order, ordered=True)\n",
    "\n",
    "    # -------- 4. Merge scores into long paired --------\n",
    "    # Added 'spearman_r' to the list of columns to merge\n",
    "    merge_cols = ['station_id','confidence_score','class','spearman_r','rmse','completeness','outlier_rate','bias_signif']\n",
    "    existing = [c for c in merge_cols if c in sc.columns]\n",
    "    m_long = m_long.merge(sc[existing], on='station_id', how='left')\n",
    "\n",
    "    # -------- 5. Scatter (single color) --------\n",
    "    lim_max = float(max(m_long['station_pentad'].max(), m_long['sat_pentad'].max()))\n",
    "    hover_data = {\n",
    "        'station_id': True,\n",
    "        'time': True,\n",
    "        'station_pentad': ':.2f',\n",
    "        'sat_pentad': ':.2f',\n",
    "        'confidence_score': ':.1f',\n",
    "        'class': True,\n",
    "        'spearman_r': ':.3f', # Ensure this is correctly referenced\n",
    "        'rmse': ':.2f',\n",
    "        'completeness': ':.2f',\n",
    "        'outlier_rate': ':.3f',\n",
    "        'bias_signif': True\n",
    "    }\n",
    "    scatter_fig = px.scatter(\n",
    "        m_long,\n",
    "        x='station_pentad',\n",
    "        y='sat_pentad',\n",
    "        title=f\"{country_label}: Station vs CHIRPS (Pentad)\",\n",
    "        hover_data=hover_data,\n",
    "        labels={'station_pentad':'Station (mm)','sat_pentad':'CHIRPS (mm)'},\n",
    "        width=width, # Pass width to scatter plot\n",
    "        height=height # Pass height to scatter plot\n",
    "    )\n",
    "    scatter_fig.update_traces(marker=dict(color=scatter_color))\n",
    "    scatter_fig.update_layout(showlegend=False)\n",
    "    scatter_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, lim_max],\n",
    "            y=[0, lim_max],\n",
    "            mode='lines',\n",
    "            line=dict(dash='dash', color='black'),\n",
    "            name='1:1'\n",
    "        )\n",
    "    )\n",
    "    scatter_fig.update_xaxes(constrain='domain')\n",
    "    scatter_fig.update_yaxes(scaleanchor=None)\n",
    "\n",
    "        # -------- 6. Geo Map (with categories, OSM basemap) --------\n",
    "    palette = {'High':'green','Medium':'orange','Low':'red'}\n",
    "    map_fig = px.scatter_mapbox(\n",
    "        score_map,\n",
    "        lat='lat',\n",
    "        lon='lon',\n",
    "        color='class',\n",
    "        color_discrete_map=palette,\n",
    "        hover_name='station_id',\n",
    "        hover_data={\n",
    "            'confidence_score': ':.1f',\n",
    "            'spearman_r': ':.3f', # Ensure this is correctly referenced\n",
    "            'rmse': ':.2f',\n",
    "            'completeness': ':.2f',\n",
    "            'outlier_rate': ':.3f',\n",
    "            'bias_signif': True,\n",
    "            'lat': ':.3f',\n",
    "            'lon': ':.3f'\n",
    "        },\n",
    "        title=f\"{country_label} Station Confidence\",\n",
    "        zoom=5,\n",
    "        height=height,\n",
    "        width=width\n",
    "    )\n",
    "    # map_fig.update_layout(\n",
    "    #     legend_title_text='Confidence',\n",
    "    #     mapbox_style=\"open-street-map\"\n",
    "    # )\n",
    "    map_fig.update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        legend_title_text=f\"Confidence<br>(High ≥ {high_thr}, Low < {low_thr})\"\n",
    "    )\n",
    "\n",
    "    if {'lat','lon'}.issubset(score_map.columns) and len(score_map):\n",
    "        lat_min, lat_max = score_map['lat'].min(), score_map['lat'].max()\n",
    "        lon_min, lon_max = score_map['lon'].min(), score_map['lon'].max()\n",
    "        lat_center = (lat_min + lat_max) / 2\n",
    "        lon_center = (lon_min + lon_max) / 2\n",
    "        map_fig.update_layout(\n",
    "            mapbox_center={\"lat\": lat_center, \"lon\": lon_center},\n",
    "            mapbox_zoom=5\n",
    "        )\n",
    "\n",
    "\n",
    "    return m_long, score_map, scatter_fig, map_fig\n",
    "\n",
    "# Get the user to input the high and low threshold\n",
    "time.sleep(1)\n",
    "high_thr = int(High_confidence_threshold)\n",
    "low_thr = int(Low_confidence_threshold)\n",
    "\n",
    "print(f\"High threshold: {high_thr}\")\n",
    "print(f\"Low threshold: {low_thr}\")\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "m_pairs, score_with_meta, scatter_fig, map_fig = plot_station_vs_chirps_and_confidence_interactive(\n",
    "    region_precip_pentad,\n",
    "    chirps_stations,\n",
    "    station_scores,\n",
    "    region_metadata,\n",
    "    country_label=f\"{region_query}\",\n",
    "    high_thr=high_thr,\n",
    "    low_thr=low_thr,\n",
    "    scatter_color=\"#3366cc\",\n",
    "    width=900, # Pass width to the function call\n",
    "    height=600 # Pass height to the function call\n",
    ")\n",
    "\n",
    "if scatter_fig is not None:\n",
    "    scatter_fig.show()\n",
    "if map_fig is not None: # Also check if map_fig is not None before showing\n",
    "    map_fig.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "tn_improved": {
   "export_time": "2025-10-09T21:39:53.622908Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
