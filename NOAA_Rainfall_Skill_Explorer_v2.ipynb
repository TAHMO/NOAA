{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NOAA Workshop - Rainfall Skill Explorer**\n",
        "**Training Guide for KMD, UNMA, and Meteo Rwanda**\n",
        "\n",
        "**Workshop Title:** Rainfall Product Skill Assessment & Flood Case Studies\n",
        "**Duration:** ~2 hours (hands-on session)\n",
        "**Notebook:** `NOAA_Rainfall_Skill_Explorer_v2.ipynb`\n",
        "\n",
        "#### **1. Objectives**\n",
        "\n",
        "By completing this exercise, you will:\n",
        "* Learn how to use the Rainfall Skill Explorer Jupyter Notebook to evaluate multiple rainfall products.\n",
        "* Assess the skill, bias, and reliability of CHIRPS, TAMSAT, IMERG, and ERA5 against validated ground stations.\n",
        "* Explore long-term rainfall trends (2014-2024) in your country's capital.\n",
        "* Analyze how each dataset captured severe flood events in Kenya, Uganda, and Rwanda.\n",
        "* Discuss operational implications for early warning, forecasting, and product selection.\n",
        "\n",
        "#### **2. Dataset Overview**\n",
        "\n",
        "| Category | Dataset | Source | Spatial Resolution | Temporal Coverage | Notes |\n",
        "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
        "| **Ground Observations** | TAHMO / National Met Stations | Met agencies | Point | Daily | High-confidence QC from Notebook 1 |\n",
        "| **Satellite Products** | CHIRPS | UCSB/ FEWS NET | $0.05^{\\circ}$ (~5 km) | 1981-present | May underestimate convective extremes |\n",
        "| | TAMSAT | University of Reading | 0.0375Â° (~4 km) | 1983-present | Strong climatology, may miss isolated storms |\n",
        "| | IMERG (GPM) | NASA | 0.1Â° (~10 km) | 2000-present | Good detection of extremes, higher variance |\n",
        "| **Reanalysis** | ERA5 | ECMWF | 0.25Â° (~25 km) | 1979-present | Smooth but underestimates short bursts |\n",
        "\n",
        "#### **3. Notebook Key steps**\n",
        "- Step 1: Load datasets (Ground and Satellite)\n",
        "- Step 2: Load helper functions\n",
        "- Step 3: Prepare ground data\n",
        "- Step 4: Long term precipitation trends and climatology\n",
        "- Step 5: Aggregation of Station data to match CHIRPS 5-day (pentads) data\n",
        "- Step 6: Build Nearest data for the high confidence ground stations\n",
        "- Step 7: Compute station scores\n",
        "- Step 8: Visualize Station Scores Across Datasets\n",
        "\n",
        "\n",
        "#### **4. Case Study Contexts**\n",
        "\n",
        "##### 4.1 Kenya â€“ Mai Mahiu Flash Flood (Nakuru County)\n",
        "* **Date:** 29 April 2024 * **Window:** 20 April - 5 May 2024\n",
        "\n",
        "##### 4.2 Uganda - Mbale-Kapchorwa Floods (Eastern Region)\n",
        "* **Date:** 30 July 2022 * **Window:** 28 July - 2 August 2022\n",
        "\n",
        "##### 4.3 Rwanda â€“ Western & Northern Provinces Floods\n",
        "* **Date:** 2 May 2023 * **Window:** 1 May - 8 May 2023\n",
        "\n",
        "\n",
        "#### **5. Outputs**\n",
        "Each team should produce PNG figures and CSV outputs, which will be saved in the `Results_SkillExplorer_<country>_<date>` folder.\n",
        "\n",
        "#### **6. Interpretation Template (for group reporting)**\n",
        "\n",
        "| Country | Station / Event | Best Performing Product | Main Finding | Operational Implication |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Kenya** | KE-012 Mai Mahiu (2024) | IMERG | Best extreme detection; CHIRPS underestimates | Combine IMERG for flood alerting + ERA5 for background context |\n",
        "| **Uganda** | UG-00456 Mbale-Kapchorwa (2022) | ERA5 | Consistent timing; TAMSAT misses short bursts | ERA5 captures system-scale but misses extremes |\n",
        "| **Rwanda** | RW-00789 Western Floods (2023) | TAMSAT | Good match to ground totals; ERA5 underestimates | Regional bias correction could improve early warnings |\n",
        "\n",
        "\n",
        "**Audience:** KMD, UNMA, Meteo Rwanda  "
      ],
      "metadata": {
        "id": "ogUad25w_-6U"
      },
      "id": "ogUad25w_-6U"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "SpdsGVZP_j67"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Step 1: Load datasets (Ground and Satellite) {\"display-mode\":\"both\"}\n",
        "\n",
        "# ðŸ’¡ This step loads previously downloaded datasets: TAHMO(Ground), CHIRPS, IMERG, TAMSAT, ERA5\n",
        "\n",
        "import os, numpy as np, pandas as pd\n",
        "from datetime import date\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Set the environment path to the shared drive folder\n",
        "shared_drive_path = '/content/drive/Shared drives/NOAA-workshop2/Datasets' # Path to datafiles\n",
        "\n",
        "\n",
        "# Set capital coordinates\n",
        "CAPITALS={'Kenya':{'name':'Nairobi','lat':-1.2921,'lon':36.8219},\n",
        "          'Uganda':{'name':'Kampala','lat':0.3476,'lon':32.5825},\n",
        "          'Rwanda':{'name':'Kigali','lat':-1.9579,'lon':30.1127}}\n",
        "\n",
        "# Select country\n",
        "COUNTRY = 'Uganda'\n",
        "\n",
        "# Set data to be used\n",
        "Satellite_data_options = ['CHIRPS', 'TAMSAT', 'IMERG', 'ERA5'] # Datasets to be used in the analysis (TAMSAT, IMERG, ERA5, CHIRPS)\n",
        "\n",
        "# Set high confidence stations\n",
        "High_confidence_stations = ['TA00234', 'TA00446', 'TA00208'] # Define top stations here, use 'none' if there is no preference\n",
        "                                                  # For testing purposes: Kenya list: ['TA00247', 'TA00453', 'TA00379']\n",
        "                                                    #                     Uganda list: ['TA00234', 'TA00446', 'TA00208']\n",
        "                                                    #                     Rwanda list: ['TA00451', 'TA00429', 'TA00326']\n",
        "\n",
        "# Set the names of the data sources as saved in the shared drive path\n",
        "GROUND_CSV=f'./tahmo_precip_2014-01-01_2024-12-31_{COUNTRY}.csv';\n",
        "CHIRPS_NC=f'./CHIRPS_2014-01-01_2024-12-31_{COUNTRY}.nc';\n",
        "TAMSAT_NC=f'./TAMSAT_precip_2014-01-01_2024-12-31_{COUNTRY}.nc';\n",
        "IMERG_NC=f'./IMERG_2014-01-01_2024-12-27_{COUNTRY}.nc';\n",
        "ERA5_NC=f'./ERA5_precip_2014-01-01_2024-12-31_{COUNTRY}.nc'\n",
        "\n",
        "# Set start and end date\n",
        "START_DATE='2014-01-01'; END_DATE='2024-12-31'; LTN_BASE=('2014-01-01','2024-12-31')\n",
        "\n",
        "# Set QC metrics\n",
        "NEIGHBORHOOD=3; # Define neighborhood size\n",
        "CORR_METHOD='pearson' # Use pearson or spearman\n",
        "\n",
        "\n",
        "# Scoring weights (must sum to 1.0)\n",
        "w_corr= 0.5   # Pearson r (higher is better)\n",
        "w_bias= 0.3   # absolute bias (lower is better)\n",
        "w_complete= 0.1 # pentad completeness (higher is better)\n",
        "w_outlier= 0.1  # outlier rate (lower is better)\n",
        "\n",
        "# Set events with approximate coordinates\n",
        "EVENTS = {\n",
        "    'Kenya â€“ Mai Mahiu flash flood (2024-04-29)': {\n",
        "        'dates': ('2024-04-20', '2024-05-05'),\n",
        "        'coords': (-1.039, 36.606)   # Mai Mahiu, Nakuru County, Kenya\n",
        "    },\n",
        "    'Uganda â€“ Mbaleâ€“Kapchorwa floods (2022-07-30)': {\n",
        "        'dates': ('2022-07-28', '2022-08-02'),\n",
        "        'coords': (1.082, 34.175)    # Mbale town, Eastern Uganda\n",
        "    },\n",
        "    'Rwanda â€“ Western/Northern floods (2023-05-02)': {\n",
        "        'dates': ('2023-05-01', '2023-05-08'),\n",
        "        'coords': (-1.680, 29.350)   # Between Rubavu & Nyabihu districts, Rwanda\n",
        "    }\n",
        "}\n",
        "Radius_event = 30 # radius in km to consider for visual\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "os.chdir(os.path.join(shared_drive_path, COUNTRY)) # set working directory to country sub folder\n",
        "print(f\"Working directory set to: {os.getcwd()}\")\n",
        "RUN_STAMP=date.today().isoformat(); RESULTS_DIR=f'/content/drive/MyDrive/Results_SkillExplorer_{COUNTRY}_{RUN_STAMP}'; os.makedirs(RESULTS_DIR, exist_ok=True); np.random.seed(42)\n",
        "print('âœ… CONFIG set â†’', RESULTS_DIR)\n",
        "\n",
        "# Check if data files exist\n",
        "data_files = [GROUND_CSV, CHIRPS_NC, TAMSAT_NC, IMERG_NC, ERA5_NC]\n",
        "\n",
        "print(\"Checking for data files:\")\n",
        "for data_file_var in data_files:\n",
        "    if os.path.exists(data_file_var):\n",
        "        print(f\"âœ… {data_file_var} found.\")\n",
        "    else:\n",
        "        print(f\"âŒ {data_file_var} not found.\")"
      ],
      "id": "SpdsGVZP_j67"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "helpers"
        ],
        "id": "lbe2iZdT_j6_"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Step 2: Load helper functions {\"display-mode\":\"form\"}\n",
        "\n",
        "# @markdown This cell defines several helper functions that are used later in the notebook for data processing and analysis.\n",
        "\n",
        "# @markdown These include code extract timeseries, calculate sums etc.\n",
        "\n",
        "# 2) HELPERS\n",
        "import numpy as np, pandas as pd, os\n",
        "def load_ground(p):\n",
        "    if not os.path.exists(p): print('âš ï¸ Ground CSV not found:', p); return None\n",
        "    df=pd.read_csv(p,parse_dates=['date']); req={'station_id','date','precip','lat','lon'}; miss=req-set(df.columns)\n",
        "    if miss: print('âš ï¸ Missing cols:',miss); return None\n",
        "    if 'high_confidence' in df.columns: df=df[df['high_confidence']==True].copy()\n",
        "    return df\n",
        "def open_nc(p):\n",
        "    try:\n",
        "        import xarray as xr\n",
        "        return xr.open_dataset(p) if os.path.exists(p) else None\n",
        "    except Exception as e: print('â„¹ï¸ xarray error:', e); return None\n",
        "def _near(arr,val):\n",
        "    arr=np.asarray(arr); return int(np.argmin(np.abs(arr-val)))\n",
        "def extract_timeseries(ds,lat,lon,k=1):\n",
        "    if ds is None: return None\n",
        "    import pandas as pd\n",
        "    var=list(ds.data_vars)[0] if ds.data_vars else None\n",
        "    if var is None: return None\n",
        "\n",
        "    # Check for 'lat'/'lon' or 'y'/'x' dimensions\n",
        "    if 'lat' in ds.coords and 'lon' in ds.coords:\n",
        "        lats = ds['lat'].values\n",
        "        lons = ds['lon'].values\n",
        "        lat_dim, lon_dim = 'lat', 'lon'\n",
        "    elif 'y' in ds.coords and 'x' in ds.coords:\n",
        "        lats = ds['y'].values\n",
        "        lons = ds['x'].values\n",
        "        lat_dim, lon_dim = 'y', 'x'\n",
        "    else:\n",
        "        print(f\"âš ï¸ Could not find 'lat'/'lon' or 'y'/'x' dimensions in dataset.\")\n",
        "        return None\n",
        "\n",
        "    da=ds[var]\n",
        "    lon_w=lon if not(lons.min()>=0 and lons.max()>180 and lon<0) else lon%360.0\n",
        "    i=_near(lats,lat); j=_near(lons,lon_w)\n",
        "\n",
        "    if k==1:\n",
        "        sub=da.isel({lat_dim:i, lon_dim:j})\n",
        "    else:\n",
        "        sub=da.isel({lat_dim:slice(max(i-k//2,0),i+k//2+1), lon_dim:slice(max(j-k//2,0),j+k//2+1)}).mean(dim=[lat_dim,lon_dim],skipna=True)\n",
        "\n",
        "    try: ts=sub.to_series()\n",
        "    except Exception: ts=pd.Series(sub.values,index=pd.to_datetime(ds['time'].values))\n",
        "    if ts.dropna().quantile(0.99)<1.0: ts=ts*1000.0\n",
        "    return ts\n",
        "def monthly_sum(ts):\n",
        "    return None if ts is None or ts.empty else ts.resample('MS').sum(min_count=1)\n",
        "def mann_kendall(ms):\n",
        "    s=ms.dropna().values; n=len(s)\n",
        "    if n<8: return {'slope':np.nan,'p':np.nan,'direction':'insufficient'}\n",
        "    S=0\n",
        "    for i in range(n-1): S+=np.sum(np.sign(s[i+1:]-s[i]))\n",
        "    varS=(n*(n-1)*(2*n+5))/18.0\n",
        "    z=(S-1)/np.sqrt(varS) if S>0 else ((S+1)/np.sqrt(varS) if S<0 else 0.0)\n",
        "    from math import erf, sqrt; Phi=lambda z:0.5*(1.0+erf(z/np.sqrt(2.0))); p=2*(1-Phi(abs(z)))\n",
        "    x=np.arange(n); slopes=[]\n",
        "    for i in range(n-1): dx=(x[i+1:]-x[i]); dy=(s[i+1:]-s[i]); slopes+=list(dy/dx)\n",
        "    slope=float(np.median(slopes)); direction='increasing' if slope>0 and p<0.05 else ('decreasing' if slope<0 and p<0.05 else 'no_significant_trend')\n",
        "    return {'slope':slope,'p':float(p),'direction':direction}\n",
        "def metrics(gt,mod,ths):\n",
        "    df=pd.concat([gt.rename('gt'),mod.rename('mod')],axis=1).dropna()\n",
        "    if df.empty: return None\n",
        "    rmse=float(np.sqrt(((df['mod']-df['gt'])**2).mean())); bias=float((df['mod']-df['gt']).abs().mean())\n",
        "    cp=df.corr(method='pearson').iloc[0,1]; cs=df.corr(method='spearman').iloc[0,1]\n",
        "    hits={}\n",
        "    for th in ths:\n",
        "        gt_hit=df['gt']>=th; mod_hit=df['mod']>=th; denom=int(gt_hit.sum()); hits[f'hitrate_{th}mm']=float((gt_hit&mod_hit).sum()/denom) if denom>0 else np.nan\n",
        "    return {'rmse':rmse,'bias':bias,'corr_pearson':float(cp),'corr_spearman':float(cs),**hits}\n",
        "\n",
        "print(\"âœ… Helper functions loaded successfully\")"
      ],
      "id": "lbe2iZdT_j6_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "data"
        ],
        "id": "kge42cUY_j7D"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Step 3: Prepare ground data {\"display-mode\":\"form\"}\n",
        "\n",
        "# @markdown This step Loads and prepares ground truth data from time series and metadata CSV files.\n",
        "# 3) LOAD\n",
        "import pandas as pd\n",
        "\n",
        "def load_ground_from_files(time_series_path, metadata_path):\n",
        "    \"\"\"\n",
        "    Loads and prepares ground truth data from time series and metadata CSV files.\n",
        "\n",
        "    Args:\n",
        "        time_series_path (str): Path to the time series CSV file.\n",
        "        metadata_path (str): Path to the metadata CSV file.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Processed ground truth data or None if files are not found or data is empty.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(time_series_path):\n",
        "        print('âš ï¸ Ground time series CSV not found:', time_series_path)\n",
        "        return None\n",
        "    if not os.path.exists(metadata_path):\n",
        "        print('âš ï¸ Ground metadata CSV not found:', metadata_path)\n",
        "        return None\n",
        "\n",
        "    df_ts = pd.read_csv(time_series_path, parse_dates=['date'])\n",
        "    df_meta = pd.read_csv(metadata_path)\n",
        "\n",
        "    if df_ts.empty or df_meta.empty:\n",
        "        print('âš ï¸ Ground truth or metadata is empty.')\n",
        "        return None\n",
        "\n",
        "    # Reshape time series data\n",
        "    df_melted = df_ts.melt(id_vars=['date'], var_name='station_id', value_name='precip').dropna(subset=['precip'])\n",
        "\n",
        "    # Merge with metadata\n",
        "    # Assuming the metadata station id column is named 'code' and location columns 'location.latitude', 'location.longitude'\n",
        "    df_merged = pd.merge(df_melted, df_meta[['code', 'location.latitude', 'location.longitude']],\n",
        "                         left_on='station_id', right_on='code', how='left')\n",
        "\n",
        "    # Rename columns to match the expected format\n",
        "    df_merged = df_merged.rename(columns={'location.latitude': 'lat', 'location.longitude': 'lon'})\n",
        "\n",
        "    # Check for required columns after merging\n",
        "    req = {'station_id', 'date', 'precip', 'lat', 'lon'}\n",
        "    miss = req - set(df_merged.columns)\n",
        "    if miss:\n",
        "        print('âš ï¸ Missing required columns after merge:', miss)\n",
        "        return None\n",
        "\n",
        "    # Drop the redundant 'code' column\n",
        "    df_merged = df_merged.drop(columns=['code'])\n",
        "\n",
        "    # Convert START_DATE and END_DATE to timezone-aware datetime objects in UTC\n",
        "    start_date_utc = pd.to_datetime(START_DATE, utc=True)\n",
        "    end_date_utc = pd.to_datetime(END_DATE, utc=True)\n",
        "\n",
        "    # Filter by date range using timezone-aware comparison\n",
        "    df_processed = df_merged[(df_merged['date'] >= start_date_utc) & (df_merged['date'] <= end_date_utc)].copy()\n",
        "\n",
        "    if 'high_confidence' in df_processed.columns:\n",
        "        df_processed = df_processed[df_processed['high_confidence'] == True].copy()\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "# Load ground truth data using the new function\n",
        "metadata_csv = './tahmo_metadata.csv' # Assuming this is the path to the metadata file\n",
        "df_gt = load_ground_from_files(GROUND_CSV, metadata_csv)\n",
        "\n",
        "\n",
        "ds_chirps=open_nc(CHIRPS_NC); ds_tamsat=open_nc(TAMSAT_NC); ds_imerg=open_nc(IMERG_NC); ds_era5=open_nc(ERA5_NC)\n",
        "\n",
        "if df_gt is not None:\n",
        "    print('âœ… Ground rows:',len(df_gt))\n",
        "else:\n",
        "    print('âŒ Ground data could not be loaded.')"
      ],
      "id": "kge42cUY_j7D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "ltn",
          "trends"
        ],
        "id": "wMZQ23XV_j7G"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Step 4: Long term precipitation trends and climatology {\"display-mode\":\"form\"}\n",
        "\n",
        "# @markdown This section focuses on analyzing long-term monthly precipitation trends and climatology for the capital city of the country\n",
        "# 5) LTN & TRENDS\n",
        "import matplotlib.pyplot as plt, pandas as pd, os\n",
        "cap=CAPITALS[COUNTRY]\n",
        "series={}\n",
        "# Iterate through the datasets specified in Satellite_data_options\n",
        "for name in Satellite_data_options:\n",
        "    ds = None\n",
        "    if name == 'CHIRPS' and ds_chirps is not None:\n",
        "        ds = ds_chirps\n",
        "    elif name == 'TAMSAT' and ds_tamsat is not None:\n",
        "        ds = ds_tamsat\n",
        "    elif name == 'IMERG' and ds_imerg is not None:\n",
        "        ds = ds_imerg\n",
        "    elif name == 'ERA5' and ds_era5 is not None:\n",
        "        ds = ds_era5\n",
        "\n",
        "    if ds is not None:\n",
        "        ts = extract_timeseries(ds, cap['lat'], cap['lon'], k=NEIGHBORHOOD)\n",
        "        if ts is None:\n",
        "            continue\n",
        "        ts=ts[(ts.index>=pd.to_datetime(LTN_BASE[0]))&(ts.index<=pd.to_datetime(LTN_BASE[1]))]\n",
        "        ms=monthly_sum(ts);\n",
        "        if ms is not None: series[name]=ms\n",
        "\n",
        "if series:\n",
        "    print('ðŸ’¡ Click the monthly totals map to zoom in')\n",
        "    fig=plt.figure(figsize=(30,4)) # Modified figsize here\n",
        "    markers = ['o', 's', '^', 'x'] # Define a list of markers\n",
        "    for i, (n,ms) in enumerate(series.items()): # Use enumerate to get index for markers\n",
        "        plt.plot(ms.index,ms.values,label=n, marker=markers[i % len(markers)]) # Added marker here\n",
        "    plt.ylim(0, 500) # Set the maximum value for the y-axis to 500\n",
        "    plt.title(f\"{COUNTRY} â€“ {cap['name']} monthly totals\"); plt.ylabel('mm'); plt.xlabel('Month'); plt.legend(loc='upper left'); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR,f\"ltn_monthly_{COUNTRY}.png\")); plt.show()\n",
        "    clims={n:ms.groupby(ms.index.month).mean() for n,ms in series.items()}\n",
        "    fig=plt.figure(figsize=(12,4))\n",
        "    for i, (n,cl) in enumerate(clims.items()): # Use enumerate again for the climatology plot\n",
        "        plt.plot(range(1,13),cl.values,marker=markers[i % len(markers)],label=n) # Add marker for each data point\n",
        "    plt.title(f\"{COUNTRY} monthly climatology (2014â€“2024)\"); plt.xlabel('Month'); plt.ylabel('mm'); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR,f\"ltn_climatology_{COUNTRY}.png\")); plt.show()\n",
        "    rows=[{'dataset':n,**mann_kendall(ms)} for n,ms in series.items()]; df_trends=pd.DataFrame(rows); display(df_trends)\n",
        "    df_trends.to_csv(os.path.join(RESULTS_DIR,f\"ltn_trends_{COUNTRY}.csv\"),index=False)\n",
        "    for _,r in df_trends.iterrows(): print(f\"- {r['dataset']}: slope={r['slope']:.2f} mm/mon, p={r['p']:.3f}, trend={r['direction']}\")\n",
        "    print('âœ… LTN exports saved')\n",
        "else:\n",
        "    print('â„¹ï¸ No LTN series available (check NetCDFs).')"
      ],
      "id": "wMZQ23XV_j7G"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5: Aggregation of Station data to match CHIRPS 5-day (pentads) data {\"display-mode\":\"form\"}\n",
        "# @markdown This section aggregates the ground truth data to pentads for all available stations and saves the result to a CSV file and a DataFrame variable.\n",
        "\n",
        "import matplotlib.pyplot as plt, os, pandas as pd, numpy as np\n",
        "\n",
        "if df_gt is None or df_gt.empty:\n",
        "    print('âš ï¸ Ground data not loaded or is empty. Cannot aggregate to pentads.')\n",
        "else:\n",
        "    # Filter df_gt to include only high confidence stations if specified\n",
        "    if High_confidence_stations and High_confidence_stations != ['none']:\n",
        "        available_high_confidence_stations = [station for station in High_confidence_stations if station in df_gt['station_id'].unique()]\n",
        "        if available_high_confidence_stations:\n",
        "            df_gt_filtered = df_gt[df_gt['station_id'].isin(available_high_confidence_stations)].copy()\n",
        "            print(f\"Aggregating data for high confidence stations: {available_high_confidence_stations}\")\n",
        "        else:\n",
        "            print(\"âš ï¸ High confidence stations specified but none found in ground data. Aggregating all stations.\")\n",
        "            df_gt_filtered = df_gt.copy() # Use all data if specified stations are not found\n",
        "    else:\n",
        "        print(\"Aggregating data for all stations.\")\n",
        "        df_gt_filtered = df_gt.copy() # Use all data if no high confidence stations are specified\n",
        "\n",
        "    # Filter by start and end date\n",
        "    df_gt_filtered = df_gt_filtered[(df_gt_filtered['date'] >= pd.to_datetime(START_DATE, utc=True)) & (df_gt_filtered['date'] <= pd.to_datetime(END_DATE, utc=True))].copy()\n",
        "\n",
        "\n",
        "    # Pivot the filtered ground truth data to have stations as columns and date as index\n",
        "    ground_truth_pentad = df_gt_filtered.pivot_table(index='date', columns='station_id', values='precip').resample('5D').sum()\n",
        "\n",
        "    if not ground_truth_pentad.empty:\n",
        "        # Save the pentad data to a CSV file\n",
        "        pentad_output_path = os.path.join(RESULTS_DIR, f'ground_truth_pentads_{COUNTRY}.csv')\n",
        "        ground_truth_pentad.to_csv(pentad_output_path)\n",
        "        print(f\"âœ… Ground truth pentad data saved to {pentad_output_path}\")\n",
        "\n",
        "        # Store the pentad data in a new variable for later use\n",
        "        ground_truth_pentads_df = ground_truth_pentad\n",
        "        print(\"âœ… Ground truth pentad data stored in 'ground_truth_pentads_df' variable.\")\n",
        "        display(ground_truth_pentads_df.head()) # Display the head of the new DataFrame\n",
        "    else:\n",
        "        print('âš ï¸ Ground truth pentad DataFrame is empty after aggregation.')"
      ],
      "metadata": {
        "id": "tYfSnefo6yyU"
      },
      "id": "tYfSnefo6yyU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 6: Build Nearest data for the high confidence ground stations\n",
        "# @markdown At this step we are building the station dataset by using the station metadata (latitude and longitude) to get the nearest value for each selected satellite product\n",
        "\n",
        "\n",
        "# Make station_datasets a global variable\n",
        "global station_datasets\n",
        "\n",
        "# filter to the required stations (high confidence stations)\n",
        "# req_stations = region_precip_pentad.columns.tolist() # This line seems unnecessary here as we are using High_confidence_stations\n",
        "\n",
        "# region_metadata = region_metadata[region_metadata['code'].isin(req_stations)] # This line seems unnecessary here\n",
        "\n",
        "def build_chirps_from_stations(chirps_ds, stations_metadata, k=NEIGHBORHOOD):\n",
        "    \"\"\"\n",
        "    Build CHIRPS station dataset by sampling CHIRPS values at station locations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    chirps_ds : xarray.Dataset\n",
        "        CHIRPS dataset with coordinates ('x', 'y', 'time')\n",
        "    stations_metadata : pd.DataFrame\n",
        "        Must contain columns ['code', 'lat', 'lon']\n",
        "    k : int, optional\n",
        "        Number of nearest CHIRPS grid points to average. Default is 1 (just nearest pixel).\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    all_stations_data = {}\n",
        "\n",
        "    # Extract coordinate arrays for CHIRPS grid\n",
        "    chirps_lons = chirps_ds['x'].values\n",
        "    chirps_lats = chirps_ds['y'].values\n",
        "\n",
        "    for _, row in stations_metadata.iterrows():\n",
        "        station_code = row['code']\n",
        "        lat = row['lat']\n",
        "        lon = row['lon']\n",
        "\n",
        "        # Compute distances (simple Euclidean approximation â€” fine for small areas)\n",
        "        dist = np.sqrt((chirps_lats[:, None] - lat)**2 + (chirps_lons[None, :] - lon)**2)\n",
        "        dist_flat = dist.ravel()\n",
        "\n",
        "        # Get the k nearest pixel indices\n",
        "        nearest_indices = np.argsort(dist_flat)[:k]\n",
        "\n",
        "        # Convert flat indices back to 2D (lat, lon)\n",
        "        lat_idx, lon_idx = np.unravel_index(nearest_indices, dist.shape)\n",
        "\n",
        "        # Extract data for those k pixels\n",
        "        da_values = [chirps_ds.precipitation.isel(y=lat_i, x=lon_i) for lat_i, lon_i in zip(lat_idx, lon_idx)]\n",
        "\n",
        "        # Compute mean across k nearest points\n",
        "        station_da = sum(da_values) / len(da_values)\n",
        "        station_df = station_da.to_dataframe().rename(columns={'precipitation': station_code})\n",
        "\n",
        "        all_stations_data[station_code] = station_df[station_code]\n",
        "\n",
        "    combined_df = pd.DataFrame(all_stations_data)\n",
        "    return combined_df\n",
        "\n",
        "\n",
        "# Define a function to build station datasets from any grid dataset\n",
        "def build_station_dataset_from_grid(grid_ds, stations_metadata, k=NEIGHBORHOOD):\n",
        "    \"\"\"\n",
        "    Build station dataset by sampling grid values at station locations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    grid_ds : xarray.Dataset\n",
        "        Grid dataset with precipitation data. Assumes time, lat, lon dimensions.\n",
        "    stations_metadata : pd.DataFrame\n",
        "        Must contain columns ['code', 'location.latitude', 'location.longitude']\n",
        "    k : int, optional\n",
        "        Number of nearest grid points to average. Default is 1 (just nearest pixel).\n",
        "    \"\"\"\n",
        "    if grid_ds is None:\n",
        "        return None\n",
        "\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    all_stations_data = {}\n",
        "\n",
        "    # Check for 'lat'/'lon' or 'y'/'x' dimensions\n",
        "    if 'lat' in grid_ds.coords and 'lon' in grid_ds.coords:\n",
        "        grid_lats = grid_ds['lat'].values\n",
        "        grid_lons = grid_ds['lon'].values\n",
        "        lat_dim, lon_dim = 'lat', 'lon'\n",
        "    elif 'y' in grid_ds.coords and 'x' in grid_ds.coords:\n",
        "        grid_lats = grid_ds['y'].values\n",
        "        grid_lons = grid_ds['x'].values\n",
        "        lat_dim, lon_dim = 'y', 'x'\n",
        "    else:\n",
        "        print(f\"âš ï¸ Could not find 'lat'/'lon' or 'y'/'x' dimensions in dataset.\")\n",
        "        return None\n",
        "\n",
        "    # Find the precipitation variable name\n",
        "    precip_var = None\n",
        "    for var in grid_ds.data_vars:\n",
        "        # Look for common precipitation variable names and dimensions\n",
        "        if 'precip' in var.lower() or 'rainfall' in var.lower() or 'precipitation' in var.lower() or 'rfe' in var.lower():\n",
        "             if (lat_dim in grid_ds[var].dims and lon_dim in grid_ds[var].dims and 'time' in grid_ds[var].dims):\n",
        "                 precip_var = var\n",
        "                 break\n",
        "\n",
        "    if precip_var is None:\n",
        "        print(f\"âš ï¸ Could not find a precipitation variable in the dataset.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    for _, row in stations_metadata.iterrows():\n",
        "        station_code = row['code']\n",
        "        # Access lat and lon using the correct column names from the metadata\n",
        "        lat = row['location.latitude']\n",
        "        lon = row['location.longitude']\n",
        "\n",
        "        # Find the nearest grid point(s)\n",
        "        # This is a simplified approach, more accurate methods might be needed for complex grids\n",
        "        lat_idx = np.abs(grid_lats - lat).argmin()\n",
        "        lon_idx = np.abs(grid_lons - lon).argmin()\n",
        "\n",
        "        if k == 1:\n",
        "            # Extract data for the single nearest pixel\n",
        "            station_da = grid_ds[precip_var].isel({lat_dim: lat_idx, lon_dim: lon_idx})\n",
        "        else:\n",
        "            # Extract data for k nearest points and compute mean\n",
        "            # This is a basic implementation, a more sophisticated approach might involve distance weighting\n",
        "            lat_slice = slice(max(lat_idx - k // 2, 0), min(lat_idx + k // 2 + 1, len(grid_lats)))\n",
        "            lon_slice = slice(max(lon_idx - k // 2, 0), min(lon_idx + k // 2 + 1, len(grid_lons)))\n",
        "\n",
        "            sub = grid_ds[precip_var].isel({lat_dim: lat_slice, lon_dim: lon_slice})\n",
        "\n",
        "            # Handle potential empty slices if k is too large near edges\n",
        "            if sub[lat_dim].size == 0 or sub[lon_dim].size == 0:\n",
        "                 print(f\"âš ï¸ Skipping station {station_code}: Could not extract data for {k} nearest points near grid edge.\")\n",
        "                 all_stations_data[station_code] = pd.Series(dtype=float) # Add an empty series for this station\n",
        "                 continue\n",
        "\n",
        "\n",
        "            station_da = sub.mean(dim=[lat_dim, lon_dim], skipna=True)\n",
        "\n",
        "\n",
        "        # Convert xarray DataArray to pandas Series and rename\n",
        "        try:\n",
        "            station_series = station_da.to_series()\n",
        "            station_series.name = station_code\n",
        "            all_stations_data[station_code] = station_series\n",
        "        except Exception as e:\n",
        "             print(f\"Error converting xarray to series for station {station_code}: {e}\")\n",
        "             all_stations_data[station_code] = pd.Series(dtype=float) # Add an empty series in case of error\n",
        "\n",
        "\n",
        "    # Combine all station series into a single DataFrame\n",
        "    # Use concat with join='outer' to handle potential missing dates in individual series\n",
        "    combined_df = pd.concat(all_stations_data.values(), axis=1, keys=all_stations_data.keys())\n",
        "    return combined_df\n",
        "\n",
        "\n",
        "# This function is now redundant as build_station_dataset_from_grid handles all datasets\n",
        "# chirps_stations = build_chirps_from_stations(chirps_ds, region_metadata)\n",
        "# plot_stations_data_randomly(chirps_stations)\n",
        "# chirps_stations\n",
        "\n",
        "# --- Extract Daily Station Data for all Satellite Datasets ---\n",
        "station_datasets = {} # Initialize the dictionary\n",
        "\n",
        "# Assuming df_meta is available from Step 3 or similar metadata loading\n",
        "# If not, reload it\n",
        "if 'df_meta' not in locals() or df_meta.empty:\n",
        "    metadata_csv = './tahmo_metadata.csv' # Assuming this is the path to the metadata file\n",
        "    if os.path.exists(metadata_csv):\n",
        "        df_meta = pd.read_csv(metadata_csv)\n",
        "        print(\"Reloaded ground truth metadata.\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Ground metadata CSV not found at {metadata_csv}. Cannot extract daily station data from satellite products.\")\n",
        "        df_meta = pd.DataFrame() # Set to empty DataFrame to prevent errors\n",
        "\n",
        "if High_confidence_stations and High_confidence_stations != ['none'] and not df_meta.empty:\n",
        "     available_high_confidence_stations_meta = df_meta[df_meta['code'].isin(High_confidence_stations)].copy()\n",
        "     # Rename latitude and longitude columns to 'lat' and 'lon'\n",
        "     # This renaming is not needed inside the function anymore but keeping it here doesn't hurt\n",
        "     # available_high_confidence_stations_meta = available_high_confidence_stations_meta.rename(columns={'location.latitude': 'lat', 'location.longitude': 'lon'})\n",
        "\n",
        "     if not available_high_confidence_stations_meta.empty:\n",
        "         print(\"Extracting daily data for high confidence stations from satellite products...\")\n",
        "         # Iterate through the satellite datasets and extract daily data for each station\n",
        "         for dataset_name in Satellite_data_options:\n",
        "             print(f\"Processing {dataset_name}...\")\n",
        "             if dataset_name == 'CHIRPS' and ds_chirps is not None:\n",
        "                 # Use the specific CHIRPS function if needed, or adapt the generic one\n",
        "                 # For now, let's use the generic one for consistency\n",
        "                 station_datasets[dataset_name] = build_station_dataset_from_grid(ds_chirps, available_high_confidence_stations_meta, k=NEIGHBORHOOD)\n",
        "             elif dataset_name == 'TAMSAT' and ds_tamsat is not None:\n",
        "                 station_datasets[dataset_name] = build_station_dataset_from_grid(ds_tamsat, available_high_confidence_stations_meta, k=NEIGHBORHOOD)\n",
        "             elif dataset_name == 'IMERG' and ds_imerg is not None:\n",
        "                  station_datasets[dataset_name] = build_station_dataset_from_grid(ds_imerg, available_high_confidence_stations_meta, k=NEIGHBORHOOD)\n",
        "             elif dataset_name == 'ERA5' and ds_era5 is not None:\n",
        "                 station_datasets[dataset_name] = build_station_dataset_from_grid(ds_era5, available_high_confidence_stations_meta, k=NEIGHBORHOOD)\n",
        "             else:\n",
        "                 print(f\"â„¹ï¸ Skipping {dataset_name}: Dataset not loaded or available.\")\n",
        "\n",
        "             if dataset_name in station_datasets and station_datasets[dataset_name] is not None and not station_datasets[dataset_name].empty:\n",
        "                 print(f\"âœ… Daily station data extracted for {dataset_name}.\")\n",
        "                 # display(station_datasets[dataset_name].head()) # Optional: display head of extracted data\n",
        "             else:\n",
        "                 print(f\"âš ï¸ No daily station data extracted for {dataset_name}.\")\n",
        "\n",
        "     else:\n",
        "         print(\"âš ï¸ High confidence stations metadata not found. Cannot extract daily station data from satellite products.\")\n",
        "else:\n",
        "     print(\"âš ï¸ Ground data or metadata not loaded, is empty, or no high confidence stations specified. Cannot extract daily station data from satellite products.\")\n",
        "\n",
        "\n",
        "# --- Aggregate to Pentads ---\n",
        "# Assuming station_datasets is available from Step 9\n",
        "if 'station_datasets' in globals() and station_datasets:\n",
        "    pentad_station_datasets = {}\n",
        "    # Set the origin for resampling to the START_DATE\n",
        "    origin_date = pd.to_datetime(START_DATE, utc=True)\n",
        "    for dataset_name, daily_df in station_datasets.items():\n",
        "        if daily_df is not None and not daily_df.empty:\n",
        "            print(f\"Aggregating {dataset_name} to pentads...\")\n",
        "            # Ensure the index of daily_df is timezone-aware (UTC) before resampling\n",
        "            if daily_df.index.tzinfo is None:\n",
        "                daily_df = daily_df.tz_localize('UTC')\n",
        "            elif daily_df.index.tzinfo != origin_date.tzinfo:\n",
        "                 daily_df = daily_df.tz_convert(origin_date.tzinfo)\n",
        "\n",
        "            # Resample to 5-day periods and sum, setting the origin to START_DATE\n",
        "            pentad_df = daily_df.resample('5D', origin=origin_date).sum()\n",
        "            pentad_station_datasets[dataset_name] = pentad_df\n",
        "            print(f\"âœ… {dataset_name} aggregated to pentads.\")\n",
        "            # print(f\"\\nHead of {dataset_name} pentad data:\")\n",
        "            # display(pentad_df.head())\n",
        "        else:\n",
        "            print(f\"â„¹ï¸ Skipping {dataset_name}: Daily station data not available or is empty.\")\n",
        "else:\n",
        "    print(\"âš ï¸ Station datasets not available. Run Step 9 first.\")\n",
        "\n",
        "# --- Add toggle and plotting functionality ---\n",
        "import ipywidgets as W\n",
        "from IPython.display import display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Determine which stations to use for the dropdown (only high confidence)\n",
        "if High_confidence_stations and High_confidence_stations != ['none'] and df_gt is not None and not df_gt.empty:\n",
        "    # Filter df_gt to include only high confidence stations that are actually in the data\n",
        "    available_high_confidence_stations = [station for station in High_confidence_stations if station in df_gt['station_id'].unique()]\n",
        "    if available_high_confidence_stations:\n",
        "        stations_to_use_plot = available_high_confidence_stations\n",
        "        print(\"Using high confidence stations for plotting.\")\n",
        "    else:\n",
        "        stations_to_use_plot = sorted(df_gt['station_id'].unique().tolist())\n",
        "        print(\"Invalid High confidence stations provided or none available in data for plotting. Reverting to use all stations.\")\n",
        "else:\n",
        "    stations_to_use_plot = sorted(df_gt['station_id'].unique().tolist())\n",
        "    print(\"No high confidence stations specified or ground data is empty for plotting. Using all stations.\")\n",
        "\n",
        "if not stations_to_use_plot:\n",
        "    print('âš ï¸ No stations available in ground data for plotting.')\n",
        "else:\n",
        "    # Create a dropdown widget for station selection for plotting\n",
        "    station_plot_w = W.Dropdown(\n",
        "        options=stations_to_use_plot,\n",
        "        description='Select Station (Plot):',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create a toggle button for plotting\n",
        "    plot_toggle = W.ToggleButtons(\n",
        "        options=['Show Plot', 'Hide Plot'],\n",
        "        description='Plot Time Series:',\n",
        "        disabled=False,\n",
        "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "        tooltips=['Click to show the time series plot', 'Click to hide the time series plot'],\n",
        "    )\n",
        "\n",
        "    display(W.VBox([station_plot_w, plot_toggle]))\n",
        "    def update_time_series_plot(change):\n",
        "        clear_output(wait=True)\n",
        "        print('ðŸ’¡ Click the plot to zoom in')\n",
        "        display(W.VBox([station_plot_w, plot_toggle]))\n",
        "\n",
        "        if plot_toggle.value == 'Show Plot':\n",
        "            if df_gt is None or df_gt.empty:\n",
        "                print('âš ï¸ Ground data not loaded or is empty.')\n",
        "                return\n",
        "\n",
        "            sid = station_plot_w.value\n",
        "            st = df_gt[df_gt['station_id'] == sid].sort_values('date')\n",
        "\n",
        "            if st.empty:\n",
        "                print(f\"âš ï¸ No data available for station {sid}.\")\n",
        "                return\n",
        "\n",
        "            # Resample ground data to 5-day periods (pentads)\n",
        "            series_plot = {'GROUND': st.set_index('date')['precip'].resample('5D', origin=pd.to_datetime(START_DATE, utc=True)).sum()}\n",
        "\n",
        "\n",
        "            # Include all available satellite data from pentad_station_datasets\n",
        "            if 'pentad_station_datasets' in globals() and pentad_station_datasets:\n",
        "                 for dataset_name, pentad_df in pentad_station_datasets.items():\n",
        "                     if pentad_df is not None and not pentad_df.empty and sid in pentad_df.columns:\n",
        "                         series_plot[dataset_name] = pentad_df[sid]\n",
        "                     else:\n",
        "                         print(f\"â„¹ï¸ Pentad data for {dataset_name} or station {sid} not available.\")\n",
        "\n",
        "            fig = plt.figure(figsize=(40, 6))\n",
        "            plt.plot(series_plot['GROUND'].index, series_plot['GROUND'].values, label='Ground', marker='o') # Added marker\n",
        "\n",
        "            # Iterate through series_plot to plot all available datasets\n",
        "            markers = ['', '', '', ''] # Define markers for satellite datasets\n",
        "            dataset_names = [d for d in series_plot.keys() if d != 'GROUND']\n",
        "            for i, d in enumerate(dataset_names):\n",
        "                data = series_plot[d]\n",
        "                # Align the satellite data to the ground data index for plotting\n",
        "                aligned = data.reindex(series_plot['GROUND'].index)\n",
        "                plt.plot(aligned.index, aligned.values, label=d, marker=markers[i % len(markers)]) # Added marker\n",
        "\n",
        "            plt.title(f\"Pentad precipitation â€“ Station {sid}\")\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('mm')\n",
        "            plt.legend(loc='upper left') # Moved legend to upper left\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(RESULTS_DIR, f\"station_pentad_ts_{sid}_with_satellite.png\"));\n",
        "            display(fig)\n",
        "            plt.close(fig)\n",
        "        else:\n",
        "            print(\"Plotting is hidden. Toggle to 'Show Plot' to display the time series.\")\n",
        "\n",
        "\n",
        "    # Link the update function to the dropdown's value change and the toggle button's value change\n",
        "    station_plot_w.observe(update_time_series_plot, names='value')\n",
        "    plot_toggle.observe(update_time_series_plot, names='value')\n",
        "\n",
        "    # Trigger the update function with the initial value\n",
        "    update_time_series_plot({'new': station_plot_w.value})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gggXeAdRaFYW"
      },
      "id": "gggXeAdRaFYW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Step 7: Compute station scores\n",
        "# @title Confidence Score Formula\n",
        "\n",
        "\n",
        "# @markdown ### **Confidence Score Formula**\n",
        "# @markdown For each station, the confidence score is computed as:\n",
        "# @markdown\n",
        "# @markdown $$\n",
        "# @markdown \\text{Confidence Score} = 100 \\times \\big(0.1 \\cdot C \\;+\\; 0.5 \\cdot r \\;+\\; 0.1 \\cdot (1 - O) \\;+\\; 0.3 \\cdot(1- b) \\big)\n",
        "# @markdown $$$\n",
        "# @markdown\n",
        "# @markdown Where:\n",
        "# @markdown - $C = 1 -$ (missing fraction of observations), i.e. **completeness**\n",
        "# @markdown - $r =$ Spearman correlation coefficient between observed and CHIRPS pentads\n",
        "# @markdown - $O =$ fraction of outliers (observed values < 0)\n",
        "# @markdown - $b =$ Bias Significance - paired $t$-test between observed and simulated values (significant if $p < 0.05$)\n",
        "# @markdown\n",
        "# @markdown ---\n",
        "# @markdown\n",
        "# @markdown **Additional metrics per station:**\n",
        "# @markdown\n",
        "# @markdown - **RMSE**\n",
        "# @markdown $$\n",
        "# @markdown \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum (obs - sim)^2}\n",
        "# @markdown $$\n",
        "# @markdown\n",
        "# @markdown - **Adjusted confidence score**\n",
        "# @markdown $$\n",
        "# @markdown \\text{Adjusted Confidence Score} = \\text{Confidence Score} - \\text{RMSE}\n",
        "# @markdown $$\n",
        "# @markdown\n",
        "\n",
        "from scipy.stats import pearsonr, ttest_rel\n",
        "# spearman correlation\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def compute_station_scores(obs_pentad, sim_pentad, corr_method='pearson'): # returns a dataframe of scores 'station_id','confidence_score','pearson_r','rmse','bias_signif'\n",
        "    \"\"\"\n",
        "    Computes station scores (confidence, RMSE, correlation, bias significance, completeness, outlier rate).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    obs_pentad : pd.DataFrame\n",
        "        Observed pentad data (stations as columns, dates as index).\n",
        "    sim_pentad : pd.DataFrame\n",
        "        Simulated pentad data (stations as columns, dates as index).\n",
        "    corr_method : str, optional\n",
        "        Correlation method ('pearson' or 'spearman'). Default is 'pearson'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame containing scores for each common station, or None if no common stations.\n",
        "    \"\"\"\n",
        "    # get the union stations\n",
        "    stations = obs_pentad.columns.intersection(sim_pentad.columns)\n",
        "    if stations.empty:\n",
        "        print(\"No common stations between observed and simulated pentad data.\")\n",
        "        return None\n",
        "\n",
        "    # Ensure indices are aligned by date and timezone before dropping NaNs\n",
        "    # Reindex sim_pentad to the obs_pentad index to ensure alignment\n",
        "    sim_pentad_aligned = sim_pentad.reindex(obs_pentad.index)\n",
        "\n",
        "\n",
        "    scores = []\n",
        "    for station in stations:\n",
        "        # Use the aligned data for calculations\n",
        "        obs = obs_pentad[station].dropna() # Drop NaNs for metric calculations\n",
        "        sim = sim_pentad_aligned[station].dropna() # Drop NaNs for metric calculations from the aligned data\n",
        "\n",
        "        # Ensure indices are aligned after dropping NaNs\n",
        "        common_index = obs.index.intersection(sim.index)\n",
        "        obs = obs.loc[common_index]\n",
        "        sim = sim.loc[common_index]\n",
        "\n",
        "        # Ensure there's enough data after dropping NaNs and aligning indices\n",
        "        # Changed condition to require at least 1 non-NaN overlapping data point\n",
        "        if len(obs) < 1 or len(sim) < 1:\n",
        "             print(f\"Insufficient overlapping data for station {station}. Skipping.\")\n",
        "             continue\n",
        "\n",
        "\n",
        "        if corr_method.lower() == 'pearson':\n",
        "            # Handle case where correlation cannot be computed (e.g., all values are the same)\n",
        "            try:\n",
        "                # Ensure data are numeric before computing correlation\n",
        "                obs_numeric = pd.to_numeric(obs, errors='coerce').dropna()\n",
        "                sim_numeric = pd.to_numeric(sim, errors='coerce').dropna()\n",
        "                if len(obs_numeric) > 1 and len(sim_numeric) > 1: # Need at least 2 data points for correlation\n",
        "                    corr, _ = pearsonr(obs_numeric, sim_numeric)\n",
        "                else:\n",
        "                    corr = np.nan\n",
        "            except ValueError:\n",
        "                corr = np.nan # Assign NaN if correlation cannot be computed\n",
        "        else:\n",
        "             # Handle case where correlation cannot be computed\n",
        "            try:\n",
        "                # Ensure data are numeric before computing correlation\n",
        "                obs_numeric = pd.to_numeric(obs, errors='coerce').dropna()\n",
        "                sim_numeric = pd.to_numeric(sim, errors='coerce').dropna()\n",
        "                if len(obs_numeric) > 1 and len(sim_numeric) > 1: # Need at least 2 data points for correlation\n",
        "                    corr, _ = spearmanr(obs_numeric, sim_numeric)\n",
        "                else:\n",
        "                    corr = np.nan\n",
        "            except ValueError:\n",
        "                corr = np.nan # Assign NaN if correlation cannot be computed\n",
        "\n",
        "\n",
        "        # Compute metrics\n",
        "        # Ensure data are numeric before computing RMSE\n",
        "        obs_numeric_rmse = pd.to_numeric(obs, errors='coerce').dropna()\n",
        "        sim_numeric_rmse = pd.to_numeric(sim, errors='coerce').dropna()\n",
        "\n",
        "        # Ensure there's enough data for RMSE calculation\n",
        "        if len(obs_numeric_rmse) >= 1 and len(sim_numeric_rmse) >= 1:\n",
        "             error = rmse(obs_numeric_rmse, sim_numeric_rmse)\n",
        "        else:\n",
        "             error = np.nan # Assign NaN if not enough data for RMSE\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Ensure there are enough data points for t-test (at least 2) and data are numeric\n",
        "            obs_numeric_ttest = pd.to_numeric(obs, errors='coerce').dropna()\n",
        "            sim_numeric_ttest = pd.to_numeric(sim, errors='coerce').dropna()\n",
        "\n",
        "            if len(obs_numeric_ttest) >= 2 and len(sim_numeric_ttest) >= 2:\n",
        "                t_stat, p_val = ttest_rel(obs_numeric_ttest, sim_numeric_ttest)\n",
        "                bias_signif = 'Significant' if p_val < 0.05 else 'Not Significant'\n",
        "            else:\n",
        "                p_val = np.nan\n",
        "                bias_signif = 'Insufficient data for t-test'\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing t-test for station {station}: {e}\")\n",
        "            p_val = np.nan\n",
        "            bias_signif = 'Error'\n",
        "\n",
        "\n",
        "        completeness = 1 - obs_pentad[station].isna().mean() # Calculate completeness based on original pentad data\n",
        "        outlier_rate = (obs_pentad[station] < 0).mean() # Calculate outlier rate based on original pentad data\n",
        "\n",
        "        # Handle potential NaNs in p_val and corr for confidence score calculation\n",
        "        p_val_for_score = p_val if not np.isnan(p_val) else 1.0 # Assume no significant bias if p_val is NaN\n",
        "        corr_for_score = corr if not np.isnan(corr) else 0.0 # Assume no correlation if corr is NaN\n",
        "\n",
        "\n",
        "        confidence_score = 100 * (w_complete * completeness + w_corr * np.nan_to_num(corr, nan=0) + w_outlier * (1 - outlier_rate)+ w_bias*(1-p_val))\n",
        "        scores.append({\n",
        "                'station_id': station,\n",
        "                'confidence_score': confidence_score,\n",
        "                f'{corr_method}_r': corr,\n",
        "                'rmse': error,\n",
        "                'bias_signif': bias_signif,\n",
        "                'completeness': completeness,\n",
        "                'outlier_rate': outlier_rate,\n",
        "                'p_value': p_val # Include p-value for clarity\n",
        "            }\n",
        "        )\n",
        "    scores_df = pd.DataFrame(scores)\n",
        "    return scores_df\n",
        "\n",
        "# @title RMSE\n",
        "def rmse(a,b):\n",
        "    a = np.asarray(a); b = np.asarray(b)\n",
        "    # Use np.sqrt and np.nanmean to handle potential NaNs gracefully\n",
        "    return float(np.sqrt(np.nanmean((a-b)**2)))\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main part to compute scores for each dataset\n",
        "\n",
        "# Resample ground truth data to pentads for all high confidence stations\n",
        "if df_gt is not None and not df_gt.empty and High_confidence_stations and High_confidence_stations != ['none']:\n",
        "     available_high_confidence_stations = [station for station in High_confidence_stations if station in df_gt['station_id'].unique()]\n",
        "     if available_high_confidence_stations:\n",
        "        # Filter df_gt to include only high confidence stations\n",
        "        df_gt_filtered = df_gt[df_gt['station_id'].isin(available_high_confidence_stations)].copy()\n",
        "        # Set the origin for resampling to the START_DATE and localize to UTC\n",
        "        origin_date = pd.to_datetime(START_DATE, utc=True)\n",
        "        # Pivot the filtered ground truth data to have stations as columns and date as index\n",
        "        # Resample to 5-day periods, setting the origin to START_DATE and ensuring index is UTC\n",
        "        if df_gt_filtered['date'].dt.tz is None:\n",
        "             df_gt_filtered['date'] = df_gt_filtered['date'].dt.tz_localize('UTC')\n",
        "        elif df_gt_filtered['date'].dt.tz != origin_date.tzinfo:\n",
        "             df_gt_filtered['date'] = df_gt_filtered['date'].dt.tz_convert(origin_date.tzinfo)\n",
        "\n",
        "\n",
        "        ground_truth_pentad = df_gt_filtered.pivot_table(index='date', columns='station_id', values='precip').resample('5D', origin=origin_date).sum()\n",
        "\n",
        "\n",
        "        all_scores_dict = {} # Use a dictionary to store scores per dataset\n",
        "        if 'station_datasets' in globals() and station_datasets:\n",
        "             for dataset_name in Satellite_data_options: # Iterate through Satellite_data_options\n",
        "                 sim_pentad_data = station_datasets.get(dataset_name) # Get the corresponding station dataset\n",
        "                 if sim_pentad_data is not None and not sim_pentad_data.empty: # Check if sim_pentad_data is not None and not empty\n",
        "                     print(f\"\\nComputing scores for {dataset_name}...\")\n",
        "                     # Ensure the index of sim_pentad_data is timezone-aware (UTC)\n",
        "                     if sim_pentad_data.index.tzinfo is None:\n",
        "                          sim_pentad_data = sim_pentad_data.tz_localize('UTC')\n",
        "                     elif sim_pentad_data.index.tzinfo != origin_date.tzinfo:\n",
        "                          sim_pentad_data = sim_pentad_data.tz_convert(origin_date.tzinfo)\n",
        "\n",
        "                     # Resample simulated data to pentads with the same origin as ground truth\n",
        "                     sim_pentad_resampled = sim_pentad_data.resample('5D', origin=origin_date).sum()\n",
        "\n",
        "                     dataset_scores = compute_station_scores(ground_truth_pentad, sim_pentad_resampled, CORR_METHOD)\n",
        "                     if dataset_scores is not None and not dataset_scores.empty: # Check if dataset_scores is not None and not empty\n",
        "                         dataset_scores['dataset'] = dataset_name\n",
        "                         # Calculate adjusted confidence score\n",
        "                         dataset_scores['adjusted_confidence_score'] = dataset_scores['confidence_score'] - dataset_scores['rmse']\n",
        "                         all_scores_dict[dataset_name] = dataset_scores # Store scores in the dictionary\n",
        "                         print(f\"\\nScores for {dataset_name}:\")\n",
        "                         display(dataset_scores.sort_values(by='adjusted_confidence_score', ascending=False))\n",
        "                         # Define the output file path\n",
        "                         scores_output_path = os.path.join(RESULTS_DIR, f'{dataset_name}_station_scores_{COUNTRY}.csv')\n",
        "                         dataset_scores.to_csv(scores_output_path, index=False)\n",
        "                         print(f\"\\nâœ… {dataset_name} station scores saved to {scores_output_path}\")\n",
        "\n",
        "                     else:\n",
        "                         print(f\"â„¹ï¸ No scores computed for {dataset_name}.\")\n",
        "                 else:\n",
        "                     print(f\"â„¹ï¸ Skipping {dataset_name}: Station dataset not available or is empty.\")\n",
        "        else:\n",
        "             print(\"âš ï¸ Station datasets not available. Run Step 9 first.\")\n",
        "\n",
        "\n",
        "        # The combined_scores_df is no longer needed for separate display,\n",
        "        # but you could combine them here if needed for a different purpose.\n",
        "        # For now, we just display and save separately within the loop.\n",
        "\n",
        "     else:\n",
        "        print(\"âš ï¸ High confidence stations specified but none found in ground data. Cannot compute scores.\")\n",
        "\n",
        "else:\n",
        "    print('âš ï¸ Ground data not loaded, is empty, or no high confidence stations specified. Cannot compute scores.')"
      ],
      "metadata": {
        "id": "rDqNxNiPJJ1g",
        "cellView": "form"
      },
      "id": "rDqNxNiPJJ1g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49c18781"
      },
      "source": [
        "# @title Step 8: Visualize Station Scores Across Datasets {\"display-mode\":\"form\"}\n",
        "# @markdown This step visualizes the computed station scores (confidence, RMSE, correlation, etc.) across the different satellite datasets.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Check if all_scores_dict is available and not empty\n",
        "if 'all_scores_dict' in locals() and all_scores_dict:\n",
        "    # Combine all scores into a single DataFrame\n",
        "    all_scores_list = []\n",
        "    for dataset_name, scores_df in all_scores_dict.items():\n",
        "        if scores_df is not None and not scores_df.empty:\n",
        "            all_scores_list.append(scores_df)\n",
        "\n",
        "    if all_scores_list:\n",
        "        combined_scores_df = pd.concat(all_scores_list, ignore_index=True)\n",
        "\n",
        "        print(\"Combined Station Scores across Datasets:\")\n",
        "        display(combined_scores_df)\n",
        "\n",
        "        # Define the output file path for combined scores\n",
        "        combined_scores_output_path = os.path.join(RESULTS_DIR, f'combined_station_scores_{COUNTRY}.csv')\n",
        "        combined_scores_df.to_csv(combined_scores_output_path, index=False)\n",
        "        print(f\"âœ… Combined station scores saved to {combined_scores_output_path}\")\n",
        "\n",
        "\n",
        "        # --- Visualization ---\n",
        "\n",
        "        # Melt the DataFrame for easier plotting of different metrics\n",
        "        # Exclude 'p_value' and 'bias_signif' for melting as they are not numerical metrics for this type of plot\n",
        "        metrics_to_plot = ['confidence_score', 'adjusted_confidence_score', f'{CORR_METHOD}_r', 'rmse', 'completeness', 'outlier_rate']\n",
        "        metrics_to_plot = [metric for metric in metrics_to_plot if metric in combined_scores_df.columns] # Ensure metrics exist\n",
        "\n",
        "        if metrics_to_plot:\n",
        "            melted_scores_df = combined_scores_df.melt(\n",
        "                id_vars=['station_id', 'dataset'],\n",
        "                value_vars=metrics_to_plot,\n",
        "                var_name='metric',\n",
        "                value_name='score_value'\n",
        "            )\n",
        "\n",
        "            print(\"\\nGenerating visualizations...\")\n",
        "\n",
        "            # Plotting each metric for each station across datasets\n",
        "            g = sns.catplot(\n",
        "                data=melted_scores_df,\n",
        "                x='dataset',\n",
        "                y='score_value',\n",
        "                col='metric',\n",
        "                kind='bar',\n",
        "                sharey=False,\n",
        "                col_wrap=3,\n",
        "                height=4,\n",
        "                aspect=0.8\n",
        "            )\n",
        "            g.fig.suptitle(f'Station Scores Across Datasets for {COUNTRY}', y=1.02)\n",
        "            g.set_titles(\"{col_name}\")\n",
        "            g.set_axis_labels(\"Dataset\", \"Score Value\")\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "            plot_output_path = os.path.join(RESULTS_DIR, f'station_scores_comparison_bar_{COUNTRY}.png')\n",
        "            plt.savefig(plot_output_path)\n",
        "            plt.show()\n",
        "            print(f\"âœ… Station scores comparison plot saved to {plot_output_path}\")\n",
        "\n",
        "             # Optional: Plotting adjusted confidence score distribution using box plot\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.boxplot(data=combined_scores_df, x='dataset', y='adjusted_confidence_score')\n",
        "            plt.title(f'Adjusted Confidence Score Distribution Across Datasets for {COUNTRY}')\n",
        "            plt.xlabel('Dataset')\n",
        "            plt.ylabel('Adjusted Confidence Score')\n",
        "            plt.tight_layout()\n",
        "            boxplot_output_path = os.path.join(RESULTS_DIR, f'adjusted_confidence_boxplot_{COUNTRY}.png')\n",
        "            plt.savefig(boxplot_output_path)\n",
        "            plt.show()\n",
        "            print(f\"âœ… Adjusted confidence score boxplot saved to {boxplot_output_path}\")\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"âš ï¸ No suitable numerical metrics found to plot.\")\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ No scores available in all_scores_dict to combine.\")\n",
        "else:\n",
        "    print('âš ï¸ Station scores data (all_scores_dict) not available. Run Step 10a first.')"
      ],
      "id": "49c18781",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 9: Case study**\n",
        "\n",
        "**Kenya Mai Mahiu Flash Flood:**\n",
        "The Mai Mahiu flash flood occurred on April 29, 2024, in Nakuru County, Kenya. It was a devastating debris flow event triggered by heavy rainfall amid the broader 2024 Kenya floods. The incident resulted in significant casualties, injuries, and displacement, primarily affecting villages near Mai Mahiu town. Initially misreported as a dam burst (e.g., the Old Kijabe Dam), it was later confirmed as a blockage in a railway culvert causing a sudden release of water, mud, and debris. This event highlighted vulnerabilities in the Great Rift Valley due to steep terrain, deforestation, and soil erosion. The specified event window (April 20â€“May 5, 2024) captures the rainfall buildup and immediate aftermath.\n",
        "\n",
        "**Rwanda Western & Northern Provinces Floods:**\n",
        "The Western and Northern Provinces floods occurred on May 2-3, 2023, in Rwanda, triggered by heavy rainfall during the March-May wet season. This disaster involved severe flooding, mudslides, and landslides, causing widespread destruction in the Western, Northern, and Southern Provinces. The hardest-hit areas were in the Western Province near Lake Kivu, with significant impacts along rivers like Mukungwa. Initial misreports evolved into confirmed counts of massive loss, highlighting vulnerabilities in hilly, densely populated regions. The event window (May 1â€“8, 2023) encompassed the peak rains (110-130 mm in affected areas) and early response. It was part of broader East African flooding patterns, potentially amplified by climate change.\n",
        "\n",
        "**Uganda Mbale-Kapchorwa Floods:**\n",
        "The Mbale-Kapchorwa floods occurred on July 30, 2022, in the Eastern Region of Uganda, triggered by heavy rainfall amid the region's wet season. This event involved flash floods, river overflows, and landslides, resulting in significant casualties, displacement, and infrastructure damage. The floods were caused by approximately 10 hours of continuous heavy rain on the slopes of Mount Elgon, leading to the overflow of rivers such as Nabuyonga, Namatala, Nashibiso, and Napwoli. The most affected districts included Mbale, Kapchorwa, Sironko, Bulambuli, Bukedea, Butaleja, and Bududa. Initial reports indicated catastrophic damage to homes, schools, roads, and farmlands, exacerbating vulnerabilities in an area prone to such hazards due to steep terrain and deforestation. The event window (July 28â€“August 2, 2022) captured the peak rainfall and immediate impacts, with forecasts predicting continued enhanced rains into August.\n",
        "\n",
        "More information related to the evet can be find in the workshop practical guide\n"
      ],
      "metadata": {
        "id": "PKtmXJRcH_pc"
      },
      "id": "PKtmXJRcH_pc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d0d4853"
      },
      "source": [
        "# @title 9a) Visualize Event Location, Radius, and Stations on a Map {\"display-mode\":\"form\"}\n",
        "# @markdown This step visualizes the selected case study event location, the radius around it from values defined at the start of the notebook, and the ground stations on a map.\n",
        "\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from IPython.display import display, HTML # Import HTML for centering\n",
        "\n",
        "radius = Radius_event*1000\n",
        "# Set map dimensions\n",
        "map_height = 500\n",
        "map_width = 800\n",
        "\n",
        "if COUNTRY=='Kenya':\n",
        "  event_key = 'Kenya â€“ Mai Mahiu flash flood (2024-04-29)'\n",
        "elif COUNTRY=='Uganda':\n",
        "  event_key = 'Uganda â€“ Mbaleâ€“Kapchorwa floods (2022-07-30)'\n",
        "elif COUNTRY=='Rwanda':\n",
        "  event_key = 'Rwanda â€“ Western/Northern floods (2023-05-02)'\n",
        "else:\n",
        "    event_key = None # Handle cases where country is not in the list\n",
        "\n",
        "if event_key and event_key in EVENTS and 'coords' in EVENTS[event_key]:\n",
        "    event_coords = EVENTS[event_key]['coords']\n",
        "elif event_key and event_key not in EVENTS:\n",
        "     print(f\"âš ï¸ Event key '{event_key}' not found in EVENTS dictionary.\")\n",
        "elif event_key and 'coords' not in EVENTS[event_key]:\n",
        "     print(f\"âš ï¸ 'coords' key not found for event '{event_key}' in EVENTS dictionary.\")\n",
        "\n",
        "\n",
        "if event_coords is None:\n",
        "    print(f\"âš ï¸ Could not determine event coordinates for {COUNTRY}.\")\n",
        "elif df_gt is None or df_gt.empty:\n",
        "    print('âš ï¸ Ground data not loaded or is empty. Cannot plot stations.')\n",
        "else:\n",
        "    event_lat, event_lon = event_coords\n",
        "\n",
        "    # Filter stations to include only those with valid lat/lon and for the selected country\n",
        "    # Assuming df_gt already contains data for the selected country based on the notebook setup\n",
        "    stations_to_plot = df_gt.dropna(subset=['lat', 'lon']).drop_duplicates(subset=['station_id'])\n",
        "\n",
        "    if stations_to_plot.empty:\n",
        "        print(f\"âš ï¸ No valid station locations found for {COUNTRY}. Cannot create map with stations.\")\n",
        "        # Create a map centered only on the event if no stations are available\n",
        "        m = folium.Map(location=[event_lat, event_lon], zoom_start=12, tiles='OpenStreetMap', height=map_height, width=map_width) # Set height and width\n",
        "    else:\n",
        "        # Calculate the bounding box\n",
        "        min_lat = min(stations_to_plot['lat'].min(), event_lat)\n",
        "        max_lat = max(stations_to_plot['lat'].max(), event_lat)\n",
        "        min_lon = min(stations_to_plot['lon'].min(), event_lon)\n",
        "        max_lon = max(stations_to_plot['lon'].max(), event_lon)\n",
        "\n",
        "        # Create a map with specified height and width\n",
        "        m = folium.Map(tiles='OpenStreetMap', height=map_height, width=map_width) # Set height and width\n",
        "\n",
        "        # Fit the map to the bounding box\n",
        "        m.fit_bounds([[min_lat, min_lon], [max_lat, max_lon]])\n",
        "\n",
        "\n",
        "    # Add a marker for the event location\n",
        "    folium.Marker([event_lat, event_lon], tooltip=event_key).add_to(m)\n",
        "\n",
        "    # Add a circle with a 1km radius (approximately 0.009 degrees latitude/longitude)\n",
        "    # Note: This is a rough approximation for visualization purposes. For accurate distance calculations,\n",
        "    # use a proper library like geopy or pyproj.\n",
        "    folium.Circle(\n",
        "        location=[event_lat, event_lon],\n",
        "        radius=radius,  # Radius in meters\n",
        "        color='blue',\n",
        "        fill=True,\n",
        "        fill_color='blue',\n",
        "        fill_opacity=0.2,\n",
        "        tooltip=f'{Radius_event}km radius'\n",
        "    ).add_to(m)\n",
        "\n",
        "    # Add markers for ground stations if available\n",
        "    if not stations_to_plot.empty:\n",
        "        for idx, row in stations_to_plot.iterrows():\n",
        "            folium.CircleMarker(\n",
        "                location=[row['lat'], row['lon']],\n",
        "                radius=3,\n",
        "                color='red',\n",
        "                fill=True,\n",
        "                fill_color='red',\n",
        "                fill_opacity=0.6,\n",
        "                tooltip=row['station_id']\n",
        "            ).add_to(m)\n",
        "    print(f'Plotting {Radius_event}km radius of event')\n",
        "    # Display the map centered using HTML and set the container height, adding a margin to the left\n",
        "    display(HTML(f'<div style=\"display: flex; justify-content: center; height: {map_height}px; margin-left: 100px;\">{m._repr_html_()}</div>'))"
      ],
      "id": "9d0d4853",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9b) Extract Satellite Precipitation Pixels Around Event Location and Plot Time Series {\"display-mode\":\"form\"}\n",
        "\n",
        "# @markdown This cell extracts satellite precipitation data for pixels around the chosen event location within the specified neighborhood and generates a combined time series plot.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np # Import numpy for nanmean\n",
        "import seaborn as sns # Import seaborn\n",
        "\n",
        "if COUNTRY=='Kenya':\n",
        "  event_key = 'Kenya â€“ Mai Mahiu flash flood (2024-04-29)'\n",
        "elif COUNTRY=='Uganda':\n",
        "  event_key = 'Uganda â€“ Mbaleâ€“Kapchorwa floods (2022-07-30)'\n",
        "elif COUNTRY=='Rwanda':\n",
        "  event_key = 'Rwanda â€“ Western/Northern floods (2023-05-02)'\n",
        "else:\n",
        "    event_key = None # Handle cases where country is not in the list\n",
        "\n",
        "if event_key is None:\n",
        "    print(f'âš ï¸ No case study defined for {COUNTRY}. Cannot extract satellite pixels around event.')\n",
        "else:\n",
        "    event_lat, event_lon = EVENTS[event_key]['coords']\n",
        "\n",
        "    src={'CHIRPS':ds_chirps,'TAMSAT':ds_tamsat,'IMERG':ds_imerg,'ERA5':ds_era5}\n",
        "    event_satellite_series_pentad = {} # Dictionary to store extracted pentad time series (mean over neighborhood)\n",
        "\n",
        "    # Determine the event date range for filtering and plotting\n",
        "    if event_key in EVENTS and 'dates' in EVENTS[event_key]:\n",
        "        start_evt_str, end_evt_str = EVENTS[event_key]['dates']\n",
        "        # Define a common origin for resampling based on the event start date\n",
        "        origin_date = pd.to_datetime(start_evt_str, utc=True)\n",
        "        start_evt,end_evt=[pd.to_datetime(x).tz_localize('UTC') for x in [start_evt_str, end_evt_str]] # Localize to UTC\n",
        "    else:\n",
        "        print(\"âš ï¸ Could not determine event date range for filtering.\")\n",
        "        origin_date = None\n",
        "        start_evt, end_evt = None, None # Set to None if dates are not available\n",
        "\n",
        "\n",
        "    for dataset_name, ds in src.items():\n",
        "        if ds is not None:\n",
        "            print(f\"Extracting pixels for {dataset_name} around event location and aggregating to pentads...\")\n",
        "            try:\n",
        "                # Assuming the dataset has 'lat' and 'lon' or 'y' and 'x' dimensions\n",
        "                if 'lat' in ds.coords and 'lon' in ds.coords:\n",
        "                    lats = ds['lat'].values\n",
        "                    lons = ds['lon'].values\n",
        "                    lat_dim, lon_dim = 'lat', 'lon'\n",
        "                elif 'y' in ds.coords and 'x' in ds.coords:\n",
        "                    lats = ds['y'].values\n",
        "                    lons = ds['x'].values\n",
        "                    lat_dim, lon_dim = 'y', 'x'\n",
        "                else:\n",
        "                    print(f\"âš ï¸ Could not find 'lat'/'lon' or 'y'/'x' dimensions in {dataset_name} dataset.\")\n",
        "                    continue\n",
        "\n",
        "                # Find the nearest grid point\n",
        "                lat_idx = np.abs(lats - event_lat).argmin()\n",
        "                lon_idx = np.abs(lons - event_lon).argmin()\n",
        "\n",
        "                # Calculate slice indices for a NEIGHBORHOOD x NEIGHBORHOOD area centered around the nearest point\n",
        "                lat_start = max(lat_idx - NEIGHBORHOOD // 2, 0)\n",
        "                lat_end = min(lat_idx + NEIGHBORHOOD // 2 + 1, len(lats))\n",
        "                lon_start = max(lon_idx - NEIGHBORHOOD // 2, 0)\n",
        "                lon_end = min(lon_idx + NEIGHBORHOOD // 2 + 1, len(lons))\n",
        "\n",
        "                lat_slice = slice(lat_start, lat_end)\n",
        "                lon_slice = slice(lon_start, lon_end)\n",
        "\n",
        "\n",
        "                sub_ds = ds.isel({lat_dim: lat_slice, lon_dim: lon_slice})\n",
        "\n",
        "                # Select the precipitation variable\n",
        "                precip_var = None\n",
        "                for var in sub_ds.data_vars:\n",
        "                    if 'precip' in var.lower() or 'rainfall' in var.lower() or 'precipitation' in var.lower() or 'rfe' in var.lower():\n",
        "                         if (lat_dim in sub_ds[var].dims and lon_dim in sub_ds[var].dims and 'time' in sub_ds[var].dims):\n",
        "                             precip_var = var\n",
        "                             break\n",
        "                if precip_var is None:\n",
        "                     print(f\"âš ï¸ Could not find a precipitation variable in {dataset_name}.\")\n",
        "                     continue\n",
        "\n",
        "                # Calculate mean precipitation across all pixels in the neighborhood for each time step\n",
        "                # Ensure the time dimension is handled correctly before taking the mean\n",
        "                if 'time' in sub_ds[precip_var].dims:\n",
        "                    time_series_daily = sub_ds[precip_var].mean(dim=[lat_dim, lon_dim], skipna=True) # Calculate mean, skipping NaNs\n",
        "                    # Convert to pandas Series and ensure index is timezone-aware (UTC)\n",
        "                    time_series_daily_series = time_series_daily.to_series()\n",
        "                    if time_series_daily_series.index.tzinfo is None:\n",
        "                         time_series_daily_series = time_series_daily_series.tz_localize('UTC')\n",
        "                    elif time_series_daily_series.index.tzinfo != 'UTC':\n",
        "                         time_series_daily_series = time_series_daily_series.tz_convert('UTC')\n",
        "\n",
        "\n",
        "                    # Resample to 5-day periods (pentads), setting the origin to the event start date\n",
        "                    if origin_date is not None:\n",
        "                        time_series_pentad = time_series_daily_series.resample('5D', origin=origin_date).sum()\n",
        "                        event_satellite_series_pentad[dataset_name] = time_series_pentad\n",
        "                    else:\n",
        "                        print(f\"âš ï¸ Cannot resample {dataset_name} to pentads: Event origin date not determined.\")\n",
        "\n",
        "\n",
        "                else:\n",
        "                    print(f\"âš ï¸ Time dimension not found in precipitation variable for {dataset_name}.\")\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting and aggregating pixels for {dataset_name}: {e}\")\n",
        "\n",
        "    # --- Calculate and Display Total Rainfall During Event ---\n",
        "    if event_key is None:\n",
        "        print(f'âš ï¸ No case study defined for {COUNTRY}. Cannot calculate total rainfall during event.')\n",
        "    else:\n",
        "        if event_key in EVENTS and 'dates' in EVENTS[event_key]:\n",
        "            start_evt_str, end_evt_str = EVENTS[event_key]['dates']\n",
        "            start_evt, end_evt = [pd.to_datetime(x).tz_localize('UTC') for x in [start_evt_str, end_evt_str]]\n",
        "        else:\n",
        "            print(\"âš ï¸ Could not determine event date range for filtering.\")\n",
        "            start_evt, end_evt = None, None\n",
        "\n",
        "        if start_evt is not None and end_evt is not None:\n",
        "            total_rainfall_data = []\n",
        "\n",
        "            if 'event_satellite_series_pentad' in globals() and event_satellite_series_pentad:\n",
        "                for dataset_name, series_pentad in event_satellite_series_pentad.items():\n",
        "                    if series_pentad is not None and not series_pentad.empty:\n",
        "                        # Ensure the index is datetime and timezone-aware\n",
        "                        if series_pentad.index.tzinfo is None:\n",
        "                             series_pentad = series_pentad.tz_localize('UTC')\n",
        "                        elif series_pentad.index.tzinfo != 'UTC':\n",
        "                             series_pentad = series_pentad.tz_convert('UTC')\n",
        "\n",
        "                        # Adjust end_evt to the end of the pentad containing the event end date\n",
        "                        pentad_indices = series_pentad.index\n",
        "                        valid_end_index = pentad_indices[pentad_indices <= end_evt].max() if not pentad_indices[pentad_indices <= end_evt].empty else None\n",
        "\n",
        "                        if valid_end_index is not None:\n",
        "                            # Filter the series to the event date range\n",
        "                            series_filtered = series_pentad.loc[start_evt:valid_end_index]\n",
        "                            # Calculate the sum of precipitation for the filtered period\n",
        "                            total_rainfall = series_filtered.sum()\n",
        "                            total_rainfall_data.append({'Dataset': dataset_name, 'Total Rainfall (mm)': total_rainfall})\n",
        "                        else:\n",
        "                            print(f\"â„¹ï¸ No pentad data available within the event date range for {dataset_name}.\")\n",
        "\n",
        "\n",
        "                if total_rainfall_data:\n",
        "                    total_rainfall_df = pd.DataFrame(total_rainfall_data)\n",
        "                    print(f\"Total Rainfall During Event ({event_key}):\")\n",
        "                    display(total_rainfall_df)\n",
        "\n",
        "                    # Save the total rainfall data to a CSV file\n",
        "                    total_rainfall_output_path = os.path.join(RESULTS_DIR, f'event_total_rainfall_{COUNTRY}.csv')\n",
        "                    total_rainfall_df.to_csv(total_rainfall_output_path, index=False)\n",
        "                    print(f\"âœ… Total rainfall data saved to {total_rainfall_output_path}\")\n",
        "\n",
        "                else:\n",
        "                    print(\"âš ï¸ No total rainfall data calculated for any dataset.\")\n",
        "\n",
        "            else:\n",
        "                print(\"âš ï¸ Satellite precipitation pentad time series data ('event_satellite_series_pentad') not available. Run Step 9b first.\")\n",
        "\n",
        "        else:\n",
        "            print(\"âš ï¸ Event start or end date not determined. Cannot calculate total rainfall.\")\n",
        "\n",
        "\n",
        "    # --- Plotting ---\n",
        "    if event_satellite_series_pentad:\n",
        "        print(\"\\nGenerating combined pentad time series plot for extracted satellite pixels...\")\n",
        "\n",
        "        # Convert the dictionary of series to a DataFrame suitable for grouped bar plot\n",
        "        plot_data = []\n",
        "        if start_evt is not None and end_evt is not None:\n",
        "             for dataset_name, series_pentad in event_satellite_series_pentad.items():\n",
        "                 if series_pentad is not None and not series_pentad.empty:\n",
        "                     # Ensure the index is datetime and timezone-aware for plotting and slicing\n",
        "                     if series_pentad.index.tzinfo is None:\n",
        "                          series_pentad = series_pentad.tz_localize('UTC')\n",
        "                     elif series_pentad.index.tzinfo != 'UTC':\n",
        "                          series_pentad = series_pentad.tz_convert('UTC')\n",
        "\n",
        "                     # Adjust end_evt to the end of the pentad containing the event end date\n",
        "                     pentad_indices = series_pentad.index\n",
        "                     valid_end_index = pentad_indices[pentad_indices <= end_evt].max() if not pentad_indices[pentad_indices <= end_evt].empty else None\n",
        "\n",
        "\n",
        "                     if valid_end_index is not None:\n",
        "                          series_filtered = series_pentad.loc[start_evt:valid_end_index].copy()\n",
        "                          # Ensure dates are timezone-naive for plotting with seaborn\n",
        "                          series_filtered.index = series_filtered.index.tz_convert(None)\n",
        "                          for date, value in series_filtered.items():\n",
        "                              plot_data.append({'Date': date, 'Dataset': dataset_name, 'Precipitation': value})\n",
        "                     else:\n",
        "                         print(f\"â„¹ï¸ No pentad data available within the event date range for plotting {dataset_name}.\")\n",
        "             if not plot_data:\n",
        "                  print(\"âš ï¸ No data available within the event date range to plot.\")\n",
        "                  plot_df = pd.DataFrame() # Create an empty DataFrame\n",
        "             else:\n",
        "                  plot_df = pd.DataFrame(plot_data)\n",
        "                  # Ensure 'Date' column is datetime type\n",
        "                  plot_df['Date'] = pd.to_datetime(plot_df['Date'])\n",
        "                  # Sort by date for correct plotting order\n",
        "                  plot_df = plot_df.sort_values(by='Date')\n",
        "\n",
        "\n",
        "        else:\n",
        "             print(\"âš ï¸ Event date range not determined. Cannot filter data for plotting.\")\n",
        "             plot_df = pd.DataFrame() # Create an empty DataFrame if no date range\n",
        "\n",
        "\n",
        "        if not plot_df.empty:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            sns.barplot(data=plot_df, x='Date', y='Precipitation', hue='Dataset')\n",
        "\n",
        "            plt.title(f\"Satellite Precipitation Pentad Totals Around Event Location ({event_key})\")\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Mean Precipitation (mm)') # Updated label to reflect mean\n",
        "            plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for readability\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save the plot\n",
        "            plot_output_path = os.path.join(RESULTS_DIR, f'event_pixels_pentad_timeseries_combined_barplot.png')\n",
        "            plt.savefig(plot_output_path)\n",
        "            plt.show()\n",
        "            print(f\"âœ… Combined pentad time series bar plot saved to {plot_output_path}\")\n",
        "\n",
        "            # Add individual bar plots for each dataset\n",
        "            print(\"\\nGenerating individual pentad time series bar plots for each dataset...\")\n",
        "            for dataset_name, series_pentad in event_satellite_series_pentad.items():\n",
        "                if series_pentad is not None and not series_pentad.empty:\n",
        "                    # Ensure the index is datetime and timezone-aware for plotting and slicing\n",
        "                    if series_pentad.index.tzinfo is None:\n",
        "                        series_pentad = series_pentad.tz_localize('UTC')\n",
        "                    elif series_pentad.index.tzinfo != 'UTC':\n",
        "                        series_pentad = series_pentad.tz_convert('UTC')\n",
        "\n",
        "                    # Adjust end_evt to the end of the pentad containing the event end date\n",
        "                    pentad_indices = series_pentad.index\n",
        "                    valid_end_index = pentad_indices[pentad_indices <= end_evt].max() if not pentad_indices[pentad_indices <= end_evt].empty else None\n",
        "\n",
        "                    if valid_end_index is not None:\n",
        "                        series_filtered = series_pentad.loc[start_evt:valid_end_index].copy()\n",
        "                        # Ensure dates are timezone-naive for plotting with seaborn\n",
        "                        series_filtered.index = series_filtered.index.tz_convert(None)\n",
        "\n",
        "                        plt.figure(figsize=(9, 4))\n",
        "                        sns.barplot(x=series_filtered.index, y=series_filtered.values, color=sns.color_palette()[Satellite_data_options.index(dataset_name)]) # Use a consistent color\n",
        "                        plt.title(f\"{dataset_name} Precipitation Pentad Totals Around Event Location ({event_key})\")\n",
        "                        plt.xlabel('Date')\n",
        "                        plt.ylabel('Mean Precipitation (mm)')\n",
        "                        plt.xticks(rotation=45, ha='right')\n",
        "                        plt.tight_layout()\n",
        "\n",
        "                        # Save the individual plot\n",
        "                        individual_plot_output_path = os.path.join(RESULTS_DIR, f'event_pixels_pentad_timeseries_{dataset_name}_barplot.png')\n",
        "                        plt.savefig(individual_plot_output_path)\n",
        "                        plt.show()\n",
        "                        print(f\"âœ… {dataset_name} pentad time series bar plot saved to {individual_plot_output_path}\")\n",
        "                    else:\n",
        "                        print(f\"â„¹ï¸ No pentad data available within the event date range for plotting individual {dataset_name} plot.\")\n",
        "                else:\n",
        "                    print(f\"â„¹ï¸ Skipping individual plot for {dataset_name}: Data not available or is empty.\")\n",
        "\n",
        "\n",
        "        else:\n",
        "            print('âš ï¸ No satellite precipitation pentad time series data available for plotting.')\n",
        "\n",
        "    else:\n",
        "        print('âš ï¸ No satellite precipitation pentad time series available for plotting.')"
      ],
      "metadata": {
        "id": "HSbP7E_9y1ZE"
      },
      "id": "HSbP7E_9y1ZE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9c) Daily Rainfall Table During Event {\"display-mode\":\"form\"}\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "if COUNTRY=='Kenya':\n",
        "  event_key = 'Kenya â€“ Mai Mahiu flash flood (2024-04-29)'\n",
        "elif COUNTRY=='Uganda':\n",
        "  event_key = 'Uganda â€“ Mbaleâ€“Kapchorwa floods (2022-07-30)'\n",
        "elif COUNTRY=='Rwanda':\n",
        "  event_key = 'Rwanda â€“ Western/Northern floods (2023-05-02)'\n",
        "else:\n",
        "    event_key = None # Handle cases where country is not in the list\n",
        "\n",
        "if event_key is None:\n",
        "    print(f'âš ï¸ No case study defined for {COUNTRY}. Cannot generate daily rainfall table.')\n",
        "else:\n",
        "    # Determine the event date range\n",
        "    if event_key in EVENTS and 'dates' in EVENTS[event_key]:\n",
        "        start_evt_str, end_evt_str = EVENTS[event_key]['dates']\n",
        "        start_evt, end_evt = [pd.to_datetime(x).tz_localize('UTC') for x in [start_evt_str, end_evt_str]]\n",
        "        event_lat, event_lon = EVENTS[event_key]['coords'] # Get event coordinates\n",
        "    else:\n",
        "        print(\"âš ï¸ Could not determine event date range or coordinates for filtering.\")\n",
        "        start_evt, end_evt = None, None\n",
        "        event_lat, event_lon = None, None\n",
        "\n",
        "    if start_evt is not None and end_evt is not None and event_lat is not None and event_lon is not None:\n",
        "        daily_rainfall_data = {}\n",
        "        src={'CHIRPS':ds_chirps,'TAMSAT':ds_tamsat,'IMERG':ds_imerg,'ERA5':ds_era5} # Assume datasets are loaded\n",
        "\n",
        "        for dataset_name, ds in src.items():\n",
        "            if ds is not None:\n",
        "                print(f\"Extracting daily pixels for {dataset_name} around event location...\")\n",
        "                try:\n",
        "                     # Assuming the dataset has 'lat' and 'lon' or 'y' and 'x' dimensions\n",
        "                    if 'lat' in ds.coords and 'lon' in ds.coords:\n",
        "                        lats = ds['lat'].values\n",
        "                        lons = ds['lon'].values\n",
        "                        lat_dim, lon_dim = 'lat', 'lon'\n",
        "                    elif 'y' in ds.coords and 'x' in ds.coords:\n",
        "                        lats = ds['y'].values\n",
        "                        lons = ds['x'].values\n",
        "                        lat_dim, lon_dim = 'y', 'x'\n",
        "                    else:\n",
        "                        print(f\"âš ï¸ Could not find 'lat'/'lon' or 'y'/'x' dimensions in {dataset_name} dataset.\")\n",
        "                        continue\n",
        "\n",
        "                    # Find the nearest grid point\n",
        "                    lat_idx = np.abs(lats - event_lat).argmin()\n",
        "                    lon_idx = np.abs(lons - event_lon).argmin()\n",
        "\n",
        "                    # Calculate slice indices for a NEIGHBORHOOD x NEIGHBORHOOD area centered around the nearest point\n",
        "                    lat_start = max(lat_idx - NEIGHBORHOOD // 2, 0)\n",
        "                    lat_end = min(lat_idx + NEIGHBORHOOD // 2 + 1, len(lats))\n",
        "                    lon_start = max(lon_idx - NEIGHBORHOOD // 2, 0)\n",
        "                    lon_end = min(lon_idx + NEIGHBORHOOD // 2 + 1, len(lons))\n",
        "\n",
        "                    lat_slice = slice(lat_start, lat_end)\n",
        "                    lon_slice = slice(lon_start, lon_end)\n",
        "\n",
        "                    sub_ds = ds.isel({lat_dim: lat_slice, lon_dim: lon_slice})\n",
        "\n",
        "                    # Select the precipitation variable\n",
        "                    precip_var = None\n",
        "                    for var in sub_ds.data_vars:\n",
        "                        if 'precip' in var.lower() or 'rainfall' in var.lower() or 'precipitation' in var.lower() or 'rfe' in var.lower():\n",
        "                             if (lat_dim in sub_ds[var].dims and lon_dim in sub_ds[var].dims and 'time' in sub_ds[var].dims):\n",
        "                                 precip_var = var\n",
        "                                 break\n",
        "                    if precip_var is None:\n",
        "                         print(f\"âš ï¸ Could not find a precipitation variable in {dataset_name}.\")\n",
        "                         continue\n",
        "\n",
        "\n",
        "                    # Calculate mean precipitation across all pixels in the neighborhood for each time step\n",
        "                    if 'time' in sub_ds[precip_var].dims:\n",
        "                        time_series_daily = sub_ds[precip_var].mean(dim=[lat_dim, lon_dim], skipna=True) # Calculate mean, skipping NaNs\n",
        "                        # Convert to pandas Series and ensure index is timezone-aware (UTC)\n",
        "                        time_series_daily_series = time_series_daily.to_series()\n",
        "                        if time_series_daily_series.index.tzinfo is None:\n",
        "                             time_series_daily_series = time_series_daily_series.tz_localize('UTC')\n",
        "                        elif time_series_daily_series.index.tzinfo != 'UTC':\n",
        "                             time_series_daily_series = time_series_daily_series.tz_convert('UTC')\n",
        "\n",
        "                        # Determine the adjusted end date for filtering to match pentads\n",
        "                        # Find the last pentad index that is less than or equal to the original event end date\n",
        "                        pentad_indices = time_series_daily_series.resample('5D', origin=start_evt).sum().index\n",
        "                        valid_end_index = pentad_indices[pentad_indices <= end_evt].max() if not pentad_indices[pentad_indices <= end_evt].empty else None\n",
        "\n",
        "                        if valid_end_index is not None:\n",
        "                           # The actual end date for filtering daily data should be the end of this last pentad\n",
        "                           adjusted_end_date = valid_end_index + pd.Timedelta(days=4) # Add 4 days to get to the end of the 5-day period\n",
        "                           # Filter daily series to the adjusted date range and store\n",
        "                           daily_filtered = time_series_daily_series.loc[start_evt:adjusted_end_date]\n",
        "                           daily_rainfall_data[dataset_name] = daily_filtered\n",
        "                        else:\n",
        "                           print(f\"â„¹ï¸ No pentad data available within the event date range for {dataset_name}. Cannot determine adjusted end date.\")\n",
        "                           continue # Skip this dataset if adjusted end date cannot be determined\n",
        "\n",
        "\n",
        "                    else:\n",
        "                        print(f\"âš ï¸ Time dimension not found in precipitation variable for {dataset_name}.\")\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting daily pixels for {dataset_name}: {e}\")\n",
        "\n",
        "\n",
        "        if daily_rainfall_data:\n",
        "            # Combine the daily rainfall series into a single DataFrame\n",
        "            # Use concat with join='outer' to include all dates within the event window\n",
        "            daily_rainfall_df = pd.concat(daily_rainfall_data, axis=1, join='outer')\n",
        "\n",
        "            # Calculate the total rainfall for each dataset\n",
        "            total_rainfall_row = daily_rainfall_df.sum().to_frame().T\n",
        "            total_rainfall_row.index = ['Total']\n",
        "\n",
        "            # Combine daily data and total row\n",
        "            daily_rainfall_with_total = pd.concat([daily_rainfall_df, total_rainfall_row])\n",
        "\n",
        "            print(f\"Daily Rainfall (mm) Around Event Location During Event Window ({event_key}):\")\n",
        "            # Format the table for better readability\n",
        "            styled_table = daily_rainfall_with_total.style.format('{:.2f}')\n",
        "            display(styled_table)\n",
        "\n",
        "            # Save the daily rainfall data with totals to a CSV file\n",
        "            daily_rainfall_output_path = os.path.join(RESULTS_DIR, f'event_daily_rainfall_table_{COUNTRY}.csv')\n",
        "            daily_rainfall_with_total.to_csv(daily_rainfall_output_path)\n",
        "            print(f\"âœ… Daily rainfall table saved to {daily_rainfall_output_path}\")\n",
        "\n",
        "        else:\n",
        "            print(\"âš ï¸ No daily rainfall data available for the event period from any dataset.\")\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ Event start or end date, or coordinates not determined. Cannot generate daily rainfall table.\")"
      ],
      "metadata": {
        "id": "rz2xtxmAZBPi"
      },
      "id": "rz2xtxmAZBPi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "workshop_meta": {
      "title": "NOAA Workshop â€“ Rainfall Skill Explorer (KE/UG/RW)",
      "built": "2025-10-10T10:12:38.082662Z"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}